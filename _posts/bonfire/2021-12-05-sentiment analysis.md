---
title : "[Data] Sentiment Analysis with BiLSTM Model trained with Multiple Datasets"
categories : 
    - Bonfire
tag : [Bonfire, ì½”ë¡œë‚˜, BiLSTM, NLP, ê°ì •ë¶„ì„]
toc : true
---

## **í•„ìš”í•œ Module Import**

```python
!pip install konlpy
```

    Collecting konlpy
      Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.4 MB 1.4 MB/s 
    [?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)
    Collecting JPype1>=0.7.0
      Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 448 kB 42.7 MB/s 
    [?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)
    Collecting beautifulsoup4==4.6.0
      Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86 kB 5.4 MB/s 
    [?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)
    Collecting colorama
      Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)
    Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)
    Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)
    Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)
    Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)
    Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)
    Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)
    Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)
    Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy
      Attempting uninstall: beautifulsoup4
        Found existing installation: beautifulsoup4 4.6.3
        Uninstalling beautifulsoup4-4.6.3:
          Successfully uninstalled beautifulsoup4-4.6.3
    Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2
    


```python
import re
import urllib.request
import numpy as np
import warnings
warnings.simplefilter(action='ignore', category=RuntimeWarning)
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use("seaborn-dark")

from konlpy.tag import Mecab
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
```

## **DATASET ì •ë¦¬**
- ê°ì •ë¶„ì„ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ê°ì • ë¼ë²¨ë§ ë°ì´í„°ì…‹
- ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹, ë„¤ì´ë²„ ì‡¼í•‘ëª° ë¦¬ë·° ë°ì´í„°ì…‹, ìŠ¤íŒ€ ê²Œì„ ë¦¬ë·° ì´ 3ê°œì˜ ë°ì´í„°ì…‹ì„ í™œìš©í–ˆë‹¤.

### **1. Naver Movie Review Datset**


```python
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt",\
                           filename="ratings_train.txt")
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt",\
                           filename="ratings_test.txt")
```




    ('ratings_test.txt', <http.client.HTTPMessage at 0x7f41000f26d0>)




```python
movie_test_data = pd.read_table("/content/ratings_test.txt")
movie_train_data = pd.read_table("/content/ratings_train.txt")
```


```python
movie_train_data
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>document</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9976970</td>
      <td>ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3819312</td>
      <td>í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>10265843</td>
      <td>ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9045019</td>
      <td>êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6483659</td>
      <td>ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149995</th>
      <td>6222902</td>
      <td>ì¸ê°„ì´ ë¬¸ì œì§€.. ì†ŒëŠ” ë­”ì£„ì¸ê°€..</td>
      <td>0</td>
    </tr>
    <tr>
      <th>149996</th>
      <td>8549745</td>
      <td>í‰ì ì´ ë„ˆë¬´ ë‚®ì•„ì„œ...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>149997</th>
      <td>9311800</td>
      <td>ì´ê²Œ ë­ìš”? í•œêµ­ì¸ì€ ê±°ë“¤ë¨¹ê±°ë¦¬ê³  í•„ë¦¬í•€ í˜¼í˜ˆì€ ì°©í•˜ë‹¤?</td>
      <td>0</td>
    </tr>
    <tr>
      <th>149998</th>
      <td>2376369</td>
      <td>ì²­ì¶˜ ì˜í™”ì˜ ìµœê³ ë´‰.ë°©í™©ê³¼ ìš°ìš¸í–ˆë˜ ë‚ ë“¤ì˜ ìí™”ìƒ</td>
      <td>1</td>
    </tr>
    <tr>
      <th>149999</th>
      <td>9619869</td>
      <td>í•œêµ­ ì˜í™” ìµœì´ˆë¡œ ìˆ˜ê°„í•˜ëŠ” ë‚´ìš©ì´ ë‹´ê¸´ ì˜í™”</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>150000 rows Ã— 3 columns</p>
</div>



### **2. Naver ShoppingMall Review Dataset**



```python
urllib.request.urlretrieve("https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt", \
                    filename="shop_ratings.txt")
```




    ('shop_ratings.txt', <http.client.HTTPMessage at 0x7f410008d410>)




```python
shop_data = pd.read_table("/content/shop_ratings.txt")
```


```python
shop_data = shop_data.rename(columns={'5' : "label", "ë°°ê³µë¹ ë¥´ê³  êµ¿" : "document"})
shop_data['label'] = shop_data['label'].apply(lambda x : 1 if x > 3 else 0)
```


```python
shop_data.shape
```




    (199999, 2)




```python
shop_train_data = shop_data.iloc[:150000, :]
shop_test_data = shop_data.iloc[150000: :]
```


```python
shop_train_data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 150000 entries, 0 to 149999
    Data columns (total 2 columns):
     #   Column    Non-Null Count   Dtype 
    ---  ------    --------------   ----- 
     0   label     150000 non-null  int64 
     1   document  150000 non-null  object
    dtypes: int64(1), object(1)
    memory usage: 2.3+ MB
    


```python
shop_test_data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 49999 entries, 150000 to 199998
    Data columns (total 2 columns):
     #   Column    Non-Null Count  Dtype 
    ---  ------    --------------  ----- 
     0   label     49999 non-null  int64 
     1   document  49999 non-null  object
    dtypes: int64(1), object(1)
    memory usage: 781.4+ KB
    


```python
shop_train_data.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>document</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>íƒë°°ê°€ ì—‰ë§ì´ë„¤ìš© ì €í¬ì§‘ ë°‘ì—ì¸µì— ë§ë„ì—†ì´ ë†”ë‘ê³ ê°€ê³ </td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>ì•„ì£¼ì¢‹ì•„ìš” ë°”ì§€ ì •ë§ ì¢‹ì•„ì„œ2ê°œ ë” êµ¬ë§¤í–ˆì–´ìš” ì´ê°€ê²©ì— ëŒ€ë°•ì…ë‹ˆë‹¤. ë°”ëŠì§ˆì´ ì¡°ê¸ˆ ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>ì„ ë¬¼ìš©ìœ¼ë¡œ ë¹¨ë¦¬ ë°›ì•„ì„œ ì „ë‹¬í–ˆì–´ì•¼ í•˜ëŠ” ìƒí’ˆì´ì—ˆëŠ”ë° ë¨¸ê·¸ì»µë§Œ ì™€ì„œ ë‹¹í™©í–ˆìŠµë‹ˆë‹¤. ì „...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>ë¯¼íŠ¸ìƒ‰ìƒ ì˜ˆë»ìš”. ì˜† ì†ì¡ì´ëŠ” ê±°ëŠ” ìš©ë„ë¡œë„ ì‚¬ìš©ë˜ë„¤ìš” ã…ã…</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>ë¹„ì¶”í•©ë‹ˆë‹¤ ê³„ë€ ë’¤ì§‘ì„ ë•Œ ì™„ì „ ë¶ˆí¸í•´ìš” ã… ã…  ì½”íŒ…ë„ ë¬»ì–´ë‚˜ê³  ë³´ê¸°ì—” ì˜ˆì˜ê³  ì‹¤ìš©ì ...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>ì£¼ë¬¸ì„ 11ì›”6ì— ì‹œì¼°ëŠ”ë° 11ì›”16ì¼ì— ë°°ì†¡ì´ ì™”ë„¤ìš” ã…ã…ã… ì—¬ê¸° íšŒì‚¬ì¸¡ê³¼ëŠ” ì „í™”...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>ë„‰ë„‰í•œ ê¸¸ì´ë¡œ ì£¼ë¬¸í–ˆëŠ”ë°ë„ ì•ˆ ë§ë„¤ìš” ë³„ë¡œì˜ˆìš”</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>ë³´í´ì´ ê³„ì† ë•Œì²˜ëŸ¼ ë‚˜ì˜¤ë‹¤ê°€ ì§€ê¸ˆì€ ì•ˆë‚˜ë„¤ìš”~</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>110ì¸ë° ì „ë¬¸ì†ì˜·ë¸Œëœë“œ ìœ„ìƒíŒ¬í‹°105ë³´ë‹¤ ì‘ì€ë“¯í•´ìš”. ë¶ˆí¸í•´ìš”. ë°´ë”©ë¶€ë¶„ì´ ë‹¤ ì‹ ...</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1</td>
      <td>ì‚¬ì´ì¦ˆë„ ë”±ì´ê³  ê·€ì—½ê³  ë„˜ ì¢‹ì•„ìš” ã…ã…</td>
    </tr>
  </tbody>
</table>
</div>




```python
shop_train_data['label'].value_counts()
```




    0    75109
    1    74891
    Name: label, dtype: int64




```python
shop_test_data['label'].value_counts()
```




    1    25071
    0    24928
    Name: label, dtype: int64



### **3. Game Review Dataset**


```python
urllib.request.urlretrieve("https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/steam.txt", \
                           filename="game_ratings.txt")
```




    ('game_ratings.txt', <http.client.HTTPMessage at 0x7f41000aa3d0>)




```python
game_data = pd.read_table("/content/game_ratings.txt")
```


```python
game_data
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>ë…¸ë˜ê°€ ë„ˆë¬´ ì ìŒ</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>ëŒê² ë„¤ ì§„ì§œ. í™©ìˆ™ì•„, ì–´í¬ ê³µì¥ ê·¸ë§Œ ëŒë ¤ë¼. ì£½ëŠ”ë‹¤.</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>ë§‰ë…¸ë™ ì²´í—˜íŒ ë§‰ë…¸ë™ í•˜ëŠ”ì‚¬ëŒì¸ë° ì¥ë¹„ë¥¼ ë‚´ê°€ ì‚¬ì•¼ë¼ ë­ì§€</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>ì°¨ì•…!ì°¨ì•…!!ì°¨ì•…!!! ì •ë§ ì´ë˜ì„œ ì™•êµ­ì„ ë˜ì°¾ì„ ìˆ˜ ìˆëŠ”ê±°ì•¼??</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>ì‹œê°„ ë•Œìš°ê¸°ì— ì¢‹ìŒ.. ë„ì „ê³¼ì œëŠ” 50ì‹œê°„ì´ë©´ ë‹¤ ê¹° ìˆ˜ ìˆì–´ìš”</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>ì—­ì‹œ ì¬ë¯¸ìˆë„¤ìš” ì „ì‘ì—ì„œ í• ìˆ˜ ì—†ì—ˆë˜ ììœ ë¡œìš´ ë± ë¹Œë”©ë„ ì¢‹ë„¤ìš”^^</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>99994</th>
      <td>0</td>
      <td>í•œê¸€í™”í•´ì£¼ë©´ 10ê°œì‚°ë‹¤</td>
    </tr>
    <tr>
      <th>99995</th>
      <td>0</td>
      <td>ê°œìŒ‰ë…¸ì¼ ã…‹ã…‹</td>
    </tr>
    <tr>
      <th>99996</th>
      <td>0</td>
      <td>ë…¸ì¼ì´ë„¤ìš”... 30ë¶„í•˜ê³  ì§€ì› ì–´ìš”...</td>
    </tr>
    <tr>
      <th>99997</th>
      <td>1</td>
      <td>ì•¼ìƒì„ ì‚¬ë‘í•˜ëŠ” ì‚¬ëŒë“¤ì„ ìœ„í•œ ì§§ì§€ë§Œ ì—¬ìš´ì´ ë‚¨ëŠ” ì´ì•¼ê¸°. ì˜ì–´ëŠ” ê·¸ë¦¬ ì–´ë µì§€ ì•ŠìŠµë‹ˆë‹¤.</td>
    </tr>
    <tr>
      <th>99998</th>
      <td>1</td>
      <td>í•œêµ­ì˜ ë©”íƒˆë ˆì´ì§€ë¥¼ ë– ì˜¤ë¥´ê²Œí•œë‹¤ ì§„ì§œ ì†ë§›ìœ¼ë¡œ í•˜ëŠ”ê²Œì„</td>
    </tr>
  </tbody>
</table>
<p>99999 rows Ã— 2 columns</p>
</div>




```python
game_data = game_data.rename(columns={'0' : "label", 'ë…¸ë˜ê°€ ë„ˆë¬´ ì ìŒ' : "document"})
```


```python
game_train_data = game_data.iloc[:80000, :]
game_test_data = game_data.iloc[80000: :]
```


```python
game_train_data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 80000 entries, 0 to 79999
    Data columns (total 2 columns):
     #   Column    Non-Null Count  Dtype 
    ---  ------    --------------  ----- 
     0   label     80000 non-null  int64 
     1   document  80000 non-null  object
    dtypes: int64(1), object(1)
    memory usage: 1.2+ MB
    


```python
game_test_data.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 19999 entries, 80000 to 99998
    Data columns (total 2 columns):
     #   Column    Non-Null Count  Dtype 
    ---  ------    --------------  ----- 
     0   label     19999 non-null  int64 
     1   document  19999 non-null  object
    dtypes: int64(1), object(1)
    memory usage: 312.6+ KB
    


```python
game_train_data['label'].value_counts()
game_test_data['label'].value_counts()
```




    1    10003
    0     9996
    Name: label, dtype: int64



### **ëª¨ë“  ë°ì´í„°ì…‹ preprocess ë° merge**


```python
def preprocess_df(train_data, test_data):

    train_data.drop_duplicates(subset=['document'], inplace=True)
    train_data.dropna(inplace=True)
    train_data['document'] = train_data['document'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")
    train_data['document'] = train_data['document'].str.replace('^ +', "")   
    train_data['document'].replace("", np.nan, inplace=True)      
    train_data.dropna(inplace=True)
    train_data.drop_duplicates(subset=['document'], inplace=True)

    test_data.drop_duplicates(subset=['document'], inplace=True)
    test_data.dropna(inplace=True)
    test_data['document'] = test_data['document'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")
    test_data['document'] = test_data['document'].str.replace('^ +', "")   
    test_data['document'].replace("", np.nan, inplace=True)      
    test_data.dropna(inplace=True)
    test_data.drop_duplicates(subset=['document'], inplace=True)

    return train_data, test_data
```


```python
movie_train_data, movie_test_data = preprocess_df(movie_train_data, movie_test_data)
shop_train_data, shop_test_data = preprocess_df(shop_train_data, shop_test_data)
game_train_data, game_test_data = preprocess_df(game_train_data, game_test_data)
```


```python
df_list = [movie_train_data, movie_test_data, shop_train_data, shop_test_data, game_train_data, game_test_data]

for df in df_list:
    print(df.shape)
```

    (143620, 3)
    (48389, 3)
    (149638, 2)
    (49936, 2)
    (79517, 2)
    (19970, 2)
    


```python
# column ìˆœì„œ ë°”ê¾¸ê³  id column ì‚­ì œ

movie_train_data.drop('id', axis=1, inplace=True)
movie_train_data = movie_train_data[['label', 'document']]
movie_test_data.drop('id', axis=1, inplace=True)
movie_test_data = movie_test_data[['label', 'document']]
```


```python
movie_train_data
movie_test_data
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>document</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>êµ³ ã…‹</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>ë­ì•¼ ì´ í‰ì ë“¤ì€ ë‚˜ì˜ì§„ ì•Šì§€ë§Œ ì  ì§œë¦¬ëŠ” ë”ë”ìš± ì•„ë‹ˆì–ì•„</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>ì§€ë£¨í•˜ì§€ëŠ” ì•Šì€ë° ì™„ì „ ë§‰ì¥ì„ ëˆì£¼ê³  ë³´ê¸°ì—ëŠ”</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>ë§Œ ì•„ë‹ˆì—ˆì–´ë„ ë³„ ë‹¤ì„¯ ê°œ ì¤¬ì„í…ë° ì™œ ë¡œ ë‚˜ì™€ì„œ ì œ ì‹¬ê¸°ë¥¼ ë¶ˆí¸í•˜ê²Œ í•˜ì£ </td>
    </tr>
    <tr>
      <th>5</th>
      <td>1</td>
      <td>ìŒì•…ì´ ì£¼ê°€ ëœ ìµœê³ ì˜ ìŒì•…ì˜í™”</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>49995</th>
      <td>1</td>
      <td>ì˜¤ëœë§Œì— í‰ì  ë¡œê¸´í–ˆë„¤ã…‹ã…‹ í‚¹ì™•ì§± ìŒˆë½•í•œ ì˜í™”ë¥¼ ë§Œë‚¬ìŠµë‹ˆë‹¤ ê°•ë ¬í•˜ê²Œ ìœ¡ì¾Œí•¨</td>
    </tr>
    <tr>
      <th>49996</th>
      <td>0</td>
      <td>ì˜ì§€ ë°•ì•½ë“¤ì´ë‚˜ í•˜ëŠ”ê±°ë‹¤ íƒˆì˜ì€ ì¼ë‹¨ ì£¼ì¸ê³µ ê¹€ëŒ€í¬ ë‹®ì•˜ê³  ì´ë“±ë³‘ ì°ë”°</td>
    </tr>
    <tr>
      <th>49997</th>
      <td>0</td>
      <td>ê·¸ë¦¼ë„ ì¢‹ê³  ì™„ì„±ë„ë„ ë†’ì•˜ì§€ë§Œ ë³´ëŠ” ë‚´ë‚´ ë¶ˆì•ˆí•˜ê²Œ ë§Œë“ ë‹¤</td>
    </tr>
    <tr>
      <th>49998</th>
      <td>0</td>
      <td>ì ˆëŒ€ ë´ì„œëŠ” ì•ˆ ë  ì˜í™” ì¬ë¯¸ë„ ì—†ê³  ê¸°ë¶„ë§Œ ì¡ì¹˜ê³  í•œ ì„¸íŠ¸ì¥ì—ì„œ ë‹¤ í•´ë¨¹ë„¤</td>
    </tr>
    <tr>
      <th>49999</th>
      <td>0</td>
      <td>ë§ˆë¬´ë¦¬ëŠ” ë˜ ì™œì´ë˜</td>
    </tr>
  </tbody>
</table>
<p>48389 rows Ã— 2 columns</p>
</div>




```python
# movie_train_data
# shop_train_data
game_train_data
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>document</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>ëŒê² ë„¤ ì§„ì§œ í™©ìˆ™ì•„ ì–´í¬ ê³µì¥ ê·¸ë§Œ ëŒë ¤ë¼ ì£½ëŠ”ë‹¤</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>ë§‰ë…¸ë™ ì²´í—˜íŒ ë§‰ë…¸ë™ í•˜ëŠ”ì‚¬ëŒì¸ë° ì¥ë¹„ë¥¼ ë‚´ê°€ ì‚¬ì•¼ë¼ ë­ì§€</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>ì°¨ì•…ì°¨ì•…ì°¨ì•… ì •ë§ ì´ë˜ì„œ ì™•êµ­ì„ ë˜ì°¾ì„ ìˆ˜ ìˆëŠ”ê±°ì•¼</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>ì‹œê°„ ë•Œìš°ê¸°ì— ì¢‹ìŒ ë„ì „ê³¼ì œëŠ” ì‹œê°„ì´ë©´ ë‹¤ ê¹° ìˆ˜ ìˆì–´ìš”</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>ì—­ì‹œ ì¬ë¯¸ìˆë„¤ìš” ì „ì‘ì—ì„œ í• ìˆ˜ ì—†ì—ˆë˜ ììœ ë¡œìš´ ë± ë¹Œë”©ë„ ì¢‹ë„¤ìš”</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>79995</th>
      <td>0</td>
      <td>ì–´ëŠì •ë„ ì¬ë¯¸ê°€ ìˆê¸´ í•˜ì§€ë§Œ ì§€ê¸ˆ í•˜ê¸°ì—” ë„ˆë¬´ ì§œì¦ë‚˜ëŠ” ì ì´ ë§ì€ ê²Œì„ ëˆëª¨ì•„ì„œ ë‹¤...</td>
    </tr>
    <tr>
      <th>79996</th>
      <td>1</td>
      <td>í‰ê°€ ì½ì„ ì‹œê°„ì— ë“€ì–¼í•˜ì…ˆ ë“€ì–¼ê·¼ ì†ì‹¤ ì˜´ ë°˜ë°•ì‹œ ë“€ì–¼</td>
    </tr>
    <tr>
      <th>79997</th>
      <td>1</td>
      <td>ì´í›„ë¡œëŠ” ê²Œì„ì´ ìƒë‹¹íˆ ë³€í™”í–ˆê³  ì‹­ìêµ° ì‹œìŠ¤í…œê³¼ í˜ˆí†µ ì‹œìŠ¤í…œì€ ìƒë‹¹íˆ ë§˜ì— ë“­ë‹ˆë‹¤ ...</td>
    </tr>
    <tr>
      <th>79998</th>
      <td>1</td>
      <td>ëƒ¥ê²œ</td>
    </tr>
    <tr>
      <th>79999</th>
      <td>1</td>
      <td>ë‹¬ê»„ë£©ì˜ ì°¬ë€í•œ ëª¨í—˜ê¸°</td>
    </tr>
  </tbody>
</table>
<p>79517 rows Ã— 2 columns</p>
</div>




```python
# concat all three datasets

train_data = pd.concat([movie_train_data, shop_train_data, game_train_data], join='outer')
test_data = pd.concat([movie_test_data, shop_test_data, game_test_data], join='outer')
```


```python
train_data.shape
# test_data.shape
```




    (372775, 2)




```python
train_data.head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>document</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>ì•„ ë”ë¹™ ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>í í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤í‰ì  ì¡°ì •</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ë˜...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>ë§‰ ê±¸ìŒë§ˆ ë—€ ì„¸ë¶€í„° ì´ˆë“±í•™êµ í•™ë…„ìƒì¸ ì‚´ìš©ì˜í™”ã…‹ã…‹ã…‹ë³„ë°˜ê°œë„ ì•„ê¹Œì›€</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>ì›ì‘ì˜ ê¸´ì¥ê°ì„ ì œëŒ€ë¡œ ì‚´ë ¤ë‚´ì§€ëª»í–ˆë‹¤</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>ë³„ ë°˜ê°œë„ ì•„ê¹ë‹¤ ìš•ë‚˜ì˜¨ë‹¤ ì´ì‘ê²½ ê¸¸ìš©ìš° ì—°ê¸°ìƒí™œì´ëª‡ë…„ì¸ì§€ì •ë§ ë°œë¡œí•´ë„ ê·¸ê²ƒë³´ë‹¨ ë‚«...</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1</td>
      <td>ì•¡ì…˜ì´ ì—†ëŠ”ë°ë„ ì¬ë¯¸ ìˆëŠ” ëª‡ì•ˆë˜ëŠ” ì˜í™”</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1</td>
      <td>ì™œì¼€ í‰ì ì´ ë‚®ì€ê±´ë° ê½¤ ë³¼ë§Œí•œë° í—ë¦¬ìš°ë“œì‹ í™”ë ¤í•¨ì—ë§Œ ë„ˆë¬´ ê¸¸ë“¤ì—¬ì ¸ ìˆë‚˜</td>
    </tr>
  </tbody>
</table>
</div>



---

## **Tokenization & Padding**


```python
stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']
stopwords += ['ì´', 'ìˆ', 'í•˜', 'ê²ƒ', 'ë“¤', 'ê·¸', 'ë˜', 'ìˆ˜', 'ì´', 'ë³´', 'ì•Š', 'ì—†', 'ë‚˜', 'ì‚¬ëŒ', 'ì£¼', 'ì•„ë‹ˆ', 'ë“±', 'ê°™', 'ìš°ë¦¬', 'ë•Œ', 'ë…„', 'ê°€', 'í•œ', 'ì§€', 'ëŒ€í•˜', 'ì˜¤', 'ë§', 'ì¼', 'ê·¸ë ‡', 'ìœ„í•˜']
stopwords = list(set(stopwords))
# stopwords
```


```python
from konlpy.tag import Okt      
from tqdm import tqdm
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

okt = Okt()
```


```python
x_train = []
for sentence in tqdm(train_data['document']):
    tokenized_sent = okt.morphs(sentence, stem=True)      # í˜•íƒœì†Œ ë³„ë¡œ ë¬¸ì¥ ë¶„í• 
    tokenized_sent = [word for word in tokenized_sent if word not in stopwords]   # ë¶ˆìš©ì–´ ì œê±°
    x_train.append(tokenized_sent)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 372775/372775 [24:49<00:00, 250.33it/s]
    


```python
x_train[:2]
```




    [['ì•„', 'ë”ë¹™', 'ì§„ì§œ', 'ì§œì¦ë‚˜ë‹¤', 'ëª©ì†Œë¦¬'],
     ['í ', 'í¬ìŠ¤í„°', 'ë³´ê³ ', 'ì´ˆë”©', 'ì˜í™”', 'ì¤„', 'ì˜¤ë²„', 'ì—°ê¸°', 'ì¡°ì°¨', 'ê°€ë³ë‹¤', 'ì•Šë‹¤']]




```python
x_test = []
for sentence in tqdm(test_data['document']):
    tokenized_sent = okt.morphs(sentence, stem=True)      # í˜•íƒœì†Œ ë³„ë¡œ ë¬¸ì¥ ë¶„í• 
    tokenized_sent = [word for word in tokenized_sent if word not in stopwords]   # ë¶ˆìš©ì–´ ì œê±°
    x_test.append(tokenized_sent)
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 118295/118295 [09:59<00:00, 197.30it/s]
    


```python
test_data['document'][:2]
x_test[:2]
```




    [['êµ³ë‹¤', 'ã…‹'], ['ë­', 'ì•¼', 'í‰ì ', 'ë‚˜ì˜ë‹¤', 'ì•Šë‹¤', 'ì ', 'ì§œë‹¤', 'ë¦¬', 'ë”', 'ë”ìš±', 'ì•„ë‹ˆë‹¤']]




```python
x_train_raw = x_train
x_test_raw = x_test
```

### **Tokenizer ë¡œ ë‹¨ì–´ì‚¬ì „ ë§Œë“¤ê¸° & ì •ìˆ˜ Encoding**
- ë‹¨ì–´ì‚¬ì „ì€ train_data ë¡œ ë§Œë“¤ì–´ì•¼ í•¨


```python
tokenizer = Tokenizer()
tokenizer.fit_on_texts(x_train)  

tokenizer.word_index      # ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë‹¨ì–´ë³„ë¡œ ì‘ì€ ìˆ˜ì˜ ì–‘ì˜ ì •ìˆ˜ë¥¼ ë¶€ì—¬í•¨
len(tokenizer.word_index)
```




    75147




```python
threshold = 4
total_cnt = len(tokenizer.word_index) # ë‹¨ì–´ì˜ ìˆ˜
rare_cnt = 0      
total_freq = 0     
rare_freq = 0       

for key, value in tokenizer.word_counts.items():
    total_freq += value

    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :',total_cnt)
print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))
print("ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:", (rare_cnt / total_cnt)*100)
print("ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:", (rare_freq / total_freq)*100)
```

    ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° : 75147
    ë“±ì¥ ë¹ˆë„ê°€ 3ë²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: 48485
    ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨: 64.52020706082745
    ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨: 1.5172733686959021
    


```python
vocab_size = total_cnt - rare_cnt + 1
vocab_size
```




    26663




```python
tokenizer = Tokenizer(vocab_size)
tokenizer.fit_on_texts(x_train)   # 0ë²ˆ ë‹¨ì–´ ~ 19395ë²ˆ ë‹¨ì–´ê¹Œì§€ë§Œ ì‚¬ìš©
```


```python
# Referring to word_index, text --> numeric sequences  

x_train = tokenizer.texts_to_sequences(x_train)
x_test = tokenizer.texts_to_sequences(x_test)
```


```python
print(x_train[:3])
print(x_test[:3])
```

    [[64, 1007, 30, 364, 1329], [897, 1127, 69, 1146, 4, 236, 1843, 99, 1259, 328, 15], [354, 4758, 3973, 4417, 1, 83, 12]]
    [[668, 160], [86, 246, 101, 375, 15, 53, 254, 770, 34, 1074, 20], [151, 15, 117, 861, 140, 102, 68, 315, 176]]
    


```python
len(x_test)
# len(x_test)
```




    118295




```python
y_train = np.array(train_data['label'])
y_test = np.array(test_data['label'])
```


```python
# Remove empty sequence in x_train, which means those seq are composed of only the words that are under set threshold

drop_target = [idx for idx, sentence in enumerate(x_train) if len(sentence) < 1]
x_train = np.delete(x_train, drop_target, axis=0)       # np.delete(array, delete_target_idx, axis=0(row) or 1(col))
y_train = np.delete(y_train, drop_target, axis=0)
```


```python
print(len(x_train))
print(len(y_train))       # 143620 --> 143376
```

    372193
    372193
    

### **Padding**
- ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ X_trainê³¼ X_testì˜ ëª¨ë“  ìƒ˜í”Œì˜ ê¸¸ì´ë¥¼ íŠ¹ì • ê¸¸ì´ë¡œ ë™ì¼í•˜ê²Œ ë§ì¶¤
- padding_len 


```python
print('ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ :', max(map(len, x_train)))
print('ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ :',sum(map(len, x_train))/len(x_train))
plt.hist([len(s) for s in x_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()
```

    ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ : 68
    ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ : 11.614372113392783
    


    
<img src="https://user-images.githubusercontent.com/92680829/144726149-7a4b8670-2bc3-4a47-83e3-e505aafb37ce.png" >
    



```python
def find_padding_len(set_len):
    cnt = 0
    for l in list(map(len, x_train)):
        if l <= set_len:
            cnt += 1
    return (cnt/len(x_train))*100

for set_len in range(10, 50, 5):
    tmp = find_padding_len(set_len)
    print("ê¸¸ì´ {0} : {1}%".format(set_len, tmp))
```

    ê¸¸ì´ 10 : 59.378870639694995%
    ê¸¸ì´ 15 : 75.60620430798%
    ê¸¸ì´ 20 : 84.42609076473765%
    ê¸¸ì´ 25 : 90.24833889944196%
    ê¸¸ì´ 30 : 94.36824443232409%
    ê¸¸ì´ 35 : 97.26539725357543%
    ê¸¸ì´ 40 : 99.18187606967352%
    ê¸¸ì´ 45 : 99.86969126232896%
    


```python
padding_len = 35
```


```python
x_train = pad_sequences(x_train, maxlen = padding_len, truncating='post', padding='post')
x_test = pad_sequences(x_test, maxlen = padding_len, truncating='post', padding='post')
```


```python
x_train
```




    array([[   64,  1007,    30, ...,     0,     0,     0],
           [  897,  1127,    69, ...,     0,     0,     0],
           [  354,  4758,  3973, ...,     0,     0,     0],
           ...,
           [  470,   824,    51, ...,     0,     0,     0],
           [  688,  1106,     8, ...,     0,     0,     0],
           [19611,  5647,  2779, ...,     0,     0,     0]], dtype=int32)




```python
x_train.shape       # padding ì™„ë£Œ
```




    (372193, 35)



---

## **LSTM Training with DATASET**

### **Import Modules**


```python
from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
```


```python
emb_dim = 100
cur_dim = 26663     # vocab size
padding_len = 35
```

### **Modeling**


```python
model = Sequential([
    Embedding(cur_dim, emb_dim, input_length=padding_len),      
    Bidirectional(LSTM(128, return_sequences=True)),
    LSTM(32),
    Dense(1, activation='sigmoid')
])
```


```python
model.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     embedding_1 (Embedding)     (None, 35, 100)           2666300   
                                                                     
     bidirectional_1 (Bidirectio  (None, 35, 256)          234496    
     nal)                                                            
                                                                     
     lstm_3 (LSTM)               (None, 32)                36992     
                                                                     
     dense_1 (Dense)             (None, 1)                 33        
                                                                     
    =================================================================
    Total params: 2,937,821
    Trainable params: 2,937,821
    Non-trainable params: 0
    _________________________________________________________________
    


```python
import pydot
import graphviz
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))
```

<img src="https://user-images.githubusercontent.com/92680829/144725961-81ab2635-f689-4fa6-8d6e-edef15d84330.png" >


```python
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
```


```python
checkpoint_path = 'my_checkpoint.ckpt'
checkpoint = ModelCheckpoint(checkpoint_path, 
                             save_weights_only=True, 
                             save_best_only=True, 
                             monitor='val_loss',
                             verbose=1)
```


```python
earlystop = EarlyStopping(monitor="val_loss", 
                          min_delta=0.001,        
                          patience=2)
```

### **Training**


```python
epochs=10
history = model.fit(x_train, y_train, 
                    validation_data=(x_test, y_test),
                    callbacks=[checkpoint, earlystop],
                    epochs=epochs)
```

    Epoch 1/10
    11631/11632 [============================>.] - ETA: 0s - loss: 0.3492 - acc: 0.8493
    Epoch 00001: val_loss improved from inf to 0.33655, saving model to my_checkpoint.ckpt
    11632/11632 [==============================] - 428s 37ms/step - loss: 0.3492 - acc: 0.8493 - val_loss: 0.3366 - val_acc: 0.8570
    Epoch 2/10
    11632/11632 [==============================] - ETA: 0s - loss: 0.2986 - acc: 0.8740
    Epoch 00002: val_loss improved from 0.33655 to 0.32506, saving model to my_checkpoint.ckpt
    11632/11632 [==============================] - 429s 37ms/step - loss: 0.2986 - acc: 0.8740 - val_loss: 0.3251 - val_acc: 0.8614
    Epoch 3/10
    11632/11632 [==============================] - ETA: 0s - loss: 0.2634 - acc: 0.8908
    Epoch 00003: val_loss did not improve from 0.32506
    11632/11632 [==============================] - 429s 37ms/step - loss: 0.2634 - acc: 0.8908 - val_loss: 0.3457 - val_acc: 0.8606
    


```python
model.load_weights(checkpoint_path)
```




    <tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe9f7682b10>




```python
loss, acc = model.evaluate(x_test, y_test)
```

    3697/3697 [==============================] - 57s 13ms/step - loss: 0.3251 - acc: 0.8614
    


```python
model.save_weights('lstm_model.h5')
```


```python
model.save("model")
```

    WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 15). These functions will not be directly callable after loading.
    

    INFO:tensorflow:Assets written to: model/assets
    

    INFO:tensorflow:Assets written to: model/assets
    WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f4102637a50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.
    WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f4102700d90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.
    WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f41027710d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.
    

### **ëª¨ë¸ ì„±ëŠ¥**

```python
print("Model Loss : {0}\nModel Accuracy : {1}".format(np.round(loss, 4), np.round(acc, 4)))
```

    Model Loss : 0.3251
    Model Accuracy : 0.8614
    

---

## **í•™ìŠµëœ ëª¨ë¸ë¡œ Naver, Twitter ì—ì„œ í¬ë¡¤ë§í•œ ê²Œì‹œê¸€ ê°ì •ë¶„ì„ í›„ ì—‘ì…€íŒŒì¼ ìƒì„±**

```python
import pandas as pd
import numpy as np
from tqdm import tqdm
```


```python
naver_emo = pd.read_excel("/content/NAVER_ì½”ë¡œë‚˜, ê°ì •.xlsx")
naver_mood = pd.read_excel("/content/NAVER_ì½”ë¡œë‚˜, ê¸°ë¶„.xlsx")
naver_daily = pd.read_excel("/content/NAVER_ì½”ë¡œë‚˜, ì¼ìƒ.xlsx")

twitter_emo = pd.read_excel("/content/Tweets_ì½”ë¡œë‚˜, ê°ì •.xlsx")
twitter_mood = pd.read_excel("/content/Tweets_ì½”ë¡œë‚˜, ê¸°ë¶„.xlsx")
twitter_daily = pd.read_excel("/content/Tweets_ì½”ë¡œë‚˜, ì¼ìƒ.xlsx")
```


```python
naver_emo.rename(columns={'Tweets' : 'Contents'}, inplace=True)
naver_mood.rename(columns={'Tweets' : 'Contents'}, inplace=True)
naver_daily.rename(columns={'Tweets' : 'Contents'}, inplace=True)
twitter_emo.rename(columns={'Tweets' : 'Contents'}, inplace=True)
twitter_mood.rename(columns={'Tweets' : 'Contents'}, inplace=True)
twitter_daily.rename(columns={'Tweets' : 'Contents'}, inplace=True)
```


```python
naver_emo['Positive'] = 0
naver_emo['Negative'] = 0

naver_mood['Positive'] = 0
naver_mood['Negative'] = 0

naver_daily['Positive'] = 0
naver_daily['Negative'] = 0

twitter_emo['Positive'] = 0
twitter_emo['Negative'] = 0

twitter_mood['Positive'] = 0
twitter_mood['Negative'] = 0

twitter_daily['Positive'] = 0
twitter_daily['Negative'] = 0
```


```python
naver_emo
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Date</th>
      <th>Weekly Frequency</th>
      <th>Contents</th>
      <th>Positive</th>
      <th>Negative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>20200101</td>
      <td>146</td>
      <td>['ë…„ ì›” ì¼ ë¹„ëŒ€ë©´ì˜¨ë¼ì¸ê°œê°•ì„±ê²½ì ê°ì •ì½”ì¹­ì´ì‹ ëŒ€í•™êµí‰ìƒêµìœ¡ì› ì „ë¬¸êµìœ¡ì•„ì¹´ë°ë¯¸', '...</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20200107</td>
      <td>166</td>
      <td>['ìŠ¤ìŠ¤ë¡œ ë³€í™”ë¥¼ ì´ëŒì–´ë‚´ëŠ” ì…€í”„ì½”ì¹­ ì¼ ê°ì •ì¼ê¸° í”„ë¡œì íŠ¸ ê¸° ëª¨ì§‘ ì¼ì • ì—°ê¸°', ...</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20200114</td>
      <td>164</td>
      <td>['ì™œ ë‚˜ëŠ” í•­ìƒ ë¨¹ê³ ë‚˜ì„œ í›„íšŒí• ê¹Œ ì›ë°ì´í´ë˜ì“° ì°¨ ì˜¤í”ˆ ì½”ë¡œë‚˜ ì‚¬íƒœë¡œ ì ì • ë³´ë¥˜'...</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20200121</td>
      <td>890</td>
      <td>[' ìš°í•œ ë°œìƒ ì‹ ì¢… ì½”ë¡œë‚˜ ë°”ì´ëŸ¬ìŠ¤ íë ´ ì§ˆí™˜ í™•ì‚°ê³¼ ì£¼ì‹ ì‹œì¥', 'ì‹ ì¢…ì½”ë¡œë‚˜ë°”...</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20200201</td>
      <td>896</td>
      <td>[' ê°ì •ì¼ê¸°  ì½”ë¡œë‚˜ ë°”ì´ëŸ¬ìŠ¤', ' ë¯¸êµ­ ì¦ì‹œì˜ ì´ê²© ì¡°ì • ê·¸ë¦¬ê³  ì‹ ì¢… ì½”ë¡œë‚˜ ...</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>83</th>
      <td>20210921</td>
      <td>896</td>
      <td>[' ìœ„ë“œì½”ë¡œë‚˜ ê¸¸ì–´ì§ˆìˆ˜ë¡ ë‚´ ê°ì •ì„ í•´ì¹˜ëŠ” ì˜í–¥ë“¤', 'ìƒì¥ì¼ì§€ ê°ì •ì„ ê¸°ë¡í•˜ëŠ” ...</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>84</th>
      <td>20211001</td>
      <td>880</td>
      <td>['ì•µì½œì „ì‹œ í™”ë¬´ì‹­ì¼í™ ì „ì‹œ  ì½”ë¡œë‚˜ ë¸”ë£¨ì‹œëŒ€ ì¸ ì‘ê°€ì˜ ê°ì •ê³¼ ì‚¬ìœ  ì•„íŠ¸í¬ìŠ¤í„° ì „...</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>85</th>
      <td>20211007</td>
      <td>898</td>
      <td>['ì•µì½œì „ì‹œ í™”ë¬´ì‹­ì¼í™ ì „ì‹œ  ì½”ë¡œë‚˜ ë¸”ë£¨ì‹œëŒ€ ì¸ ì‘ê°€ì˜ ê°ì •ê³¼ ì‚¬ìœ  ì•„íŠ¸í¬ìŠ¤í„° ì „...</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>86</th>
      <td>20211014</td>
      <td>894</td>
      <td>['ì¼ë³¸ì¸  ì¤‘êµ­ ì‹«ë‹¤ì½”ë¡œë‚˜ ì†  êµ­ë¯¼ê°ì • ì—­ëŒ€ ìµœì•…', 'ì¼ë³¸ì¸  ì¤‘êµ­ ì‹«ë‹¤ì½”ë¡œë‚˜...</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>87</th>
      <td>20211021</td>
      <td>894</td>
      <td>['ì¼ë³¸ì¸  ì¤‘êµ­ ì‹«ë‹¤ì½”ë¡œë‚˜ ì†  êµ­ë¯¼ê°ì • ì—­ëŒ€ ìµœì•…', 'ì¼ë³¸ì¸  ì¤‘êµ­ ì‹«ë‹¤ì½”ë¡œë‚˜...</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>88 rows Ã— 5 columns</p>
</div>




```python
def get_sentiment(sen):
    sen = okt.morphs(sen, stem=True)
    sen = [word for word in sen if word not in stopwords]
    encoded = tokenizer.texts_to_sequences([sen])       # label encoding
    padded = pad_sequences(encoded, maxlen=padding_len)       # padding
    # print(padded)
    score = float(model.predict(padded))
    if score >= 0.65:
        return 1
    else:
        return -1
```


```python
pos_sen = []
neg_sen = []
```


```python
def measure_sentiment_for_each_df(df):
    for i, contents in tqdm(enumerate(df['Contents'])):
        content_list = contents.split(',')

        for sen in content_list:
            sen = re.sub(r"[^ã„±-ã…ã…-ã…£ê°€-í£ ]", "", sen)
            tmp_res = get_sentiment(sen)
            if tmp_res == 1:
                df['Positive'][i] += 1
                pos_sen.append(sen)
            else:
                df['Negative'][i] += 1
                neg_sen.append(sen)
```


```python
pd.options.mode.chained_assignment = None
```


```python
df_list = [twitter_mood, twitter_daily]
df_name_list = ["twitter_mood", "twitter_daily"]
# f"{df_name_list[3]}"
```


```python
i = 0
for df in tqdm(df_list):
    measure_sentiment_for_each_df(df)
    df.to_excel(f"{df_name_list[i]}.xlsx")
    keywords = {'pos' : [pos_sen], 'neg': [neg_sen]}
    keywords_df = pd.DataFrame(keywords)
    keywords_df.to_excel("categorized_keywords.xlsx")
    i += 1
```

      0%|          | 0/2 [00:00<?, ?it/s]
    0it [00:00, ?it/s][A
    1it [00:01,  1.30s/it][A
    3it [00:01,  2.60it/s][A
    4it [00:03,  1.16it/s][A
    5it [00:12,  3.78s/it][A
    6it [00:21,  5.31s/it][A
    7it [00:33,  7.36s/it][A
    8it [01:01, 13.89s/it][A
    9it [01:31, 18.66s/it][A
    10it [01:59, 21.54s/it][A
    11it [02:26, 23.26s/it][A
    12it [02:53, 24.31s/it][A
    13it [03:14, 23.56s/it][A
    14it [03:36, 23.08s/it][A
    15it [03:54, 21.41s/it][A
    16it [04:11, 20.03s/it][A
    17it [04:21, 17.03s/it][A
    18it [04:39, 17.24s/it][A
    19it [04:52, 16.00s/it][A
    20it [05:08, 16.17s/it][A
    21it [05:17, 14.08s/it][A
    22it [05:29, 13.37s/it][A
    23it [05:38, 12.14s/it][A
    24it [05:47, 11.10s/it][A
    25it [05:55, 10.01s/it][A
    26it [06:02,  9.13s/it][A
    27it [06:08,  8.37s/it][A
    28it [06:14,  7.62s/it][A
    29it [06:19,  6.78s/it][A
    30it [06:25,  6.57s/it][A
    31it [06:48, 11.43s/it][A
    32it [07:15, 16.07s/it][A
    33it [07:31, 16.04s/it][A
    34it [07:46, 15.73s/it][A
    35it [07:57, 14.47s/it][A
    36it [08:08, 13.33s/it][A
    37it [08:15, 11.53s/it][A
    38it [08:24, 10.64s/it][A
    39it [08:30,  9.43s/it][A
    40it [08:37,  8.52s/it][A
    41it [08:42,  7.54s/it][A
    42it [08:46,  6.46s/it][A
    43it [08:53,  6.65s/it][A
    44it [09:04,  7.94s/it][A
    45it [09:17,  9.46s/it][A
    46it [09:32, 11.26s/it][A
    47it [09:47, 12.12s/it][A
    48it [10:01, 12.81s/it][A
    49it [10:10, 11.70s/it][A
    50it [10:18, 10.69s/it][A
    51it [10:26,  9.88s/it][A
    52it [10:35,  9.49s/it][A
    53it [10:42,  8.60s/it][A
    54it [10:47,  7.74s/it][A
    55it [10:53,  7.07s/it][A
    56it [10:59,  6.78s/it][A
    57it [11:04,  6.32s/it][A
    58it [11:08,  5.72s/it][A
    59it [11:15,  5.83s/it][A
    60it [11:20,  5.75s/it][A
    61it [11:24,  5.31s/it][A
    62it [11:31,  5.56s/it][A
    63it [11:37,  5.78s/it][A
    64it [11:43,  6.01s/it][A
    65it [11:48,  5.49s/it][A
    66it [11:54,  5.69s/it][A
    67it [12:00,  5.79s/it][A
    68it [12:05,  5.56s/it][A
    69it [12:10,  5.55s/it][A
    70it [12:15,  5.19s/it][A
    71it [12:21,  5.40s/it][A
    72it [12:24,  4.67s/it][A
    73it [12:28,  4.73s/it][A
    74it [12:45,  8.27s/it][A
    75it [12:57,  9.24s/it][A
    76it [13:06,  9.34s/it][A
    77it [13:13,  8.73s/it][A
    78it [13:24,  9.43s/it][A
    79it [13:33,  9.05s/it][A
    80it [13:43,  9.37s/it][A
    81it [13:49,  8.32s/it][A
    82it [13:57,  8.32s/it][A
    83it [14:06,  8.68s/it][A
    84it [14:15,  8.63s/it][A
    85it [14:22,  8.04s/it][A
    86it [14:29,  7.95s/it][A
    87it [14:36,  7.61s/it][A
    88it [14:44, 10.05s/it]
     50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [14:45<14:45, 885.01s/it]
    0it [00:00, ?it/s][A
    2it [00:00, 17.78it/s][A
    4it [00:00,  5.88it/s][A
    5it [00:04,  1.17s/it][A
    6it [00:08,  2.15s/it][A
    7it [00:14,  3.20s/it][A
    8it [00:40,  9.99s/it][A
    9it [01:08, 15.57s/it][A
    10it [01:34, 18.69s/it][A
    11it [02:02, 21.36s/it][A
    12it [02:30, 23.27s/it][A
    13it [02:59, 24.97s/it][A
    14it [03:25, 25.44s/it][A
    15it [03:45, 23.84s/it][A
    16it [04:01, 21.49s/it][A
    17it [04:17, 19.61s/it][A
    18it [04:46, 22.59s/it][A
    19it [05:02, 20.61s/it][A
    20it [05:34, 24.01s/it][A
    21it [05:43, 19.65s/it][A
    22it [05:54, 16.91s/it][A
    23it [06:04, 14.80s/it][A
    24it [06:13, 13.21s/it][A
    25it [06:22, 11.82s/it][A
    26it [06:32, 11.15s/it][A
    27it [06:40, 10.44s/it][A
    28it [07:07, 15.23s/it][A
    29it [07:14, 12.80s/it][A
    30it [07:21, 11.17s/it][A
    31it [07:51, 16.76s/it][A
    32it [08:17, 19.48s/it][A
    33it [08:35, 19.11s/it][A
    34it [08:52, 18.49s/it][A
    35it [09:05, 16.70s/it][A
    36it [09:21, 16.48s/it][A
    37it [09:28, 13.87s/it][A
    38it [09:29,  9.93s/it][A
    39it [09:38,  9.58s/it][A
    40it [10:03, 14.35s/it][A
    41it [10:30, 18.05s/it][A
    42it [10:58, 20.92s/it][A
    43it [11:11, 18.55s/it][A
    44it [11:29, 18.62s/it][A
    45it [11:45, 17.69s/it][A
    46it [12:01, 17.25s/it][A
    47it [12:18, 17.02s/it][A
    48it [12:31, 15.91s/it][A
    49it [12:46, 15.74s/it][A
    50it [13:00, 15.08s/it][A
    51it [13:11, 13.93s/it][A
    52it [13:21, 12.81s/it][A
    53it [13:30, 11.41s/it][A
    54it [13:40, 11.00s/it][A
    55it [13:50, 10.74s/it][A
    56it [14:05, 12.17s/it][A
    57it [14:13, 10.75s/it][A
    58it [14:24, 10.85s/it][A
    59it [14:35, 10.88s/it][A
    60it [14:47, 11.18s/it][A
    61it [15:00, 11.95s/it][A
    62it [15:11, 11.65s/it][A
    63it [15:22, 11.40s/it][A
    64it [15:25,  8.81s/it][A
    65it [15:34,  8.80s/it][A
    66it [15:47, 10.18s/it][A
    67it [15:55,  9.53s/it][A
    68it [16:03,  8.98s/it][A
    69it [16:10,  8.34s/it][A
    70it [16:17,  7.99s/it][A
    71it [16:24,  7.72s/it][A
    72it [16:31,  7.54s/it][A
    73it [16:36,  6.79s/it][A
    74it [16:42,  6.55s/it][A
    75it [16:52,  7.58s/it][A
    76it [17:04,  8.93s/it][A
    77it [17:12,  8.71s/it][A
    78it [17:21,  8.64s/it][A
    79it [17:36, 10.58s/it][A
    80it [17:51, 12.05s/it][A
    81it [18:11, 14.23s/it][A
    82it [18:33, 16.66s/it][A
    83it [18:45, 15.29s/it][A
    84it [19:15, 19.84s/it][A
    85it [19:41, 21.51s/it][A
    86it [20:13, 14.11s/it]
    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [34:59<00:00, 1049.80s/it]
    


```python
naver_emo
# naver_mood
# naver_daily

# twitter_emo
# twitter_mood
twitter_daily.to_excel("twitter_daily_2.xlsx")
```
