---
layout: post
title : "[Paper Review & Implementation] Hyperbolic Neural Networks - Part 2 : Implementing Hyperbolic Graph Convolutional Networks (HGCN)"
img: papers/gnn/hnn2.png
categories: [papers-gnn]  
tag : [Differential Geometry, Hyperbolic Neural Network, Hyperbolic Geometry]
toc : true2020
toc_sticky : true
---

## **Outlines**

- [**Reference**](#reference)

- [**0. HGCN : Implementing GCN in Hyperbolic Space**](#0-hgcn--implementing-gcn-in-hyperbolic-space)
- [**1. Hyperboloid Model**](#1-hyperboloid-model)
        <!-- -> Relation with Poincare Disk Model -->
- [**2. Generalization of Feature Transformation**](#2-generalization-of-feature-transformation)
    - [**2.1. Matrix Multiplication**](#21-encoder-decoder-architecture-for-semi-supervised-learning)
    - [**2.2. Bias Addition**](#22-hyperbolic-space-as-smooth-riemannian-manifold) 
    - [**2.3. Non-Linear Activation**](#23-non-linear-activation) 

- [**3. Attention-Based Aggregation**](#3-attention-based-aggregation)
    - [**3.1. Getting Attention Score**](#31-encoder-decoder-architecture-for-semi-supervised-learning)
    - [**3.2. Aggregation**](#32-hyperbolic-space-as-smooth-riemannian-manifold) 

- [**4. Trainable Curvature**](#4-trainable-curvature) 
    - [**4.1. Features Embedded in Hyperboloid of Differing Curvature**](#41-features-embedded-in-hyperboloid-of-differing-curvature)
    - [**4.2. Fermi-Dirac Decoder and Hyperparameters**](#42-fermi-dirac-decoder-and-hyperparameters)

- [**5. HGCN Architecture and Comparison to GNN-Based Models**](#5-hgcn-architecture-and-comparison-to-gnn-based-models) 


<br/>

## **Reference**

<br/>

- [**Hyperbolic Deep Neural Networks: A Survey, Peng et al, 2021**](https://arxiv.org/abs/2101.04562){:target="_blank"}
- [**Hyperbolic Graph Convolutional Neural Networks, Chami et al, 2019**](https://arxiv.org/abs/1910.12933){:target="_blank"}
- [**Official Impelementation Code for HGCN in PyTorch**](https://github.com/HazyResearch/hgcn/tree/master){:target="_blank"}


<br/>

## **0. HGCN : Implementing GCN in Hyperbolic Space**

<br/>

- Real-world graphs, such as molecules and social networks, have highly complex hierarchical structures that expands exponentially in space.

- Embedding these tree-like structures into Euclidean space that only grows polynomially can cause large distortion to the graph representation. 

- Hyperbolic space, with its intrinsic property to grow exponentially, allows optimal embedding representation of real-world hierarchical data. 

- HGCN (Hyperbolic Graph Convolutional Networks) successfully combines this geometrical advantage of hyperbolic space to its great inductive capacity to capture neighboring node features and shows remarkably improved performance on both link prediction and node classification tasks compared to other GNN-based baselines. 

&emsp; **Figure 3. Visualization of Euclidean (Left) and Hyperbolic (Right) Embeddings on Poincare Model**

&emsp;&emsp;<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/cba7d5fc-d213-4db6-8422-d848dc48b82d" width="500">

- HGCN shows better class separation (indicated by different colors)

- In summary, HGCN claims to make three major contributions.

    1. Derive the core operations of GCN in the hyperboloid model to transform Euclidean input features onto the hyperbolic space. 

    2. Utilize attention based aggregation method that improves the expressivness of the networks by inductively reflecting the hierarchical relations between neighboring nodes. 

    3. Introduce trainable curvature for each layer of the neural networks, which facilitates an optimization of the model by learning the right scale of embeddings at each layer.


<br/>

## **1. Generalization of Feature Transformation**

<br/>

- Part 1 describes the preliminaries on differential geometry, which are necessary to further implement basic arithmetic operations in the space other than Euclidean. 

- For this time, using all them, let's implement some fundamental matrix based operations for neural networks. 

