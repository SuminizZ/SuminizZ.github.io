---
layout: post
title : "[GNN] Graph Convolutional Networks (GCNs) - Part 2 : Towards Spatial Graph Convolution (ChebNet, GCN)"
img: papers/gnn/gcn2.png
categories: [papers-gnn]  
tag : [Paper Review, GNN, GCN, ChebNet]
toc : true2020
toc_sticky : true
---

## **Outlines**
- [**Reference**](#reference)
- [**0. Spectral Convolution Operation on Graphs**](#0-spectral-convolution-operation-on-graphs)
- [**1. Fourier Transform on Graph**](#1-fourier-transform-on-graph)
- [**2. Laplacian(Laplace Operator) and Laplacian Matrix**](#2-laplacianlaplace-operator-and-laplacian-matrix)
- [**3. Graph Fourier Transform**](#3-graph-fourier-transform)
    - [**3.1. Laplacian Quadratic Form : Meaure of Smoothness of Graph**](#31-laplacian-quadratic-form--meaure-of-smoothness-of-graph)
    - [**3.2. Spectral Filtering with Graph Fourier Transform (GFT)**](#32-spectral-filtering-with-graph-fourier-transform-gft)    
- [**4. Spectral Graph Convolution**](#4-spectral-graph-convolution)
    - [**4.1. Learnable Graph Filters**](#41-learnable-graph-filters)
    - [**4.2. Convolution Theorem**](#42-convolution-theorem) 

<br/>

## **Reference**

<br/>

- [**Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, Defferrard et al, 2016**](https://arxiv.org/pdf/1606.09375.pdf){:target="_blank"}
- [**Semi-Supervised Classification with Graph Convolutional Networks, Kipf et al, 2017**](https://arxiv.org/pdf/1609.02907.pdf){:target="_blank"}
- [**Spectral GCN**](https://tootouch.github.io/research/spectral_gcn/){:target="_blank"}
- [**ChebNet PyTorch Implementation**](https://github.com/dsgiitr/graph_nets/blob/master/ChebNet/Chebnet_Blog%2BCode.ipynb){:target="_blank"}

<br/>

## **1. Spectral Convolution**

<br/>

- Standard CNN captures local features and composes them into a series of hierarchical patterns with greater semantic context by using shift invariant compactly supported filters.

- Spectral covolution is able to generalize the ability of CNN to graphs by transforming the spatial structure of graph into spectral domain where one can filter the desired frequency component of the graph signal. 

    &emsp;&emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/560728ef-a1e5-4175-9a28-416037130850" width="235">

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/7a4714ec-6a5d-4555-a4fb-71e6152159a8" width="450">

- However, spectral convolution has **several limitations**

    1. High computational cost
        
        - Needs to operate eigen-decomposition of graph laplacian to generate graph fourier basis, which is computationally expensive. 

        - Costs $\large O(n^{2})$ due to the two-fold multiplication of the fourier basis matrix with input features.

    2. Lack of spatial locality

        - Lacks an explicit notion of spatial locality, which is a fundamental aspect of traditional CNNs. 

        - Filters the entire graph simultaneously, making it challenging to capture local neighborhood information directly. 

    3. Over-smoothing problem 

        - Involves low-pass filtering, and as layers are stacked in deep graph networks, it can lead to over-smoothing of node representations. 

        - Loss of discriminative power to distinguish between differnet nodes.

<br/>

## **2. ChebNet : Re-Formulation of Spectral Filtering with K-Hops Localized Filters**

<br/>

- ChebNet addresses the major bottlenecks of generalizing CNN to graphs, defining localized graph filters and reducing computational complexity.

- Further, it introduces graph-coarsening and pooling steps to simplify and downsize the graph structure while preserving the significant information about the graph. 

    - This enables the multi-scale approach where single architecture, chebnet, can be applied to multiple graph resolutions.

        <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/07920fd9-8b75-4c3d-8bcc-978fcf929e81" width="750">

<br/>

### **2.1. K-Localized Spectral Filters**

<br/>

- **Graph fourirer transform** with normalized laplacian

    &emsp;<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/ffbc4b1a-1000-4529-b773-a331e7695549" width="250">

    &emsp;<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/fcd5e728-4ab1-485d-aab0-c065962ee39b" width="300">

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/4962c5fc-2258-4bf2-a250-3e3e861b4222" width="450">

<br/>

- By utilizing $Kth$ order polynomials of the laplacian as the graph filters, ChebNet limits the receptive field of the filters to the nodes that are at maximum K steps away from the central node. 

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/256dc16f-8571-43aa-bbe6-373fa3fb95f5" width="500">


- Then, **how can Kth order polynomials of laplacian can capture the nodes that are exactly K-localized** from the target node ? 

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/57beb604-9209-4e0b-8b1b-872f30277c9a" width="900">


<br/>

### **2.2. Fast and Stable Filtering : Recursive Formulation of Chebyshev Polynomials**

<br/>

- Monomial (Higher-order polynomial) basis is not orthogonal and becomes more susceptible to large oscillations as the degree of polynomial increases. 

- To overcome this, one need to replace monomials with the orthogonal polynomials computed based on the stable recurrence relation. 

<br/>

#### **2.2.1. Chebyshev Polynomials**

<br/>
- Chebyshev polynomials of the first kind ($\large T_{n}$)

    - $\large T_{n+1}(x)\, + T_{n-1}(x)\, = 2x \times T_{n}(x)\,$

    - $\large T_{n}(cos\theta)\, = cos(n\theta)$

- Chebyshev polynomials of the second kind ($\large U_{n}$)

    - $\large U_{n+1}(x)\, + U_{n-1}(x)\, = 2x \times U_{n}(x)\,$

    - $\large U_{n}(cos\theta)\, = \frac{sin((n+1)\theta)}{sin\theta}$

- **Derivation**

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/f82d5db6-34c4-4241-80a5-72ab699ec3d7" width="1100">

- Chebyshev expansion has been traditionally used in graph signal processing to approximate kernels due to its stability.

- **Key properties** of chebyshev polynomials : 
    
    - **Orthogonality Chebyshev polynomials**

        <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/4ab402e9-b2db-4009-bcee-799360bb1d88" width="730">

        - $\large T_{n}(x)$ and $\large T_{m}(x)$ are orthogonal to each other with respect to $\large \frac{dx}{\sqrt{1\,-\,x^{2}}}$
        
        <br/>
    
    - **MinMax Property** : Any point lies within the interval $\large [-1, 1]$

    - **Recurrence relation** : Allows efficient computation of higher-order polynomials based on the lower-order ones.

<br/>

#### **2.2.2. Parametrization of Filters with Chebyshev Expansion**

<br/>

&emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/8d09be0d-06a7-43c1-852e-4163a8789ca1" width="200">&emsp;→&emsp;<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/a334cd54-e9bc-4f0e-8024-020191a200fa" width="200"> 

- While satisfying the recurrence relation, <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/212b423e-baf8-4dae-9eb0-9dc999b7fce6" width="300"> with $\large T_{0} = 1$ and $\large T_{1} = x$

- Note that these polynomials form an orthogonal basis w.r.t $\large \frac{dx}{\sqrt{1\,-\,x^{2}}}$

- Here, $\large \tilde{\Lambda} \, = \, 2\Lambda / \lambda_{max} \, - \, I_{n}$, a diagonal matrix scaled to lie within $\large [-1, 1]$

- Re-formulate the spectral convolution with chebyshev polynomials 

    &emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/86a9ea0b-fabd-4db2-a3d8-9a50e74ad5b9" width="320"> → <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/bdfc1cbf-52d3-4d1f-a82c-6bbb4167f9bd" width="300">
    
    - <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/560889c3-8e16-4d8b-bc2f-8e1c66f9b682" width="200">

    - where <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/a857c64e-dc0d-40d0-bca7-361fd4b5dc38" width="220"> with <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/ce882274-5008-460c-8e97-7f7e5d92aa24" width="200">

    - <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/39243a69-b84f-404f-9027-55287d094c02" width="300">

- No need to operate eigen-decomposition of graph laplacian and $\large O(n^{2})$ computations for mulitiplication with eigenvector matrix.

- The entire filtering operation costs $\large O(K\|\xi\|)$ as it requires K times computation of highly sparse matrix L with the number of edges $\large \|\xi\|$, which is far more efficient than $\large O(n^{2})$.

<br/>

### **2.3. Learning Filters**

<br/>

- **Forwrad Pass**

    - $j^{th}$ output feature map of the sample $s$ of batch size $S$ 

        $\large y_{s, j} \, = \, \sum_{i=1}^{F_{in}} \, g_{\theta_{i, j}}(L)\,x_{s, i}$ &emsp;where&emsp; $\large y_{s, j}, x_{s, i} \in \mathbb{R}^{n}$ 

    - $\large \theta$ : $\large F_{in} \times F_{out}$ vectors with each $\large \theta_{i, j} \, \in \, \mathbb{R}^{K}$

    - $\large y_{s} \,\in \, \mathbb{R}^{n \times F_{out}}$

- **Backward Pass**

    - Through the backpropagation, need to compute two gradients of the mini-batch loss $\large E$ with respect to $\large \theta$ and $\large x$.

        <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/04675092-35a1-4d47-8cf6-c5971c753079" width="750">


<br/>

### **2.4. Group Coarsening and Fast Pooling**

<br/>

- **Graph coarsening** 

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/c3606f24-0a0e-4398-89eb-f3d07ad17c4b" width="420">
    
    - Uses the Graclus multilevel clustering algorithm.

    - Done for multiple layers to perform multi-scale clustering where similar vertices are clustered together.

    - Reduces the size of the graph while preserving the meaningful information of the graph.

- **Efficient Pooling**

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/96e45335-b1b5-4534-b8b3-e859578fe835" width="420">

    - Rearrange the vertices to make the pooling operation as efficient as 1D-pooling

        1. Create a balanced binary tree 

        2. Rearragne the vertices 

- To sum up, ChebNet enables the **application of K-localized fiters on graphs** to capture spatial information and **improve computational efficiency** of convolution operation by removing the eigen-decomposition step and **parameterizing the localized graph filters using chebyshev expansion**.

<br/>

## **3. GCN : Layer-Wise Linear Formulation of GCN with K = 1**

<br/>

- GCN facilitates the transformation of spectral graph convolution into spatial graph convolution, boosting the application of CNN on graph-structured data.

    - It approximates the spectral graph convolution process as a layer-wise linear operation, limiting the convolution operation to first-order neighborhoods (K = 1).

    - Adds non-linearity after the linear filtering step and stack the multiple linear graph convolutional layers to build deeper models. 

- Key changes in GCN from ChebNet

    1. Limit K = 1 

    2. Constrain the number of parameters to address overfitting and reduce computations.

        - $\large \theta \,=\, \theta_{0} \,=\, - \theta_{1}$