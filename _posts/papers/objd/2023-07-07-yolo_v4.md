---
layout: post
title : "[Paper Review & Partial Implementation] YOLOv4: Optimal Speed and Accuracy of Object Detection (YOLOv4, 2020)"
img: papers/objd/yolo_v4.png
categories: [papers-objd]  
tag : [Paper Review, Object Detection, YOLO, YOLO V3, PyTorch]
toc : true
toc_sticky : true
---

## **Outlines**
- [**Reference**](#reference)
- [**Implementation with PyTorch**](#implementation-with-pytorch)
- [**YOLO : Single-State Object Detection**](#yolo--single-state-object-detection)
- [**YOLO Backbone Architecture : Feature Extractor and Region Proposals**](#yolo-backbone-architecture--feature-extractor-and-region-proposals)
- [**Customize PASCAL VOC Dataset**](#customize-pascal-voc-dataset)
- [**Process the Predicted Bounding Boxes from YOLO Model and Obtain Final Target Boxes**](#process-the-predicted-bounding-boxes-from-yolo-model-and-obtain-final-target-boxes)
    - [**Step 1 : Convert bounding boxes relative to cell ratio into entire image ratio**](#step-1--convert-bounding-boxes-relative-to-cell-ratio-into-entire-image-ratio)
    - [**Step 2 : Non-Maximal Suppression onto Predicted Bounding Boxes**](#step-2--non-maximal-suppression-onto-predicted-bounding-boxes)
- [**Calculate Mean Average Precison (mAP) between Predicted Boxes and Ground Truths**](#calculate-mean-average-precison-map-between-predicted-boxes-and-ground-truths)
- [**Computing Loss and Optimizing the Model**](#computing-loss-and-optimizing-the-model)
- [**Training Result & Display Predicted Anchors from Trained Model onto Image**](#training-result--display-predicted-anchors-from-trained-model-onto-image)

<br/>

## **Reference**

<br/>

- [**YOLOv4: Optimal Speed and Accuracy of Object Detection, Alexey Bochkovskiy, 2020**](https://arxiv.org/pdf/2004.10934.pdf){:target="_blank"}
- [**YOLO v4 논문(YOLOv4: Optimal Speed and Accuracy of Object Detection) 리뷰**](https://csm-kr.tistory.com/11){:target="_blank"}
- [**YOLO v4 PyTorch Implementation**](https://github.com/csm-kr/YOLOv4_pytorch/blob/master/model.py#L158){:target="_blank"}
- [**CSPNet: A New Backbone that can Enhance Learning Capability of CNN**](https://arxiv.org/pdf/1911.11929v1.pdf){:target="_blank"}

<br>

## **From YOLOv1 to YOLOv3**

<br>

#### **YOLO v1**:

- [**Review & Implementation**](https://suminizz.github.io/yolo_v1/){:target="_blank"}
- YOLO v1 introduced the concept of the YOLO architecture. It divided the input image into a grid and assigned each grid cell the responsibility of predicting bounding boxes and class probabilities.
- Predicted fixed number of bounding boxes per grid cell, leading to potential localization errors.
- Used a single scale feature map for detection, limiting its ability to detect multi-scale objects.

<br>

#### **YOLO v2 (YOLO9000)**:

&emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/fa3a0e0c-457c-4d4f-9106-264d7627a057" width="700">

<br>

- YOLO v2 made significant improvements over YOLO v1, addressing its limitations.
- Introduces a new architecture with anchor boxes, which allowed the network to predict bounding box offsets relative to these anchor boxes. This improved the localization accuracy and enabled better handling of objects of different scales and aspect ratios.
- Uses K-Means Clustering to determine the best number of anchor boxes that can optimize average IoU.
- Limits the range of the coordinates of predicted bounding box within 0 ~ 1 by taking logistic regression (sigmoid) to the regression output, acclerating convergence.
- Implements a multi-scale approach, where the networks combine features maps of different scales (26x26 and 13x13) using skip-connection. This facilitates the information flow from low-level (larger scale) to higher levels (smaller scale), enhancing the detection of objects of various sizes. 
- Trains the networks with multi-scale image inputs from 320 x 320 to 608 x 608.

<br>

#### **YOLO v3**:


&emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/9ed84298-f473-4985-98f0-c3f5cb7c83ba" width="700">

<br>

- YOLO v3 further improves upon the previous versions, focusing on better detection accuracy and handling a larger number of object categories.
- Utilizes the concept of feature pyramid networks (FPN) to handle feature maps at different scales more efficiently.
- Extracts multi-scaled feature maps (52, 26, 13) from different levels of feature pyramid and make separate predictions from each level.  
- Employs the use of the Darknet-53 backbone, a deep CNN architecture that enhanced the network's feature extraction capabilities.

<br>

## **YOLO v4 : Designing the Optimal Model**

<br>

&emsp; **Figure 1: Comparison of the proposed YOLOv4 and other state-of-the-art object detectors.**

&emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/4c783c3f-8968-4a67-be8e-bcde4283d42a" width="500">

<br>

- YOLOv4 performed extensive experiments to find out the optmial combinations of existing deep learning techniques for constructing each component (**backbone, head, neck**) of the architecture of YOLO v4.

- Additionally, YOLOv4 also focuses on two types of methods, **Bag of Freebies (BoF)** and **Bag of Specials (BoS)**, to further improve the object detection performance in terms of accuracy and speed.

- As a result, YOLOv4 runs twice faster than EfficientDet with comparable performance and shows significant improvement in terms of performance compared to previous YOLO v3. 

- While the paper introduced several other recent deep learning techniques, I will specifically focus on the techniques that are actually adopted in YOLOv4.

<br>

## **Object Detection Model**

<br>

<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/f4511112-23bc-4c40-a95a-07260927577a" width="400">

<br/>

### **BackBone : CSPDarkNet 53**

<br>

- Single architecture can show varying performance depeding on the selection of multiple sets of options, including the choice of dataset. 

- Paper thoroughly compared several backbone architectures to find the optimal balance among the input resolution, size of receptive field, depth convolutional layer, parameter number, number of outputs (channels) and the computational load. ( **FPS** : frames per second to measure the speed and efficiency of a model to process image data)

<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/d7d169f2-5981-4b80-9bd6-9b54338123cd" width="900">

<br>

- Paper explained that the CSPResNext50 is considerably better compared to CSPDarknet53 in terms of object classification on the ILSVRC2012 (ImageNet)
dataset. However, conversely, the CSPDarknet53 is better compared to CSPResNext50 in terms of detecting objects on the MS COCO dataset.

- Considiering all these options, CSPDarknet53 is selected as a backbone architecture of YOLOv4.

<br>

#### **Cross Stage Partial DenseNet (CSPDenseNet)**

<br>

&emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/c2d99210-68bb-443d-af9c-c94c8abf04a1" width="800">

<br>

- Basic structure of CSPDarknet is similar with the figure above. 

- Cross stage partial network (CSPNet) consists of base layer, two separate paths, and final transition layer that merges two paths together. 

<br>

&emsp;&emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/20d0126d-fd16-4da8-b91a-da417720608c" width="400">

<br>

- Part 1 path (left) is a simple convolution layer - batch normalization - activation structure.

- Part 2 path (right) is referred to as **Partial Dense Block**, which is a typical dense block that is composed of repeated dense layer and transition layer with a certain growth rate. Note the size of the output is same in both paths. 

- Transition 1 : transition layer only applied to partial dense block. 

- Transition 2 : receives combined outputs (channel-wise concatenation) from two paths as an input and performs transition. 

<br>

#### **PyTorch Implementation of Basic CSPBlock**

<br>

```python
class CSPBlock(nn.Module):
    def __init__(self, in_channel, is_first=False, num_blocks=1):
        super().__init__()
        self.part1_conv = nn.Sequential(nn.Conv2d(in_channel, in_channel//2, 1, stride=1, padding=0, bias=False),
                                        nn.BatchNorm2d(in_channel//2),
                                        Mish())
        self.part2_conv = nn.Sequential(nn.Conv2d(in_channel, in_channel//2, 1, stride=1, padding=0, bias=False),
                                        nn.BatchNorm2d(in_channel//2),
                                        Mish())
        self.features = nn.Sequential(*[ResidualBlock(in_channel=in_channel//2) for _ in range(num_blocks)])
        self.transition1_conv = nn.Sequential(nn.Conv2d(in_channel//2, in_channel//2, 1, stride=1, padding=0, bias=False),
                                              nn.BatchNorm2d(in_channel//2),
                                              Mish())
        self.transition2_conv = nn.Sequential(nn.Conv2d(in_channel, in_channel, 1, stride=1, padding=0, bias=False),
                                              nn.BatchNorm2d(in_channel),
                                              Mish())
        if is_first:
            self.part1_conv = nn.Sequential(nn.Conv2d(in_channel, in_channel, 1, stride=1, padding=0, bias=False),
                                            nn.BatchNorm2d(in_channel),
                                            Mish())
            self.part2_conv = nn.Sequential(nn.Conv2d(in_channel, in_channel, 1, stride=1, padding=0, bias=False),
                                            nn.BatchNorm2d(in_channel),
                                            Mish())
            self.features = nn.Sequential(*[ResidualBlock(in_channel=in_channel,
                                                          hidden_channel=in_channel//2) for _ in range(num_blocks)])
            self.transition1_conv = nn.Sequential(nn.Conv2d(in_channel, in_channel, 1, stride=1, padding=0, bias=False),
                                                  nn.BatchNorm2d(in_channel),
                                                  Mish())
            self.transition2_conv = nn.Sequential(nn.Conv2d(2 * in_channel, in_channel, 1, stride=1, padding=0, bias=False),
                                                  nn.BatchNorm2d(in_channel),
                                                  Mish())
```

<br>

#### **Purposes of Designing CSPNet**

<br>

- There are several **purposes** of designing this kind of partial structure.

1. **Increase gradient path** 

    - By separting the dense block and transition layer, one can double the gradient path, preventing reuse of gradient in other path. 

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/0baad1cf-d08f-4a47-9ed9-88fe62c50ebd" width="550">

    - From the above figure, standard denseblock sequentially receives previous output as an input of next dense layer, which leads to excessive reuse of gradients across all depths.

    - However, partial dense block has two separate paths that don't share gradient generated from each side, which in turn doubles the gradient flow in the network. 

    - Separating transition layer further maximizes the difference of gradient combination. 
        
        - **Fusing transition layer** after cocatenation of two paths (**fusion first**) significantly drops the performance (-1.5%) compared to CSPPeleeNet, whereas the case of only applying transition to Part 2 (**fusion last**) is not much affected (-0.1%). Computational cost is decreased for both cases, obviously. 

        - This results demonstrate that **enriched gradient flow is the key part that enhances the performance of CSPDenseNet.** 

            <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/27cc4277-51e8-4feb-9936-c42abb96be68" width="800">

2. **Prevent computational bottleneck**

    - As the amount of feature maps (channels) subjected to dense block becomes half of original dense block, the computational bottleneck issue due to large gap between the number of channels and growth rate can also be alleviated.


3. **Reduce memory traffic**

    - Computations required for dense block is $\large (c \times n) + \{n \times (n + 1) \times k\}$, where c is the number of channels, n is the number of dense layer, and k is the growth rate. 

    - As the number of channel (c) is reduced to half, which is usually far greater than n, k, memory traffic can be saved by nearly half as well. 

<br>

### **Neck : SPP, SAM, PAN**

<br>

- There are some layers inserted between backbone and head (make predictions for classes and boxes).

- These layers are typically used to integrate and re-organize the feature maps extracted from backbone to make more comprehensive and semantically strong features that are robust to scale changes of objects. 

<br>

#### **Spatial Pyramid Pooling (SPP)**

<br>

- YOLOv4 adds the SPP block over the backbone to process the topmost feature maps (512 x 13 x 13).

- SPPNet applys multiple MaxPooling layers with different kernel sizes (5, 9, 13) to the given input in parallel and concatenate each output (naive input with no pooling as well) to enlarge the receptive field. (final output : 512*4 x 13 x 13)

<br>

```python
class SPPNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Sequential(
                        nn.Conv2d(1024, 512, 1, stride=1, padding=0, bias=False),
                        nn.BatchNorm2d(512),
                        Mish(),
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(2048, 2048, 1, stride=1, padding=0, bias=False),
            nn.BatchNorm2d(2048),
            Mish(),
        )

        self.maxpool5 = nn.MaxPool2d(kernel_size=5, stride=1, padding=5//2)
        self.maxpool9 = nn.MaxPool2d(kernel_size=9, stride=1, padding=9//2)
        self.maxpool13 = nn.MaxPool2d(kernel_size=13, stride=1, padding=13//2)

    def forward(self, x):
        x = self.conv1(x)   # torch.Size([1, 512, 16, 16])
        maxpool5 = self.maxpool5(x)
        maxpool9 = self.maxpool9(x)
        maxpool13 = self.maxpool13(x)
        x = torch.cat([x, maxpool5, maxpool9, maxpool13], dim=1)
        x = self.conv2(x)
        return x
```

<br>

#### **Self-Attention Module (SAM)**

<br>

- Inspired from [**Convolutional Block Attention Module (CBAM)**](https://arxiv.org/pdf/1807.06521v2.pdf){:target='_blank'}, one of the variants of SAM, YOLOv4 utilizes modified SAM to give weighted attention to extracted feature maps. 

<br>

<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/cd5c97fb-63e8-41a6-a197-e6a66f5d621c" width="570">

<br>

- Instead of taking max and average pooling to get vectorized channel-wise attention, YOLOv4 implements 3 x 3 convolution to get pixel-wise attention that has same shape as the input feature map.

- Take sigmoid activation to computed attention to get probabilistic attention scores (0 ~ 1) and multiply them to target feature maps. 

- There are other types of methods for assigning attention to backbone features such as Squeeze-Excitation module (SE) but this approach increases the inference time by aboout 10%, while SAM only needs to pay 0.1% extra calculation with slight improvement (0.5%) to SE based ResNet50 model. 

<br>

#### **Path Aggregation Networks (PAN)**

<br>

<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/537490ff-e0e3-4a37-9d04-f92f6c5b05a1" width="500">

<br/>

- YOLOv4 adopted PANet to build stronger feature maps that combine features from multiple levels of the pyramid, which are far more helpful for following predictions compared to features made from a single level. 

- Detailed explanation about the architecture of PANet is [**HERE**](http://127.0.0.1:4000/panet/){:target='_blank'}.

- A modification from original PANet : Uses concatenation instead of addition for bottom-up pathway.

<br>

```python
class PANet(nn.Module):
    def __init__(self):
        super(PANet, self).__init__()

        self.p52d5 = nn.Sequential(nn.Conv2d(2048, 512, 1, stride=1, padding=0, bias=False),
                                   nn.BatchNorm2d(512),
                                   Mish(),
                                   nn.Conv2d(512, 1024, 3, stride=1, padding=1, bias=False),
                                   nn.BatchNorm2d(1024),
                                   Mish(),
                                   nn.Conv2d(1024, 512, 1, stride=1, padding=0, bias=False),
                                   nn.BatchNorm2d(512),
                                   Mish(),
                                   )

        self.p42p4_ = nn.Sequential(nn.Conv2d(512, 256, 1, stride=1, padding=0, bias=False),
                                    nn.BatchNorm2d(256),
                                    Mish(),
                                    )

        self.p32p3_ = nn.Sequential(nn.Conv2d(256, 128, 1, stride=1, padding=0, bias=False),
                                    nn.BatchNorm2d(128),
                                    Mish(),
                                    )

        self.d5_p4_2d4 = nn.Sequential(nn.Conv2d(512, 256, 1, stride=1, padding=0, bias=False),
                                       nn.BatchNorm2d(256),
                                       Mish(),
                                       nn.Conv2d(256, 512, 3, stride=1, padding=1, bias=False),
                                       nn.BatchNorm2d(512),
                                       Mish(),
                                       nn.Conv2d(512, 256, 1, stride=1, padding=0, bias=False),
                                       nn.BatchNorm2d(256),
                                       Mish(),
                                       nn.Conv2d(256, 512, 3, stride=1, padding=1, bias=False),
                                       nn.BatchNorm2d(512),
                                       Mish(),
                                       nn.Conv2d(512, 256, 1, stride=1, padding=0, bias=False),
                                       nn.BatchNorm2d(256),
                                       Mish(),
                                       )

        self.d4_p3_2d3 = nn.Sequential(nn.Conv2d(256, 128, 1, stride=1, padding=0, bias=False),
                                       nn.BatchNorm2d(128),
                                       Mish(),
                                       nn.Conv2d(128, 256, 3, stride=1, padding=1, bias=False),
                                       nn.BatchNorm2d(256),
                                       Mish(),
                                       nn.Conv2d(256, 128, 1, stride=1, padding=0, bias=False),
                                       nn.BatchNorm2d(128),
                                       Mish(),
                                       nn.Conv2d(128, 256, 3, stride=1, padding=1, bias=False),
                                       nn.BatchNorm2d(256),
                                       Mish(),
                                       nn.Conv2d(256, 128, 1, stride=1, padding=0, bias=False),
                                       nn.BatchNorm2d(128),
                                       Mish(),
                                       )

        self.d52d5_ = nn.Sequential(nn.Conv2d(512, 256, 1, stride=1, padding=0, bias=False),
                                    nn.BatchNorm2d(256),
                                    Mish(),
                                    nn.Upsample(scale_factor=2)
                                    )

        self.d42d4_ = nn.Sequential(nn.Conv2d(256, 128, 1, stride=1, padding=0, bias=False),
                                    nn.BatchNorm2d(128),
                                    Mish(),
                                    nn.Upsample(scale_factor=2)
                                    )

        self.u32u3_ = nn.Sequential(nn.Conv2d(128, 256, 3, stride=2, padding=1, bias=False),
                                    nn.BatchNorm2d(256),
                                    Mish())

        self.u42u4_ = nn.Sequential(nn.Conv2d(256, 512, 3, stride=2, padding=1, bias=False),
                                    nn.BatchNorm2d(512),
                                    Mish())

        self.d4u3_2u4 = nn.Sequential(nn.Conv2d(512, 256, 1, stride=1, padding=0, bias=False),
                                      nn.BatchNorm2d(256),
                                      Mish(),

                                      nn.Conv2d(256, 512, 3, stride=1, padding=1, bias=False),
                                      nn.BatchNorm2d(512),
                                      Mish(),

                                      nn.Conv2d(512, 256, 1, stride=1, padding=0, bias=False),
                                      nn.BatchNorm2d(256),
                                      Mish(),

                                      nn.Conv2d(256, 512, 3, stride=1, padding=1, bias=False),
                                      nn.BatchNorm2d(512),
                                      Mish(),

                                      nn.Conv2d(512, 256, 1, stride=1, padding=0, bias=False),
                                      nn.BatchNorm2d(256),
                                      Mish(),
                                      )

        self.d5u4_2u5 = nn.Sequential(nn.Conv2d(1024, 512, 1, stride=1, padding=0, bias=False),
                                      nn.BatchNorm2d(512),
                                      Mish(),

                                      nn.Conv2d(512, 1024, 3, stride=1, padding=1, bias=False),
                                      nn.BatchNorm2d(1024),
                                      Mish(),

                                      nn.Conv2d(1024, 512, 1, stride=1, padding=0, bias=False),
                                      nn.BatchNorm2d(512),
                                      Mish(),

                                      nn.Conv2d(512, 1024, 3, stride=1, padding=1, bias=False),
                                      nn.BatchNorm2d(1024),
                                      Mish(),

                                      nn.Conv2d(1024, 512, 1, stride=1, padding=0, bias=False),
                                      nn.BatchNorm2d(512),
                                      Mish(),
                                      )

    def forward(self, P5, P4, P3):
        D5 = self.p52d5(P5)    # [B, 512, 13, 13]
        D5_ = self.d52d5_(D5)  # [B, 256, 26, 26]
        P4_ = self.p42p4_(P4)  # [B, 256, 26, 26]
        D4 = self.d5_p4_2d4(torch.cat([D5_, P4_], dim=1))   # [B, 256, 26, 26]
        D4_ = self.d42d4_(D4)                               # [B, 128, 52, 52]
        P3_ = self.p32p3_(P3)                               # [B, 128, 52, 52]
        D3 = self.d4_p3_2d3(torch.cat([D4_, P3_], dim=1))   # [B, 128, 52, 52]

        U3 = D3                                             # [B, 128, 52, 52]   V
        U3_ = self.u32u3_(U3)
        U4 = self.d4u3_2u4(torch.cat([D4, U3_], dim=1))     # [B, 256, 26, 26]   V
        U4_ = self.u42u4_(U4)                               # [B, 512, 13, 13]
        U5 = self.d5u4_2u5(torch.cat([D5, U4_], dim=1))     # [B, 512, 13, 13]   V

        return [U5, U4, U3]
```

<br>

### **Head : YOLOv4**

<br>

- Extract features with 3 scales (13, 26, 52) each for large, middle, small objects. 

<br>

```python
class YOLOv4(nn.Module):
    def __init__(self, backbone, num_classes=80):
        super(YOLOv4, self).__init__()
        self.num_classes = num_classes
        self.backbone = backbone
        self.SPP = SPPNet()
        self.PANet = PANet()

        self.pred_s = nn.Sequential(nn.Conv2d(128, 256, 3, stride=1, padding=1, bias=False),
                                    nn.BatchNorm2d(256),
                                    Mish(),
                                    nn.Conv2d(256, 3 * (1 + 4 + self.num_classes), 1, stride=1, padding=0))

        self.pred_m = nn.Sequential(nn.Conv2d(256, 512, 3, stride=1, padding=1, bias=False),
                                    nn.BatchNorm2d(512),
                                    Mish(),
                                    nn.Conv2d(512, 3 * (1 + 4 + self.num_classes), 1, stride=1, padding=0))

        self.pred_l = nn.Sequential(nn.Conv2d(512, 1024, 3, stride=1, padding=1, bias=False),
                                    nn.BatchNorm2d(1024),
                                    Mish(),
                                    nn.Conv2d(1024, 3 * (1 + 4 + self.num_classes), 1, stride=1, padding=0))

        print("num_params : ", self.count_parameters())

    def count_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

    def forward(self, x):

        P3 = x = self.backbone.features1(x)  # [B, 256, 52, 52]
        P4 = x = self.backbone.features2(x)  # [B, 512, 26, 26]
        P5 = x = self.backbone.features3(x)  # [B, 1024, 13, 13]

        P5 = self.SPP(P5)
        U5, U4, U3 = self.PANet(P5, P4, P3)

        p_l = self.pred_l(U5).permute(0, 2, 3, 1)  # B, 13, 13, 255
        p_m = self.pred_m(U4).permute(0, 2, 3, 1)  # B, 26, 26, 255
        p_s = self.pred_s(U3).permute(0, 2, 3, 1)  # B, 52, 52, 255

        return [p_l, p_m, p_s]
```

<br>

## **Selection of BoF and BoS**

<br>




ff