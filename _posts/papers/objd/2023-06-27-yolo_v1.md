---
layout: post
title : "[Paper Review & Implementation] You Only Look Once: Unified, Real-Time Object Detection (YOLO V1, 2016)"
img: papers/objd/yolo_v1.png
categories: [papers-objd]  
tag : [Paper Review, Object Detection, Faster R-CNN, PyTorch]
toc : true
toc_sticky : true
---

## **Outlines**
- [**Reference**](#reference)
- [**Faster R-CNN vs Fast R-CNN vs R-CNN**](#faster-r-cnn-vs-fast-r-cnn-vs-r-cnn)
- [**Step By Step Implementation of Faster R-CNN with PyTorch**](#step-by-step-implementation-of-faster-r-cnn-with-pytorch)
- [**Attention of Transformer**](#attention-of-transformer)
- [**Embedding and Positional Encoding**](#embedding-and-positional-encoding)
- [**Encoder and Decoder Architecture**](#encoder-and-decoder-architecture)
- [**Comparisoin of Computational Efficiency to Other Models**](#comparisoin-of-computational-efficiency-to-other-models)
- [**Performance of Transformer in Machine Translation**](#performance-of-transformer-in-machine-translation)

<br/>

## **Reference**

<br/>

- [**You Only Look Once: Unified, Real-Time Object Detection, Joseph Redmon, 2016**](https://arxiv.org/abs/1506.02640){:target="_blank"}
- [**Machine-Learning-Collection/ML/Pytorch/object_detection/YOLO/**](https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/object_detection/YOLO){:target="_blank"}

<br>

## **Implementation with PyTorch**

<br>

- [**Full codes here**](https://github.com/SuminizZ/Implementation/tree/main/YOLO_V1){:target="_blank"}  

- I referenced the entire code from [**here**](https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/object_detection/YOLO){:target="_blank"}, but I made a few modifications to optimize performance, such as replacing explicit for loops with vectorization for matrix operations.

<br/>

## **YOLO : Single-State Object Detection**

<br/>

<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/a318bd5b-e657-4de8-ad28-f26450f6d3b1" width="700">

<br/>

- **(a)** : Two-stage object detection models like faster R-CNN consists of two main components : a region proposal network (RPN) and a classifier. 

    - RPN generates region proposals (potential bounding box locations), and these proposals are then filtered and refined to be the final target RoIs by the classifier.

<br/>

- **(b)** : In contrast, YOLO is a single-stage object detection model where region proposal stage is incorporated into a feature extractor architecture. 

    - It divides the input image into a set of grid cells and predicts bounding boxes and class probabilities directly from each cell instead of explicitly generating region proposals. 
    
    - Uses a CNN to extract features from the entire image at once and predict object detections.

&emsp;&emsp;&emsp;<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/875c2aef-8c18-4961-bebc-4c3520ffe75e" width="700">


<br>

## **YOLO Backbone Architecture : Feature Extractor and Region Proposals**

<br>

&emsp; **Figure 3. YOLO V1 Architecure (Darknet framework) : 24 conv layers followed by 2 fc layers**

&emsp;<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/4054e515-492d-4ac7-8f11-4cb82a0649b2" width="840">


```python
import torch
import torch.nn as nn

architecture_configs = [
    (7, 64, 2, 3),
    "M",
    (3, 192, 1, 1),
    "M",
    (1, 128, 1, 0),
    (3, 256, 1, 1),
    (1, 256, 1, 0),
    (3, 512, 1, 1),
    "M",
    [(1, 256, 1, 0), (3, 512, 1, 1), 4],
    (1, 512, 1, 0),
    (3, 1024, 1, 1),
    "M",
    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],
    (3, 1024, 1, 1),
    (3, 1024, 2, 1),
    (3, 1024, 1, 1),
    (3, 1024, 1, 1),
]


class CNNBlock(nn.Module):
    def __init__(self, in_channels, out_channels, **kwargs):
        super(CNNBlock, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
        self.batchnorm = nn.BatchNorm2d(out_channels)
        self.leakyrelu = nn.LeakyReLU(0.1)

    def forward(self, x):
        return self.leakyrelu(self.batchnorm(self.conv(x)))


class YOLOv1(nn.Module):
    def __init__(self, in_channels=3, **kwargs):
        super(YOLOv1, self).__init__()
        self.architecture = architecture_configs
        self.in_channels = in_channels
        self.darknet = self._create_conv_layers(self.architecture)
        self.fcs = self._create_fcs(**kwargs)

    def forward(self, x):
        x = self.darknet(x)
        return self.fcs(torch.flatten(x, start_dim=1))

    def _create_conv_layers(self, architecture):
        layers = []
        in_channels = self.in_channels

        for x in architecture:
            if type(x) == tuple:
                layers += [
                    CNNBlock(
                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],
                    )
                ]
                in_channels = x[1]

            elif type(x) == str:
                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]

            elif type(x) == list:
                conv1 = x[0]
                conv2 = x[1]
                num_repeats = x[2]

                for _ in range(num_repeats):
                    layers += [
                        CNNBlock(
                            in_channels,
                            conv1[1],
                            kernel_size=conv1[0],
                            stride=conv1[2],
                            padding=conv1[3],
                        )
                    ]
                    layers += [
                        CNNBlock(
                            conv1[1],
                            conv2[1],
                            kernel_size=conv2[0],
                            stride=conv2[2],
                            padding=conv2[3],
                        )
                    ]
                    in_channels = conv2[1]

        return nn.Sequential(*layers)

    def _create_fcs(self, split_size, num_boxes, num_classes):
        S, B, C = split_size, num_boxes, num_classes

        return nn.Sequential(
                nn.Flatten(),
                nn.Linear(1024*S*S, 4096),
                nn.LeakyReLU(0.1),
                nn.Linear(4096, S*S*(B*5+C))
            )
```
<br/>

- Input : images with shape (3, 448, 448) 

    - pretrained with ImageNet (224 x 224) and double the resolution of input at detection.

- Inspired by Inception Net, but simply used the 1 x 1 conv layer followed 3 x 3 conv layer (basically just a bottleneck block) instead of cocatenating them in parallel.

- Output : (n_batch, S*S*(B*5+C) = 1470) 

    - S : grid size (here, total 49 cells from 7 x 7 grid)

    - B : the number of bounding boxes attached to each cell. (in paper, 2)

    - C : the number of classes (20)

    - 5 predictions: x, y, w, h, and confidence score (objectness)

<br/>

## **Get Normalized Bounding Boxes For Each Grid Cell**

<br/>

```python
def get_bboxes(n_batch, loader, model, iou_threshold, threshold, device, box_format='center'):
    pred_boxes, gt_boxes = [], []
    model = model.to(device)
    model.eval()   # make sure to turn off the train mode before getting final bboxes. 

    for batch_idx, (img, labels) in enumerate(loader):
        train_idxs = (torch.arange(n_batch) + (batch_idx*n_batch)).unsqueeze(-1).unsqueeze(-1).repeat(1, 49, 1)

        img = img.to(device)
        labels = labels.to(device)

        with torch.no_grad():
            preds = model(img)   # (n_batch, S*S*(C+B*5))

        S = 7
        n_batch = len(preds)
        pred_bboxes = convert_cellboxes(preds).reshape(n_batch, S*S, -1)
        gt_bboxes = convert_cellboxes(labels).reshape(n_batch, S*S, -1)

        pred_bboxes = torch.concat([train_idxs, pred_bboxes], dim=-1)
        gt_bboxes = torch.concat([train_idxs, gt_bboxes], dim=-1)

        pred_boxes += non_max_suppression(pred_bboxes, iou_threshold, threshold, "center")
        for bb in [box[np.where(box[:, 2] > threshold)[0]] for box in gt_bboxes]:
            gt_boxes += [b for b in bb]
        # if batch_idx == 10: break

    return pred_boxes, gt_boxes
```

<br/>

#### **Step 1 : Convert the scale of bounding boxes within a cell to the scale relative to the entire image**

<br/>

```python
def convert_cellboxes(predictions, S=7):
    """
    Convert output of yolo v1
    (n_batch, 7*7*(5*B+C)) -> (n_batch, 7, 7, [pred_clss, best_confidence_score, center coordinates])
    - Convert bounding boxes with grid split size S relative to cell ratio into entire image ratio.
    """

    predictions = predictions.to("cpu")
    batch_size = predictions.shape[0]
    predictions = predictions.reshape(batch_size, 7, 7, 30)
    bboxes1 = predictions[..., 21:25]    # 1 bbox 
    bboxes2 = predictions[..., 26:30]    # 2 bbox 
    
    # select best bounding box with highest confidence score among 2 candidates
    scores = torch.cat((predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0)   # 2 x (n_batch x 7 x 7)
    best_box = scores.argmax(0).unsqueeze(-1)     # n_batch x 7 x 7 x 1
    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2  

    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)
    x = (1/S) * (best_boxes[..., :1] + cell_indices)    # 0 + x*(1/S), 1 + x*(1/S), 2 + x*(1/S), ...  6 + x*(1/S)
    y = (1/S) * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))  
    w_y = (1/S) * best_boxes[..., 2:4]  
     
    converted_bboxes = torch.cat((x, y, w_y), dim=-1)
    pred_class = predictions[..., :20].argmax(-1).unsqueeze(-1)      # class with best pred scores. 
    best_conf = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(-1)    # best confidence score 
    converted_preds = torch.cat((pred_class, best_conf, converted_bboxes), dim=-1)
    return converted_preds
```

<br/>

- Only select the anchor with best confidence score out of the two attached anchors per cell and mask the other.

- Predicted bounding boxes and labels are normalized relatvie to cell size. Make the cell based scale of each anchor relative to the entire image. 

- Also, normalize the bounding box width and height by the image width and height so that they fall between 0 and 1