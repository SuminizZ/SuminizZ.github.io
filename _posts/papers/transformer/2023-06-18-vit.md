---
layout: post
title : "[Paper Review & Implementation] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT, 2021)"
img: papers/transformer/vit.png
categories: [papers-transformer]  
tag : [Paper Review, Attention, Transformer, PyTorch]
toc : true
toc_sticky : true
---

## Outlines 
- [**Reference**](#reference)
- [**Implementation with PyTorch**](#implementation-with-pytorch)
- [**Constraint of Recurrent Models : Sequential Computation**](#constraint-of-recurrent-models--sequential-computation)
- [**Attention of Transformer**](#attention-of-transformer)
- [**Embedding and Positional Encoding**](#embedding-and-positional-encoding)
- [**Encoder and Decoder Architecture**](#encoder-and-decoder-architecture)
- [**Comparisoin of Computational Efficiency to Other Models**](#comparisoin-of-computational-efficiency-to-other-models)
- [**Performance of Transformer in Machine Translation**](#performance-of-transformer-in-machine-translation)

<br/>

## **Implementation with PyTorch**

<br/>

- [**Implementation**](https://github.com/SuminizZ/Implementation/tree/main/Transformer){:target="_blank"}


<br/>

##  **Reference**

- [Attention Is All You Need, Ashish Vaswani, 2017](https://arxiv.org/abs/1706.03762){:target="_blank"}
- [[NLP 논문 구현] pytorch로 구현하는 Transformer (Attention is All You Need)](https://cpm0722.github.io/pytorch-implementation/transformer){:target="_blank"}

<br/>