---
layout: post
title : "[Paper Review] Sharpness-Aware Minimization for Efficiently Improving Generalization (SAM, 2021)"
img: papers/opt/sam.png
categories: [papers-opt]  
tag : [Paper Review, Loss Landscape, Optimization, Explainable AI]
toc : true
toc_sticky : true
---

## **Outlines** 
<br/>

- [**References**](#references)
- [**1. Weak Generalization Power of Sharp Minima**](#1-weak-generalization-power-of-sharp-minima)
- [**2. Sharpness-Aware Minimization (SAM)**](#2-sharpness-aware-minimization-sam)
    - [**2.1. PAC Bayesian Generalization Bound**](#21-pac-bayesian-generalization-bound)
    - [**2.2. SAM Objective**](#22-sam-objective)
- [**3. Empirical Evaluation**](#3-empirical-evaluation)

<br/>

## **References**

- [**Sharpness-Aware Minimization for Efficiently Improving Generalization, Foret et al, 2021**](https://arxiv.org/abs/2010.01412){:target="_blank"}
- [**Proving that the dual of the lp norm is the lq norm**](https://math.stackexchange.com/questions/265721/proving-that-the-dual-of-the-mathcall-p-norm-is-the-mathcall-q-norm){:target='_blank'}
- [**An Introduction to PAC-Bayes**](https://www.youtube.com/watch?v=t5GBuBD0ibc&t=2246s){:target='_blank'}

<br/>

## **1. Weak Generalization Power of Sharp Minima**

<br/>

- Overparameterizing the model with an objective to minimize the training loss can results in suboptimal model that fails to generalize over the entire distribution of the data. 

- Visual representation of the loss landscape of the overfitted model shows sharp minima where the curvature of loss landscape becomes significantly sharp near the minima. 

- Overly peaked loss minima indicates that the model can fail to show stable performance over the deviations from the data to which it's originally fitted during training.

- Thus, leveraging the geometry of the landscape to have flatter minima can yield better generalization to the test set.

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/d418ae0c-ffab-4159-a4d5-33a5fcc1e9f2" width="650">

- Figure above gives visual intuition that flatter minima tends to be more robust to the deviation between training and test function (goal) with smaller generalization gap compared to sharp minima. 

- In order to alleviate loss sharpness and achieve better generalization, this paper suggests to extend the minimization objective to the neighborhoods of the parameters, not just the parameters themselves. 

<br/>

## **2. Sharpness-Aware Minimization (SAM)**

<br/>

- **Notations**

    - Training dataset : <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/f200ee2e-7f8a-4ed7-97a5-07ffa59c924c" width="200">

    - Population Distribution : <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/d98cb3d0-5d93-4a9f-a607-c278c3f1c2f4" width="25">

    - Training loss : <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/96c1cd5a-cc35-4cb5-be4e-dee15f1996e4" width="280">

    - Population loss : <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/caf239f5-0087-4bb1-a955-1790317b1591" width="300">

- Typical approach of searching for the parameters is by solving $\large \text{min}\, L_{S}(w)$ with respect to $\large w$, which can easily result in suboptimal performance at test time.

- Instead SAM seeks out the parameters whose bounded neighborhoods have uniformly low training loss value and thus forms wide and flat curvature of the loss landscape. 

<br/>

### **2.1. PAC Bayesian Generalization Bound**

<br/>

&emsp;&emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/ecdca744-6187-490c-ac6b-231d86ca8576" width="600">

<br/>

- **Probably Approximately Correct (PAC) Bayes Bound**

    - PAC Goal : with high probability (at least 1-$\delta$), the empirical loss is approximately correct (error from true risk is bounded by certain small value)

    &emsp;&emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/ae28022e-47d8-491b-9248-05773d49d4a4" width="560">

    - This inequality holds for any prior P over parameters p and posterior Q over parameters. 

    - n : the size of the dataset S 

    - Q : Posterior on hypotheses that depends on the training datasets S. 

    - P : Priors on hypotheses that doen NOT depends on the training datasets S.

    - Assuming that each of prior and posterior follows distinct normal distribution <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/494c26ea-7dde-4516-9339-ee164f3e6426" width="370">, then the KL divergence between them is as follows 

        <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/b04f2698-8891-47b9-95e8-ec2000b4f636" width="540">

    - Can check the full derivations **HERE**

        - [**McAllester's PAC-Bayes Bound**](https://suminizz.github.io/pac_bayes/){:target="_blank"}

        - [**KL Divergence between Two Normal Distributions**](https://suminizz.github.io/kl_divergence/){:target="_blank"}

<br/>

### **2.2. SAM Objective**

<br/>

- SAM aims to minimize the training loss not only concerning the parameters themselves but also by considering their bounded neighborhoods.

- Starting from the PAC-bayes bound derived over the parameter set,

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/ae28022e-47d8-491b-9248-05773d49d4a4" width="520">

- SAM extends the generalization bound based on the sharpness.

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/5e3edd60-1223-4af8-8fa2-28ffd8472522" width="800">

    - More formally, the inequality is as follows

        <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/1527d1c2-718d-448b-8faa-a2d90c8ed980" width="760">

    - Proof required to derive the inequality above is provided in the paper. (just focus on the fact that $\large h$ here is still the increasing function of $\large \|\|w\|\|_{2}^{2}$)

    - **Intuition of why SAM leverages sharpness of the loss landscape**

       - Rewriting the RHS

            <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/66ddac44-39a0-4f5b-a22b-58b59dc91f10" width="500">

        - Term in square brackets captures the loss sharpness and thus minimizing the summed of $\large \text{max} \, L_{s}(w + \epsilon)$ results in flattening the curvature around the neighborhoods of the parameters. 

- Given that the specific function h (increasing f w.r.t power of w) is heavily influenced by the details of the proof, it's substituted with $\large \lambda \|\|w\|\|^{2}$ for a hyperparameter $\large \lambda$, yielding a standard L2 regularization term.

    - Then the SAM can be simplified to 

        <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/6fba882a-5571-4829-84c8-c80a79ce69ee" width="650">
        
    - To sum up, SAM objective is given by minimizing the superior of neighborhood loss summed over a batch set with a L2 regularization over the magnitude of the parameters and this improves the generalization of the predictor by limiting the upper bound of the true loss ($\large L_{\mathcal{D}(w)}$)

- As the objective of SAM is given, parameters can be optimzed to minimize $\large L_{S}^{SAM}(w)$ by stochastically update the term using gradient descent. 

    &emsp; g = <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/09c07e94-7d8d-4bab-8625-9d0058ba32d5" width="150">

    &emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/3cb939e1-87bc-4d78-b8da-7477079be385" width="150">

- First, to express <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/b7e9d72d-9ba4-480c-9e83-ffe5fea046cd" width="160"> in a solvable form w.r.t $\large \epsilon$, approximate it via a first-order Tylor expansion (this strategy is valid as $\large \epsilon$ is set to be near 0)

    &emsp; <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/c616d08a-3504-4c62-918f-08684e96d25d" width="580">

    - As $\large L_{S}(w)$ is determined by data, the problem gets down to simply solving $\large \epsilon$ that maximizes the second term. 

        <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/cd0bb449-16af-4f9e-a6ea-b1ee5053b0bc" width="220">

    - Solution to this problem $\large \hat{\epsilon}(w)$ is given by the solution to a classical dual p-norm problem

        <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/1e0d4ea3-aee5-4493-b320-1c44cf1cb45c" width="530">

    - **Proof for Solving Dual p-Norm Problem**

        <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/84608a09-7b5b-4186-a55b-8edc15d0f76b" width="920">

- Thus, <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/c8978b27-b83e-4e9d-98f6-2c723eca760e" width="460">

- Then differentiating $\large L_{S}^{SAM}(w)$ gives

    &emsp;<img src="https://github.com/SuminizZ/Algorithm/assets/92680829/c65683c6-014d-401e-8302-cb15842aa0c5" width="670">

- As the second term contains the Hessian of loss with respect $\large w$ ($\large \tfrac{d\hat{\epsilon}(w)}{dw}$), which is too expensive to compute, SAM drops it from the gradient formula.

- Then the final approximation of the SAM gradient is 

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/81c26c29-7923-43c0-8612-f2beed36f4e0" width="350">

- **SGD with SAM Objective**

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/b2bbf69e-4f41-47d6-80a0-bfe6589b298c" width="800">

<br/>

## **3. Empirical Evaluation**

<br/>

- **m-Sharpness**

    - Generalization power of the model grows with the batch size (m) for SGD update with SAM, demonstrated by the result of the experiment that shows the correlation between sharpness and generalization is higher for smaller m. 

        <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/67d51514-1879-49a1-8886-076bc331bfa2" width="610">

    - Smaller m tends to be more sensitive to changes in $\large \rho$, a bound of the magnitude of $\large \epsilon$, which means that the effect of SAM objective becomes more significant in smaller batch size. 

- **Hessian Spectra**

    - Spectrum of Hessian (ratio of $\large \lambda_{max}/\lambda_{5}$) is a widely used measure of loss sharpness. ($\large \lambda$ here stands for the eigenvalue of the Hessian matrix at convergence).

        <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/50bab109-9153-49e3-b5ba-e1a4334e6e6e" width="400">

    - left : standard SGD / right : SGD with SAM objective 
    
    - As the number of epoch increases, the distribution of eigenvalues becomes more left-shifted and Hessian spectrum decreases. 


- **Performance Comparison**

    <img src="https://github.com/SuminizZ/Algorithm/assets/92680829/c7463291-65a5-4fde-a7de-0fb8f929823a" width="750">

    - Error rates for fine-tuning various SOTA models (EffNet, BiT, ViT, and etc.)

    - SAM uniformly improves the performance relative to finetuning w/o SAM. 


<br/>

---

<br/>

- SAM derived from PAC-Bayesian bound, successfully improves the generalization power of the predictors by leveraging the sharpness of loss geometry.