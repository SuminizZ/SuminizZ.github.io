---
layout: post
title : "[Information Theory] Entropy, KL Divergence, and Mutual Information"
img: ml/inform/kl_div.png
categories: [ml-inform] 
tag : [Information Thoery, KL Divergence, ML]
toc : true
toc_sticky : true
---

<br/>

### Contents
- Entropy, Cross Entropy, Conditional Entropy
- KL Divergence
- Mutual Information
- KL Divergence of Two Different Normal Distribution & Bernoulli Distribution

### Notes 
- [<span style="color:purple">**Entropy, KL Divergence, and Mutual Information**</span>](https://drive.google.com/file/d/19bcc1joSFlIA31pPV2-ShzYikXPPMwIb/view?usp=share_link){:target="_blank"}


