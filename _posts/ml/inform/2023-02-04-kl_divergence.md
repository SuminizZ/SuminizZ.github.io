---
layout: post
title : "[Information Theory] Entropy, KL Divergence, and Mutual Information"
img: ml/inform/kl_div.png
categories: [ml-inform] 
tag : [Information Thoery, KL Divergence, ML]
toc : true
toc_sticky : true
---
<br/>

- Entropy is the averaged amount of bits to express the message, derived from the theromodynamic concept of entropy in physics <br/>
    
    &emsp;&emsp;&emsp; $\large \sum_{i}^{n}\,p(x_{i})\,\log{p(x_{i})}$

### Summary Notes

- [<span style="color:purple">**Entropy, KL Divergence, and Mutual Information**</span>](https://drive.google.com/file/d/17E1L4417BJVggUa83DZ1c8lmq9awJoDs/view?usp=share_link){:target="_blank"}


