I"›1<p><br /></p>

<h2 id="problem-formulation"><strong>Problem Formulation</strong></h2>
<ul>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/158548925-9de07576-9a00-4759-8cf3-bb82f43efd99.png" width="500" /></p>
  </li>
  <li>
    <p>Notations</p>
    <ul>
      <li>nu - Number of users (called ?nu occasionally as we can‚Äôt subscript in superscript)</li>
      <li>nm - Number of movies</li>
      <li>r(i, j) - 1 if user j has rated movie i (i.e. bitmap)</li>
      <li>y(i,j) - rating given by user j to move i (defined only if r(i,j) = 1</li>
      <li>mj - Number of movies rated by the user (j)</li>
    </ul>
  </li>
</ul>

<hr />

<p><br /></p>

<h2 id="content-based-recommenation"><strong>Content-Based Recommenation</strong></h2>

<p><br /></p>

<ul>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/158550480-9412e2ac-62eb-434a-852c-4544b5a35e78.png" width="620" /></p>
  </li>
  <li>x(i) denotes ith the 3x1 feature vector of ith movie
    <ul>
      <li>[intercept x0 = 1, x1(romance), x2(action)]</li>
    </ul>
  </li>
  <li>we have {x(1), x(2), x(3), x(4), x(5)} for all 5 movies</li>
  <li>now, for this dataset,
    <ul>
      <li>n = 2 (number of features)</li>
      <li>m = 5 (number of examples)</li>
    </ul>
  </li>
  <li>For each user j, we have to learn parameters Œò(j) [3x1] vector
    <ul>
      <li>by using Œò(j), we predict user (j)‚Äôs ratings of movie (i)
        <ul>
          <li>(Œ∏j)T xi = stars (rating)</li>
        </ul>
      </li>
      <li>For example, lets take user 1 (Alice) and see what ratings she will give for the movie, Cute Puppies of Love (x(3))
        <ul>
          <li>x(3)T = [1, 0.99, 0]</li>
          <li>We have some parameter vector (Œ∏1T = [0, 5, 0] ) associated with Alice (how these values are derived will be explained later)</li>
          <li>now, we can calculate the predicted ratings as <strong>Œ∏1T.x(3) = 4.95</strong></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="how-to-learn-Œ∏j"><strong>How to learn (Œ∏j)</strong></h3>

<p><br /></p>

<ul>
  <li>To learn Œ∏j for j in all user (1~j)
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158713848-5f22125e-d342-4bca-8c2b-e808de0875a7.png" width="650" /></li>
    </ul>
  </li>
  <li>i:r(i, j) = 1 .. only if r(i, j) = 1 (only if the user actually rated the movie)</li>
  <li>we can delete the divider mj for simplification, as it has no effect on the result</li>
  <li>we don‚Äôt panalize Œ∏0, as you can see the regularization term starts from k = 1 (not 0)</li>
  <li>
    <p>it equlas to the cost function of multivariate linear regression to find the Œ∏ that can minimizes the cost (error of prediction)</p>
  </li>
  <li><strong>Gradient Descent Update</strong>
    <ul>
      <li>Repeat
        <ul>
          <li><img src="https://user-images.githubusercontent.com/92680829/158714263-ae15a6f0-468a-44ba-a9a5-0fe12388bd34.png" width="610" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>We just predicted the ratings of movies from user j based on the information of movie as x(i)</li>
  <li>This is why we call this type of learning <strong>‚ÄúContent Based Learning‚Äù</strong></li>
  <li>x(i) includes the information that how much this specific movie is related to each genres of movie such as romance, action, and etc.</li>
  <li>However, in practice. these information about movies are not really available all the time.</li>
  <li>Therefore, next we‚Äôll gonna discuss about <strong>‚ÄúNon-Contents Based Approach‚Äù</strong></li>
</ul>

<p><br /></p>

<h3 id="user-based-learning"><strong>User-Based Learning</strong></h3>

<p><br /></p>

<ul>
  <li>previously, we calculated Œ∏j based on x(i)</li>
  <li>but now, we will caculate x(i) based on the given Œ∏j, which is the preferences of user j for all features(genre)</li>
  <li>We must minimize an optimization function which tries to identify the best parameter vector associated with a <strong>film</strong>, not user</li>
  <li>So, the cost function can be
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158716844-d54039fb-2380-435b-a2dd-b506fbd0fa9a.png" width="650" /></li>
    </ul>
  </li>
</ul>

<hr />

<p><br /></p>

<h2 id="collaborative-filtering"><strong>Collaborative Filtering</strong></h2>

<ul>
  <li>Here we combine the ideas from before to build a <strong>Collaborative Filtering Algorithm</strong></li>
  <li>Our starting point is as follows
    <ol>
      <li>If we‚Äôre given the <strong>film‚Äôs features</strong> we can use that to work out the <strong>users‚Äô preference</strong>
        <ul>
          <li><img src="https://user-images.githubusercontent.com/92680829/158717060-923f0d2a-5613-4d28-acbc-3754b479ae46.png" width="600" /></li>
        </ul>
      </li>
      <li>If we‚Äôre given the <strong>users‚Äô preferences</strong> we can use them to work out the <strong>film‚Äôs features</strong>
        <ul>
          <li><img src="https://user-images.githubusercontent.com/92680829/158716844-d54039fb-2380-435b-a2dd-b506fbd0fa9a.png" width="600" /></li>
        </ul>
      </li>
    </ol>
  </li>
  <li>As each user gives ratings to multiple movies and each movie is rated by multiple users</li>
  <li>
    <p>So we go back and forth to collaboratively gain the information about both users and movies</p>
  </li>
  <li>Collaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users.</li>
  <li>It works by searching a large group of people and finding a smaller set of users with tastes similar to a particular user</li>
</ul>

<p><br /></p>

<h3 id="algorithm"><strong>Algorithm</strong></h3>

<p><br /></p>

<ul>
  <li>So basically, what we‚Äôre gonna do is just to mix both of the algorithms (1. and 2.)
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158718603-9b8086ff-07f7-4ca9-839f-398592964048.png" width="800" /></li>
    </ul>
  </li>
  <li>The squared error term is the same as the squared error term in the two individual objectives above</li>
  <li>So it‚Äôs summing over every movie rated by every users
    <ul>
      <li>x(i) for i in (1 - nm) and Œ∏j for j in (1 - nu)</li>
      <li>Sum over all pairs (i,j) for which r(i,j) is equal to 1</li>
    </ul>
  </li>
  <li>This newly defined function has the property that
    <ul>
      <li>If you held x constant and only solved Œ∏ then you solve the, ‚ÄúGiven x, solve Œ∏‚Äù objective above</li>
      <li>Similarly, if you held Œ∏ constant you could solve x</li>
      <li>Only difference between this in the <strong>back-and-forward</strong> approach is that we <strong>minimize with respect to both x and Œ∏ simultaneously</strong></li>
    </ul>
  </li>
</ul>

<p><strong>Algorithm Structure</strong></p>
<ol>
  <li>Initialize (Œ∏1, ‚Ä¶, Œ∏nu) and (x1, ‚Ä¶, xnm) to small random values
    <ul>
      <li>A bit like neural networks - initialize all parameters to small random numbers</li>
      <li>for symmetry breaking</li>
    </ul>
  </li>
  <li>Minimize cost function (J(x1, ‚Ä¶, xnm, Œ∏1, ‚Ä¶,Œ∏nu) using gradient descent (or other advanced optimization algorithm)
    <ul>
      <li>We find that the update rules look like this</li>
      <li>Upadting parameters for every movie as well as every users</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/158729102-fb83f8d5-c126-45e6-936e-a3040c02c23f.png" width="600" /></li>
    </ul>
  </li>
</ol>

<ul>
  <li>With all these parameters updated to minimize the cost, we can train the model to predict ratings of the movie that has not been rated by a user</li>
</ul>

<hr />

<h2 id="vectorization--low-rank-matrix-factorization"><strong>Vectorization : Low Rank Matrix Factorization</strong></h2>

<ul>
  <li>predicted ratings
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158731658-b8627a28-42cf-4749-834e-b8e95b028d41.png" width="550" /></li>
    </ul>
  </li>
  <li>Œ∏jTx(i) : ratings of x(i) movie from user j</li>
  <li>Define Feature [nm x 1] matrix X : information about all movies 1 ~ nm
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158732047-c77e2034-a3e3-4fc4-b012-75363800d882.png" width="200" /></li>
    </ul>
  </li>
  <li>Also Define [nu x 1] matrix Œ∏ : information about all users 1 ~ nu
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158732244-8d2e259b-1670-4b8e-9035-2ffc57a8d26e.png" width="250" /></li>
    </ul>
  </li>
  <li>Now with the matrices Œ∏ and  X that we defined above,
    <ul>
      <li>X * Œ∏T equals to the matrix of predicted ratings</li>
      <li>This is a <strong>vectorized way of computing the prediction range matrix by doing X * Œ∏T</strong></li>
      <li>the name of this algorithm is <strong>‚ÄúLow Rank Matrix Factorization‚Äù</strong>
        <ul>
          <li>low-rank approximation is a minimization problem, in which the cost function measures the fit between a given matrix (the data, x) and an approximating matrix (the optimization variable, Œ∏), subject to a constraint that the approximating matrix has reduced rank.</li>
          <li><img src="https://user-images.githubusercontent.com/92680829/158735251-f7e79210-77b6-4718-a01a-0b7fb3df47bc.png" width="500" /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="finding-related-movies"><strong>Finding Related Movies</strong></h3>
<ul>
  <li>For each movie (i), we learn x(i) [nx1 vector]
    <ul>
      <li>x1(i) = romance, x2(i) = comedy, ‚Ä¶ xn(i) = action</li>
    </ul>
  </li>
  <li>Then, how can we find movie j that is closely related to movie i
    <ul>
      <li>[[xi - xj]]  : can be a good estimate of movie similarity</li>
      <li>Provides a good indicator of how similar two films are in the sense of user perception</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="mean-normalization--implementational-detail"><strong>Mean Normalization : implementational detail</strong></h2>

<ul>
  <li>Suppose there is a user 5 that has rated no moives at all
    <ul>
      <li>
        <p><img src="https://user-images.githubusercontent.com/92680829/158736957-1f303b3b-88a5-43a4-b4f6-6fb5f7be41df.png" width="650" /></p>
      </li>
      <li>then r(i, 5) is always 0 so, the error part (predict - actual) is irrelevant with the cost</li>
      <li>therefore our goal is to minimize the squared Œ∏5</li>
      <li>but, this means that we have to set Œ∏5 = [0, 0], which makes the predicted ratings for all movies is also 0</li>
      <li>this doesn‚Äôt make sense!!</li>
      <li>THEN, how can we solve this problem?</li>
    </ul>
  </li>
</ul>

<h3 id="mean-normalization"><strong>Mean Normalization</strong></h3>
<ul>
  <li>Group all our ratings into matrix Y [5x4] (5 movie ratings from 4 users)</li>
  <li>Compute the average rating of each movie and store them to a new matrix [nm x 1]</li>
  <li>Subtract off the mean rating from Y
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158737754-c951a479-464a-42f5-879c-13b6fb9a48a6.png" width="700" /></li>
    </ul>
  </li>
  <li>Now, let‚Äôs pretend that newly normalized Y‚Äô matrix as our actual data</li>
  <li>use this Y‚Äô to learn Œ∏j and Xi
    <ul>
      <li>for user j, predicted rating for movie i : (Œ∏j)T * (xi) + <strong>(Œºi)</strong></li>
      <li>Now then, we can set all predicted ratings of the empty value as the average (Œºi) ratings of movie i</li>
    </ul>
  </li>
  <li>Back to our previous problem Œ∏5, now we can reset predicted rating of movie i from user 5
    <ul>
      <li>(Œ∏5)T xi + Œºi</li>
      <li>Where (Œ∏5)T xi = to 0 (still)</li>
      <li>But we then add the mean (Œºi) which means user 5 has an average rating assigned to each movie for here</li>
    </ul>
  </li>
  <li>This makes sense
    <ul>
      <li>If Eve hasn‚Äôt rated any films, predict the average rating of the films based on everyone</li>
      <li>This is the best we can do</li>
    </ul>
  </li>
  <li>As an aside - we spoke here about mean normalization for users with no ratings
    <ul>
      <li>But! If you have some <strong>movies with no ratings</strong>, you can also play with versions of the algorithm where you normalize the <strong>columns</strong></li>
      <li>BUT this is probably less relevant - probably shouldn‚Äôt recommend an unrated movie</li>
    </ul>
  </li>
  <li>To summarize, this shows how to use mean normalization preprocessing to allow your system to deal with users who have not yet made any ratings
    <ul>
      <li>‚Äì&gt; Means system <strong>recommend the best average rated products to the user we know little about</strong></li>
    </ul>
  </li>
</ul>
:ET