I"º$<p><br /></p>

<h2 id="apache-hadoop"><strong>Apache Hadoop</strong></h2>

<p><br /></p>

<ul>
  <li>hadoop is a framework that allows us to store and process large data sets in parallel and distributed fashion</li>
  <li>HDFS : Distributed File System - <strong>for data storage</strong></li>
  <li>Map Reduce : Parallel &amp; distributed <strong>data processing (MapReduce)</strong></li>
  <li>Master/Slave Architecture : HDFS, YARN
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/159821647-1285cb07-c5af-4d62-8da6-257497afef94.png" width="550" /></li>
    </ul>
  </li>
</ul>

<hr />

<p><br /></p>

<h2 id="hdfs"><strong>HDFS</strong></h2>

<p><br /></p>

<ol>
  <li>NameNode : Master
    <ul>
      <li>controls and distributes resources to each datanode</li>
      <li>receives all report from datanodes</li>
      <li>records metadata (<strong>information about data</strong> blocks e.g. location of files stored, size of files, permissions, hierarchy)</li>
    </ul>
  </li>
  <li>DataNode : Slave
    <ul>
      <li>store actual data</li>
      <li>serves read &amp; write requests from clients</li>
    </ul>
  </li>
  <li>Secondary NameNode
    <ul>
      <li>receive first copy of fsimage</li>
      <li>serves <strong>Checkpointing</strong> : combines editlog + fsimage and create newly updated fsimage and send it to NameNode
        <ul>
          <li>editlog : contains records of modifications in data</li>
          <li>editlog (new) : retains all new changes untill the next checkpoint happens and send it to editlog</li>
          <li>fsimage : contains total informations about data</li>
          <li>happens periodically (default 1 per 1h)</li>
        </ul>
      </li>
      <li><img src="https://user-images.githubusercontent.com/92680829/159822617-0653a4aa-bbd2-44d5-896b-0833515a36d6.png" width="380" /></li>
      <li>makes NameNode more available</li>
    </ul>
  </li>
</ol>

<h3 id="hdfs-data-blocks"><strong>HDFS Data Blocks</strong></h3>
<ul>
  <li>how the data is actually stored in datanodes?
    <ul>
      <li>Each file is stored on several HDFS blocks (default size is 128MB, but last node will only contain the remaining size)</li>
      <li>These datablocks are distributed across all the dataNodes in your Hadoop cluster</li>
    </ul>
  </li>
</ul>

<h3 id="advantages-of-distributing-data-file"><strong>Advantages of distributing data file</strong></h3>
<ul>
  <li>This kind of distributing file system is <strong>highly scalable (flexible)</strong> and efficient without making resources wasted</li>
  <li>Save your time by processing data in a parallel manner</li>
  <li><strong>Fault Tolerance</strong> : How Hadoop Adress DataNode Failure?
    <ul>
      <li>suppose one of the DataNode containing the datablocks crashed</li>
      <li>to prevent data loss from defective DataNode, we should makes multiple copies of data
        <ul>
          <li>Solution : <strong>Replication Factor</strong> - how many replicas are created per 1 datablock
            <ul>
              <li>Each of DataNode has different sets of copies of datablocks (thrice by default)</li>
              <li>Even if two of the copis become defective, we still have 1 left intact</li>
              <li><img src="https://user-images.githubusercontent.com/92680829/159824849-e72774d3-ae2a-4de2-bce7-5b159d3d2bfb.png" width="430" /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="hdfs-writing-mechanism"><strong>HDFS Writing Mechanism</strong></h3>
<ul>
  <li>Step 1. <strong>Pipeline Setup</strong>
    <ul>
      <li>ClientNode sends write request about Block A to NameNode</li>
      <li>NameNode sent IP addresses for DN 1,4,6 where block A will be copied</li>
      <li>First, CN asks DN1 to be ready to copy block A, and sequentially DN1 ask the same thing to DN4 and DN4 to DN6</li>
      <li>This is the Pipeline!
        <ul>
          <li><img src="https://user-images.githubusercontent.com/92680829/159825946-55b6ae8f-7c57-44cf-9838-f45f2aa88b37.png" width="500" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Step2 : <strong>Data Streaming</strong> (Actual Writing)
    <ul>
      <li>As the pipeline has been created, the client will push the data into the pipeline</li>
      <li>Client will copy the block (A) to DataNode 1 only.</li>
      <li>The remaining replication process is always done by <strong>DataNodes sequentially</strong>.
        <ul>
          <li>DN1 will connect to DN4 and DN4 will copy the block and DN4 will connect to DN6 and DN6 will copy the block</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Step3 : <strong>Acknowledgement Reversely</strong>
    <ul>
      <li>each DN will confirm that each step of replication succeed</li>
      <li>DN6 -&gt; DN4 -&gt; DN1 -&gt; Client -&gt; NameNode -&gt; Update Metadata</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/159828057-361d2462-598c-459b-9aa3-06591fc64a61.png" width="500" /></li>
    </ul>
  </li>
  <li><strong>Summary for Multiple Blocks</strong>
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/159828251-c8f41f6a-99ea-4425-a18d-6ca8f3ea64eb.png" width="520" /></li>
    </ul>
  </li>
</ul>

<h3 id="hdfs-reading-mechanism"><strong>HDFS Reading Mechanism</strong></h3>
<ul>
  <li><img src="https://user-images.githubusercontent.com/92680829/159828597-816c7875-e6b1-468b-94ca-e465e3184214.png" width="600" /></li>
</ul>

<hr />

<h2 id="map-reduce--parallel--distributed-processing"><strong>Map Reduce : Parallel &amp; Distributed Processing</strong></h2>

<ul>
  <li>Single file is splited into multiple parts and each is processed by one DataNode simultaneously</li>
  <li>This system allows much faster processing</li>
  <li>Map Reduce can be divided into 2 Distinct Steps
    <ol>
      <li>Map Tasks</li>
      <li>Reduce Tasks
        <ul>
          <li>Example : Overall MapReduce Word Counting Process</li>
          <li><img src="https://user-images.githubusercontent.com/92680829/159829821-c74ca6ec-2759-4089-a28d-e68b1b0118c8.png" width="600" /></li>
        </ul>
      </li>
    </ol>
  </li>
  <li>3 Major Parts of MapReduce Program
    <ul>
      <li>Mapper Code : how map tasks will process the data to prdouce the key-value pair</li>
      <li>Reducer Code : combine intermediate key-value pair generated by Mapper and give the final aggregated output</li>
      <li>Driver Code : specify all the job configurations (job name, input path, output path, etc..)</li>
    </ul>
  </li>
</ul>

<h3 id="yarn-yet-another-resource-negotiator--for-mapreduce-process"><strong>YARN (Yet Another Resource Negotiator) : for MapReduce Process</strong></h3>
<ul>
  <li>Cluster management component of Hadoop</li>
  <li>It includes Resource Manager, Node Manager, Containers, and Application Master
    <ul>
      <li><strong>Resource Manager (NameMode)</strong>
        <ul>
          <li>major component that manages application and job scheduling for the batch processes</li>
          <li>allocates first container for AppMaster</li>
        </ul>
      </li>
      <li><strong>Slave Nodes (DataNode)</strong>
        <ul>
          <li>Node Manager :
            <ul>
              <li>control task distribution for each DataNode in cluster</li>
              <li>report node status to RM : monitors containerâ€™s resource usage(memory, cpu..) and report to RM</li>
            </ul>
          </li>
          <li>AppMaster :
            <ul>
              <li>coordinates and manages individual application</li>
              <li>only run during the application, terminated as soon as MapReduce job is completed</li>
              <li>resource request : negotiate the resources from RM</li>
              <li>works with NM to monitor and execute the tasks</li>
            </ul>
          </li>
          <li>Container :
            <ul>
              <li>allocates a set of resources (ram, cpu..) on a single DataNode</li>
              <li>report MapReduce status to AppMater</li>
              <li>scheduled by RM, monitored by NM</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>YARN Architecture</strong>
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/159842914-d0031950-45ff-488c-904c-df9fc425d11e.png" width="600" /></li>
    </ul>
  </li>
</ul>

<h3 id="hadoop-architecture--hdfs-storage--yarn-mapreduce"><strong>Hadoop Architecture : HDFS (Storage) &amp; YARN (MapReduce)</strong></h3>

<ul>
  <li><img src="https://user-images.githubusercontent.com/92680829/159844204-ead81820-97a7-4d41-9216-a1d637544e6a.png" width="500" /></li>
</ul>

<hr />

<h2 id="hadoop-cluster"><strong>Hadoop Cluster</strong></h2>
<ul>
  <li><img src="https://user-images.githubusercontent.com/92680829/159844804-52c3292f-0898-4591-a96a-be80977fc0ac.png" width="500" /></li>
</ul>

<h3 id="hadoop-cluster-modes"><strong>Hadoop Cluster Modes</strong></h3>
<ul>
  <li>Multi-Node Cluster Mode
    <ul>
      <li>one cluster cosists of multiple nodes</li>
    </ul>
  </li>
  <li>Pseudo-Distributed Mode</li>
  <li>Standalone Mode</li>
</ul>

<h2 id="hadoop-ecosystem"><strong>Hadoop Ecosystem</strong></h2>
<ul>
  <li><img src="https://user-images.githubusercontent.com/92680829/159845221-238495fc-7b93-4183-87d5-efa7b3b46812.png" width="600" /></li>
</ul>
:ET