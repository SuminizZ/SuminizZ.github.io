I"(<p><br /></p>

<h2 id="neural-networks--classification"><strong>Neural Networks : Classification</strong></h2>
<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/92680829/156963145-e4a1b901-d390-4aee-8535-6be459d6169a.png" width="600" />
<br /></p>

<h3 id="cost-function-for-neural-network-with-sigmoidal-activation"><strong>Cost Function for Neural Network with Sigmoidal Activation</strong></h3>
<p><br /></p>

<ul>
  <li>The (regularized) logistic regression cost function is as follows;
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156687790-4532412e-706c-435c-b5aa-7d4a5f9145c3.png" width="600" /></li>
    </ul>
  </li>
  <li>For neural networks, regularized cost function is a generalization of this equation above (instead of one output, we generate k outputs)
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156964377-a4b5d0f8-d17a-4982-8ba4-438b4cb676bf.png" width="900" /></li>
      <li>finally, output is class that can maximize the probability that h(x) = 1</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="back-propagation-algorithm--to-minimize-cost-function"><strong>Back Propagation Algorithm : to minimize cost function</strong></h2>
<p><br /></p>

<ul>
  <li>need to compute <strong>1. J(Ɵ)</strong> and <strong>2. derivative of J(Ɵ)</strong></li>
  <li>Partial derivative of J(Ɵ)
    <ul>
      <li>Ɵ is indexed in three dimensions because we have separate parameter values for each node (j) in each layer (L) going to each node (i) in the following layer</li>
      <li>from j (in layer L) to i (in layer L+1)</li>
      <li>each layer has a Ɵ matrix associated with it</li>
      <li>We want to calculate the partial derivative of Ɵ with respect to a single parameter</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156965682-06c60ecd-8f30-4022-8d39-e4940990efb7.png" width="150" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="forward-propagation-algorithm-operates-as-follows"><strong>Forward propagation algorithm</strong> operates as follows**</h3>
<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Layer 1
   - a1 = x 
   - z2 = Ɵ1a1
- Layer 2
    - a2 = g(z2) (add a02), g here is sigmoidal function
    - z3 = Ɵ2a2
- Layer 3
    - a3 = g(z3) (add a03)
    - z4 = Ɵ3a3
- Output
    - a4 = hƟ(x) = g(z4)
</code></pre></div></div>

<p><br /></p>

<h3 id="backward-propagation-algorithm"><strong>Backward propagation algorithm</strong></h3>
<p><br /></p>

<ul>
  <li>to compute the partial derivatives</li>
  <li>For each node we can calculate (δjl) - this is the error of node j in layer l
    <ul>
      <li>we need to calculate error compared to the “real” value</li>
      <li>
        <p>the only real value we got is the result (actual y), so we have to <strong>start with the final output!!</strong></p>
      </li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156968787-5607b197-68f3-412f-88fb-6765357f2184.png" width="300" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>Given the above neural network as an example,</li>
  <li>first error :
    <ul>
      <li>δj4 = aj4 - yj</li>
      <li>[Activation of the unit] - [the actual value in the training set] (aj4 = hƟ(x)j)</li>
    </ul>
  </li>
  <li>
    <p>derivative 
  <img src="https://user-images.githubusercontent.com/92680829/156974170-fdcf87d4-78a0-40df-abfb-5b7d13ab2b47.png" width="250" /></p>

    <ul>
      <li>out (aj[L]) here is current node (aj[L])  : layer L  &lt;/br&gt;</li>
      <li><strong>delta(δ)</strong> here is from [ (ai[L+1] - y) * (ai[L+1])(1-ai[L+1]) ]  : layer L+1</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="summary"><strong>Summary</strong></h3>
<p><br /></p>

<ul>
  <li>
    <p>loop through the training set</p>
  </li>
  <li>
    <p>Set a1 (activation of input layer) = xi</p>
    <ul>
      <li>Perform forward propagation to compute a(L) for each layer (L = 1,2, … L)</li>
      <li>run forward propagation</li>
      <li>Then, use the <strong>output label</strong> to calculate the delta value for the output layer δL = aL - yi)</li>
      <li>Then, using back propagation we move back through the network from layer L-1 down to layer</li>
      <li>
        <p>Finally, use Δ to accumulate the partial derivative terms</p>

        <p><img src="https://user-images.githubusercontent.com/92680829/156976785-882bd053-cc84-45c5-8892-6fa77d49e721.png" width="300" />
 <br /></p>
      </li>
      <li>Note that
        <ul>
          <li>l = layer</li>
          <li>j = node in that layer</li>
          <li>i = the error of the affected node in the target layer</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="-proof-of-derivative-of-jɵ-"><strong>– proof of derivative of J(Ɵ) –</strong></h3>
<p><br /></p>

<p><a href="https://goofcode.github.io/back-propagation">if you want to know more detailed process of derivation, click here</a></p>
<ul>
  <li>with <strong>Chain Rule</strong></li>
</ul>

<p><img src="https://user-images.githubusercontent.com/92680829/156974547-774345eb-1ca5-44f9-b6de-741b29071138.png" width="350" />
<br /></p>

<ul>
  <li>(1) ai[L+1] - y</li>
  <li>(2) (ai[L+1]) * (1-ai[L+1])  : derivative of sigmodial function of z (net)
    <ul>
      <li>δi[L+1] = (1) * (2) = (ai[L+1] - y) * (ai[L+1])(1-ai[L+1])</li>
    </ul>
  </li>
  <li>(3) aj[L]</li>
  <li>(1) * (2) * (3) = updating value of W</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/92680829/156972765-21ffaccb-7fc0-4096-92af-bb0b891fd8ab.png" width="450" />
<br /></p>

<ul>
  <li>So we need δ to update the W (theta here), then</li>
  <li><strong>How to get δ</strong> : Back-Propagation</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/92680829/158723928-1ae750bf-da88-492d-951a-7ebebf167919.png" width="600" /></p>

<p><br /></p>

<h2 id="backpropagation-algorithm-intuition"><strong>BackPropagation Algorithm Intuition</strong></h2>
<p><br /></p>

<h3 id="forward-propagation-to-get-activation-values-of-each-layer"><strong>Forward Propagation to get activation values of each layer</strong></h3>
<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/92680829/156980584-7dd139ce-5f8d-46a8-af0c-fc7cb0917cff.png" width="700" /></p>

<ul>
  <li>
    <ol>
      <li>get z (each x multiplied by Ɵ)</li>
    </ol>
  </li>
  <li>
    <ol>
      <li>calculate g(z) (in this case, sigmoidal function)</li>
    </ol>
  </li>
</ul>

<p><br /></p>

<h3 id="back-propagation-to-minimize-the-cost-function"><strong>Back Propagation to minimize the cost function</strong></h3>
<p><br /></p>

<ul>
  <li>δ term on a unit as the “error” of cost for aj(L) (unit jth in layer L)</li>
  <li><strong>δi[L+1] = (ai[L+1] - y) * (ai[L+1])(1-ai[L+1])</strong></li>
  <li>J(Ɵ) = cost(i)</li>
  <li>
    <p>z : net</p>
  </li>
  <li><img src="https://user-images.githubusercontent.com/92680829/156982115-5feb0f67-1e09-4cfd-a7ae-60eb8dcb5a9a.png" width="200" /></li>
</ul>

<p><br /></p>

<ul>
  <li><strong>What is Back propagation?</strong>
    <ul>
      <li>it calculates the δ, and those δ values are the weighted sum of the next layer’s delta values, weighted by the parameter associated with the links</li>
      <li>δ2(2) = [Ɵ12(2) * δ1(3)] + [Ɵ22(2) * δ2(3)] + [Ɵ32(2) * δ3(3)]</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156984818-d43c4172-2d87-435c-a0a7-d16d284f62a9.png" width="700" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="gradient-checking"><strong>Gradient Checking</strong></h2>
<p><br /></p>

<ul>
  <li>back prop algorithm is quite susceptible to many subtle bugs, which leads you to get a higher levels error than you do in bug-free implementation</li>
  <li>there is a good way to deal with most issues associated with the buggy back-prop, called <strong>Gradient Checking</strong></li>
  <li>get Numerical Approximates of Gradient
  &lt;/br&gt;
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156989063-78e4ee34-3f47-40b3-bf05-7df01685c238.png" width="600" /></li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156990466-c6f7f3c7-797c-4e26-a9a7-7149ea1b4fd8.png" width="500" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>then, check whether approximates have similar value with derivative gained from back-prop</li>
</ul>

<p><br /></p>

<h2 id="random-initialization"><strong>Random Initialization</strong></h2>
<p><br /></p>

<ul>
  <li>Pick random small initial values for all the theta values of first input layer
    <ul>
      <li>If you start them on zero (which does work for linear regression) then the algorithm fails - all activation values for each layer are the same</li>
      <li>if you set all theta by an identical number, you’ll faill to break the symmetry, as all hidden units will repeatedly get exactly the same signal from previous layers</li>
    </ul>
  </li>
  <li>So chose random values to break the symmetry!!
    <ul>
      <li>Between 0 and 1, then scale by epsilon (where epsilon is a constant)</li>
      <li>rand(10, 11) (10x11 vector) * 2(init_epsilon) - (init_epsilon)
        <ul>
          <li>[-epsilon, +epsilon]</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="putting-it-all-together"><strong>Putting it all together</strong></h2>
<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/92680829/156998625-cca147dc-f987-49da-a646-53c22354404b.png" width="600" /></p>

<ul>
  <li>Use gradient descent or an advanced optimization method with back propagation to try to minimize J(Ɵ) as a function of parameters Ɵ
    <ul>
      <li>But, J(Ɵ) is non-convex –&gt; could be susceptible to local minimum</li>
      <li>In practice this is not usually a huge problem</li>
      <li>Can’t guarantee programs to find global optimum, instead, should find <strong>good local optimum at least</strong>
  <img src="https://user-images.githubusercontent.com/92680829/157000799-64e649d9-73e5-44ad-b1e1-43daf5fb5a1a.png" width="600" /></li>
    </ul>
  </li>
</ul>
:ET