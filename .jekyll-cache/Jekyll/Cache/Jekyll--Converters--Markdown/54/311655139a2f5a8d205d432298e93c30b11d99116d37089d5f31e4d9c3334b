I"û<p><br /></p>

<h2 id="poor-or-appropriate-fitting"><strong>Poor or Appropriate Fitting</strong></h2>
<p><br /></p>

<ul>
  <li>Data Fitting in Linear Regression</li>
  <li>UnderFitting (Fit a linear function)
    <ul>
      <li>high bias : there‚Äôs a strong preperception that data would follow the specific linear correlation</li>
      <li>generalize too much</li>
      <li>poor at predicting both training dataset and new data</li>
    </ul>
  </li>
  <li>Apporpriate Fitting (Fit a quadratic function)
    <ul>
      <li>appopriate generalization</li>
    </ul>
  </li>
  <li>OverFitting (Fit a 4th order polynomial)
    <ul>
      <li>high polynomial function used ‚Äì&gt; weak generalization</li>
      <li>good at predicting training set, but not good at new untrained data</li>
      <li>too many features with small data lead to overfitting</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li><img src="https://user-images.githubusercontent.com/92680829/156711914-22c2922c-7146-453b-bc0c-e72ddd79a271.png" width="600" /></li>
</ul>

<p><br /></p>

<ul>
  <li>
    <p>Data Fitting in Logistic Classification</p>

    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156712079-6671a1c0-bbe8-4407-8e9b-a0c63d4bad83.png" width="600" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="adress-overfitting"><strong>Adress Overfitting</strong></h2>
<p><br /></p>

<ol>
  <li>Reduce the number of features
    <ul>
      <li>Manually select which features to keep and which to not</li>
      <li>Model Selection Algorithm</li>
      <li>problems : there‚Äôre possibilities that we might loose valuable data that can actually contirbutes to improve predicting power of model</li>
    </ul>
  </li>
  <li>Regularization
    <ul>
      <li>Keep all the features, but reduce the magnitude/values of parameters Œ∏j, thus reducing the amount of effect that each variable can exert</li>
      <li>penalize the features of less relevant to y and limit their contribution in predicting</li>
      <li>small values for parameters leads to ‚Äúsimpler‚Äù hypothesis and make the model less prone to overfitting</li>
      <li>Then, how we can decide which parameters we should shrink in advance?
        <ul>
          <li>1) <strong>Add a term at the end</strong>
            <ul>
              <li>This regularization term shrinks every parameter</li>
              <li>By convention you don‚Äôt penalize Œ∏0 - minimization is from Œ∏1 onwards</li>
              <li><strong>Œª is the regularization parameter</strong>
                <ul>
                  <li>(1) Want to fit the training set well</li>
                  <li>(2) Want to keep parameters small</li>
                </ul>
              </li>
              <li>
                <p><img src="https://user-images.githubusercontent.com/92680829/156714814-49b0a6f6-0aef-4b23-8849-1b7cb579d604.png" width="500" />
  <br /></p>
              </li>
              <li>What if Œª too big up to like 1e100 ? ‚Ä¶ algorithm results in underfitting
                <ul>
                  <li>y alomost equals to Œ∏0</li>
                  <li>all other parameters almost equal to 0 compared to Œª</li>
                </ul>
              </li>
              <li>what if Œª too small? ‚Ä¶ can‚Äôt achieve the original goal, that is to limit the effects of each parameter</li>
              <li><img src="https://user-images.githubusercontent.com/92680829/156717727-8a91b545-5f37-475d-a377-ddf745806908.png" width="400" /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<p><br /></p>

<h2 id="regularized-linear-regression"><strong>Regularized Linear Regression</strong></h2>
<p><br /></p>

<ul>
  <li>Previous Gradient Descent
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156717937-38fa5fe1-2a99-4b82-8345-29757925bb84.png" width="400" /></li>
    </ul>
  </li>
  <li>Regularized Gradient Descent
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156718155-014e2729-3a6a-4c24-9f30-5880cd113574.png" width="500" /></li>
    </ul>
  </li>
  <li>the term (1-Œ±(Œª/m))
    <ul>
      <li>necessarily smaller than 1 ‚Äì&gt; penalize each feature Œ∏j by the magnitude of Œª</li>
      <li>being multipled for every repeat, being penalized repeatedly</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="regularization-by-normal-equation"><strong>Regularization by Normal Equation</strong></h2>
<ul>
  <li><img src="https://user-images.githubusercontent.com/92680829/156720941-c0b195ca-e3bf-41b7-8d1f-f10f5802fdff.png" width="500" /></li>
</ul>

<p><br /></p>

<h2 id="regularized-logistic-regression"><strong>Regularized Logistic Regression</strong></h2>
<p><br /></p>

<ul>
  <li>quite similar with regularized linear regression</li>
  <li>but, obviously the hypothesis is very different</li>
  <li><img src="https://user-images.githubusercontent.com/92680829/156721480-d112d1d2-f5fa-4f2d-81bf-6ff2dcf07d7a.png" width="450" /></li>
</ul>

:ET