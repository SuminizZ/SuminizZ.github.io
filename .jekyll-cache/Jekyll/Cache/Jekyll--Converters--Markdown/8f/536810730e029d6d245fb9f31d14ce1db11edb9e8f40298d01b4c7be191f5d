I"ƒ<p><br /></p>

<h3 id="quasi-newton-method">Quasi-Newton method</h3>
<ul>
  <li>Iterative optimization algorithm used to solve unconstrained nonlinear optimization problems without explicit calculation for Jacobian or Hessian matrices.</li>
  <li>Basic idea behind the Quasi-Newton method is to approximate the Hessian matrix of the objective function using an iterative formula that includes information from the gradient of the objective function and the difference between the current and previous iterates.</li>
</ul>

<h3 id="broydens-method">Broydenâ€™s Method</h3>
<ul>
  <li>to find the solution to a vector-valued function that represents the nonlinear system of equations that Broydenâ€™s method is used to solve.
    <ul>
      <li>$f(x) \,= \,0$</li>
      <li>$J_{k+1} = J_k + \frac{(f(x_{k+1}) - f(x_k) - J_k(x_{k+1} - x_k)) \cdot (x_{k+1} - x_k)^T}{\lVert x_{k+1} - x_k \rVert^2}$</li>
    </ul>
  </li>
  <li>constructs an approximation to the Jacobian matrix at each iteration using an update formula that involves the current iterate and the change in the function values between the current and previous iterates. The update formula is based on the idea that the change in the Jacobian matrix is proportional to the change in the function values.</li>
</ul>

<p>where f is a vector-valued function of x. The idea behind Broydenâ€™s method is to construct an iterative sequence of approximate solutions, starting with an initial guess x_0. At each iteration, Broydenâ€™s method constructs an approximation to the Jacobian matrix of f at the current iterate, and uses this approximation to update the current iterate.</p>

<h3 id="notes">Notes</h3>

<ul>
  <li><a href="https://drive.google.com/file/d/1bg4O_4jck2AGwa9tO4XqU89ovHq4Kij5/view?usp=share_link" target="_blank"><span style="color:purple"><strong>Convex Optimization 1 - Quasi-Newton Method : Broydenâ€™s Method</strong></span></a>s</li>
</ul>

:ET