I"Ÿ&<p><br /></p>

<h2 id="outlines">OUTLINES</h2>

<ol>
  <li>Locally Weighted Regression</li>
  <li>Probabilistic Interpretation (Maximum Log Likelihood)</li>
  <li>Logistic Regression</li>
  <li>Newtonâ€™s method
<br /></li>
</ol>

<hr />

<p><br /></p>

<h1 id="1-locally-weighted-regression">1. Locally Weighted Regression</h1>

<ul>
  <li>fitting a model to a dataset by giving more weight to the data points that are close to the point being predicted</li>
  <li>Non-parametric learning algorithm where the number of parameters you need to keep grows with the size of the dataset, while parametric learning has fixed set of parameters.</li>
</ul>

<p><br /></p>

<h2 id="11-cost-function-to-minimize">1.1. Cost function to minimize</h2>

<p>â€ƒâ€ƒâ€ƒâ€ƒ $\normalsize \sum\limits_{i=1}^{m} \omega^{i}(y^{i} - \theta^{T}x^{i})^{2}$ â€ƒ where â€ƒ $\normalsize  \omega^{i} = exp(\frac{- (x^{i} - x)^{2}}{2}) $ <br /></p>

<ul>
  <li>Weighting function $\omega^{i}$ : used to assign a weight to each training example based on its distance from the point being predicted.</li>
  <li>$x^{i} \,$ : data points that youâ€™re processing</li>
  <li>$x \,$ : point of interest to be predicted</li>
  <li>Automatically gives more weight to the points of close to $x$ (max weight = 1)</li>
  <li>Points too far from the point of interest will fade away with infinitesimally small weight $\omega^{i}$</li>
  <li>locally fit an almost straight line centered at the point to be predicted. <br />
<br /></li>
</ul>

<p><img src="https://user-images.githubusercontent.com/92680829/227078685-2665a976-5c46-42af-a9a8-893d3c074a8c.png" width="570" /></p>

<h2 id="12-normalsize-tau--bandwidth-parameter">1.2. $\normalsize \tau$ : bandwidth parameter</h2>
<p><br /></p>

<p>â€ƒâ€ƒ $\large \omega^{i} = exp(\frac{-(x^{i} - x)^{2}}{2\tau^{2}})$</p>

<ul>
  <li>Weight term depends on the choice of $\large \tau$</li>
  <li>this controls how quickly the weight is adjusted by the distance of data points from the point to be predicted.</li>
  <li>called as bandwith parameter as it determines the width of linearly fitted local area with respect to the query point.</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="2-probabilistic-interpretation-of-least-mena-square">2. Probabilistic Interpretation of Least Mena Square</h1>
<p><br /></p>

<ul>
  <li>Conver the problem from <code class="language-plaintext highlighter-rouge">minimizing error term</code> to <code class="language-plaintext highlighter-rouge">maximize the probability</code> of $y^{i}$ given with $x^{i}\,$ parameterized by  $\theta$</li>
  <li>Can make an assumption that $\epsilon^{i}$ are distributed IID (independently and identically distributed)</li>
  <li>
    <p>According to the Central Limit Theorem (CLT) with large enough training examples, $\epsilon^{i}$ converges to Gaussian Distribution <br /></p>

    <p>â€ƒâ€ƒ $\normalsize \epsilon^{i} \sim~ \mathcal{N}(\mu = 0,\,\sigma^{2})\,$) <br /></p>
  </li>
  <li>This implies that : <br />
    <ul>
      <li>the distribution of $y^{i}$ given $x^{i}\,$ parameterized by  $\theta$ follows the Gaussian Distribution of average $\theta^{T}x$ and variance $\sigma^{2}$</li>
    </ul>

    <table>
      <tbody>
        <tr>
          <td>$\normalsize p(y^{i}\,</td>
          <td>\,x^{i};\,\theta)\, \sim~ \,\frac{1}{\sqrt{2\pi}\sigma}exp(\frac{-(y^{i}\,-\theta^{T}x^{i})}{2\sigma^{2}})$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The function $p(y^{i}\,</td>
          <td>\,x^{i};\,\theta)$ can be explicitly veiwed as the likelihood of $y$ for a varying $\theta$  <br /></td>
        </tr>
      </tbody>
    </table>

    <table>
      <tbody>
        <tr>
          <td>â€ƒâ€ƒâ€ƒ $\normalsize L(\theta)\,=\,p(y^{i}\,</td>
          <td>\,x^{i};\,\theta)$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h2 id="likelihood-function---ltheta">Likelihood Function : $ L(\theta)$</h2>

<ul>
  <li>
    <p>as weâ€™ve made an IID assumption, the likelihood for entire training set can be computed as the product of each probability of $y^{i}$. <br /></p>

    <p><img src="https://user-images.githubusercontent.com/92680829/227084393-1c03057f-b858-41a7-a524-9081b2aad3c1.png" width="430" /></p>
  </li>
  <li>Given this likelihood function, our probelm turn into finding the sets of $\theta$ that maximizies the probabilistic distribution of $y$ given by the $x$</li>
  <li>
    <p>As the function $L(\theta)$ contains exponential term, we can make it simpler by taking log to the function to make it linear and also turn the product into summed form.</p>

    <p><img src="https://user-images.githubusercontent.com/92680829/227085579-0ba83582-5630-4f4f-a5a9-9f9579d34b9a.png" width="450" /> <br /></p>
  </li>
  <li>Hence, maximizing $\ell(\theta)$ actually becomes same as minimizing $\sum\limits_{i=1}^{m}(y^{i}\,-\,\theta^{T}x^{i})$, which is the error term weâ€™ve seen before.</li>
  <li>To summarize, optimizing $\theta$ with least-square approach to error term ($\epsilon^{i}$) corresponds to finding $\theta$ that gives maximized likelihood distribution of $p(y^{i})$</li>
</ul>

<hr />

<h1 id="classification-and-logistic-regression">Classification and Logistic Regression</h1>

<ul>
  <li>Logistic regression is used for the binary classification in which y takes only two discrete values, 0 and 1.</li>
  <li>LR models the probability that the $y^{i}$ takes on a particular value given the $x^{i} parameterized by \theta$.</li>
  <li>
    <p>Logistic function, which maps the input values to a value between 0 and 1, representing the probability of $y^{i}$ taking the value 1.</p>
  </li>
  <li>
    <p>To map the input values ($x$) to proability with range [0, 1], we need to change the form of <code class="language-plaintext highlighter-rouge">hypothese function using sigmoid function</code> that converts the input values defined from negative to positive infinity into the output values from 0 to 1. <br /></p>

    <p>â€ƒâ€ƒâ€ƒ $ \normalsize h_{\theta}(x) = g(\theta^{T}x) = \large \frac{1}{1+e^{-\theta^{T}x}} $ where $\normalsize \, g(z)\,=\,\frac{1}{1+e^{-z}}$</p>

    <p><img src="https://user-images.githubusercontent.com/92680829/227090153-b7ee412d-1135-40b8-9520-d486cb20cf30.png" width="340" /> <br /></p>

    <ul>
      <li>$g(z)$ goes toward 1 as z goes to positive infinity and 0 as z goes to negative infinity, bounded by [0, 1]</li>
    </ul>
  </li>
</ul>

<h2 id="maximum-likelihood-estimator">Maximum Likelihood Estimator</h2>

<ul>
  <li>To fit the best estimate of $\theta$, we need to define the likelihood function for logistic classifier same as we did for linear regression.</li>
  <li>
    <p>Probaility Function : get $(h_{\theta}(x))$ when y = 1 and get $1\,-\,(h_{\theta}(x))$ when y equals to 0 <br /></p>

    <table>
      <tbody>
        <tr>
          <td>â€ƒâ€ƒâ€ƒ $ P(y\,=\,1\,</td>
          <td>x;\theta\,)\,=\,h_{\theta}(x) $ <br /></td>
        </tr>
        <tr>
          <td>â€ƒâ€ƒâ€ƒ $ P(y\,=\,0\,</td>
          <td>x;\theta\,)\,=\,1\,-\,h_{\theta}(x)$ <br /></td>
        </tr>
      </tbody>
    </table>

    <table>
      <tbody>
        <tr>
          <td>â€ƒâ€ƒâ€ƒ Both combined, $ P(y\,</td>
          <td>\,x;\theta\,)\,=\,(h_{\theta}(x))^{y}\,(1\,-h_{\theta}(x))^{1-y} $ <br /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Each data point is in IID, likelihood for entire dataset equals to product of the probability for each $y^{i}$ <br />
  <img src="https://user-images.githubusercontent.com/92680829/227101948-2e70c490-f08a-4be4-9c4f-87c1101ef88f.png" width="400" /> <br /></p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">Log Likelihood</code> for easier Optimization : <br /></li>
    </ul>

    <p>â€ƒâ€ƒâ€ƒ $\normalsize \ell(\theta)\,=\,logL(\theta) = \sum\limits_{i=1}^{m}\,y\,log(h_{\theta})\,+\,(1-y)\,log(1\,-\,h_{\theta}(x))$</p>
  </li>
</ul>

<h2 id="maximization-with-gradient-ascent">Maximization with Gradient Ascent</h2>

<p>â€ƒâ€ƒâ€ƒ $\normalsize \theta_{j} := \theta_{j} \,\, + \,\, \alpha\frac{\partial \ell(\theta)}{\partial \theta_{j}}$ <br /></p>

<p><img width="566" alt="Screen Shot 2023-03-23 at 9 48 42 PM" src="https://user-images.githubusercontent.com/92680829/227210477-1ded6090-2cc8-4e56-953c-1caaecd7c4d8.png" /> <br /></p>

<ul>
  <li>The explicit form of optimizing equation looks almost identical with gradient descent for linear regression, but the hypotheses function ($h_{\theta}(x)$) is different.</li>
</ul>

<hr />

<h1 id="newtons-algorithm">Newtonâ€™s Algorithm</h1>

<ul>
  <li>Newtonâ€™s algorithm, also known as Newton-Raphson method, is an iterative numerical method for finding the roots of a differentiable function (root : the point where $ f(x)\, = \,0$).</li>
  <li>
    <p>Finds the root of first derivative of log likelihood function ($\ellâ€™(\theta)$) using sercond derivative.
<br /></p>
  </li>
  <li>
    <p><strong>Algorithm</strong> <br /></p>

    <p><img width="481" alt="Screen Shot 2023-03-23 at 9 48 47 PM" src="https://user-images.githubusercontent.com/92680829/227210836-e52b06ba-6d4a-48dc-bd3f-1560d8d609f1.png" /> <br /></p>

    <ol>
      <li>set initial $\theta_{j}$ as random value and approximates next optimized $\theta_{j}$ by drawing a line tangent to the function at the currest guess of $\theta$</li>
      <li>solve for the point where that linear function equals to zero.</li>
      <li>repeat 1. and 2. untll covergence of $\theta$</li>
    </ol>
  </li>
  <li>Advantage of Newtonâ€™s method is that it takes less computations needed to converge each $\theta$</li>
  <li>But the amount of computations grows with the number of parameters to fit.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>
:ET