I"O<p><br /></p>

<h3 id="broydens-method">Broyden’s Method</h3>

<ul>
  <li>to find the solution to a vector-valued function that represents the nonlinear system of equations that Broyden’s method is used to solve.
    <ul>
      <li>$f(x) \,= \,0$</li>
      <li>for example, in logistic regression, the gradient of the likelihood function is used as the vector-valued function f(x).</li>
    </ul>
  </li>
  <li>
    <p>iterative update formula : $J_{k+1} = J_k + \frac{(f(x_{k+1}) - f(x_k) - J_k(x_{k+1} - x_k)) \cdot (x_{k+1} - x_k)^T}{\lVert x_{k+1} - x_k \rVert^2}$</p>
  </li>
  <li>constructs an approximation to the Jacobian matrix</li>
  <li>The update formula is based on the idea that the change in the Jacobian matrix is proportional to the change in the function values.</li>
</ul>

<p><br /></p>

<h3 id="derivation">Derivation</h3>

<ul>
  <li><a href="https://drive.google.com/file/d/1vjIsY3McleGxrU_dj8fz4MbLyVLGt6hA/view?usp=share_link" target="_blank"><span style="color:purple"><strong>Convex Optimization 2 - Quasi-Newton Method : SR1, BFGS, DFP</strong></span></a>s</li>
</ul>

:ET