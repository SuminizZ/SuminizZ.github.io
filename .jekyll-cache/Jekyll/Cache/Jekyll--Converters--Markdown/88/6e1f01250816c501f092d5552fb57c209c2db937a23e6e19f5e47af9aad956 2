I"<p><br /></p>

<h3 id="contents">Contents</h3>
<ul>
  <li>Entropy, Cross Entropy, Conditional Entropy</li>
  <li>KL Divergence</li>
  <li>Mutual Information</li>
  <li>KL Divergence of Two Different Normal Distribution &amp; Bernoulli Distribution</li>
</ul>

<h3 id="notes">Notes</h3>
<ul>
  <li><a href="https://drive.google.com/file/d/19bcc1joSFlIA31pPV2-ShzYikXPPMwIb/view?usp=share_link" target="_blank"><span style="color:purple"><strong>Entropy, KL Divergence, and Mutual Information</strong></span></a></li>
</ul>

:ET