I"ü)<p><br /></p>

<h2 id="outlines">OUTLINES</h2>

<ul>
  <li>Generative Learning Algorithms</li>
  <li>GDA</li>
  <li>Naive Bayes</li>
</ul>

<p><br /></p>

<h1 id="1-generative-learning-algorithms">1. Generative Learning Algorithms</h1>

<p><br /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Generative Learning Algorithm</code>
    <ul>
      <li>model the underlying distribution of input features separately for each class (label, y)</li>
      <li>first model the $p(y)$ and $p(x\, |\ \,y)$ and use <code class="language-plaintext highlighter-rouge">Bayes Rule</code> to derive the posterior distribution of y given x</li>
      <li>match new example to each model and find the class (y) that maximizes $p(y\, |\ x)$</li>
      <li>include Naive Bayes, Gaussian Mixture Models (GDA), and Hidden Markov Models</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Discriminative Learning Algorithm</code>
    <ul>
      <li>mapping the input features and output value $p(y\, |\ \,x)$</li>
      <li>directly predict the output based on input variables weighted by learned parameters</li>
      <li>no need to know underlying distribution of input space</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="2-gaussian-discriminative-analysis">2. Gaussian Discriminative Analysis</h1>

<ul>
  <li>as one of the generative learning algorithms, this model makes an assumption that $p(x\, |\ \,y)$ follows multivariate normal distribution</li>
</ul>

<p><br /></p>

<h2 id="21-multivariate-normal-distribution">2.1. Multivariate Normal Distribution</h2>

<p><br /></p>

<ul>
  <li>
    <p>$p(x  |\  y)$ is parameterized by mean vector and Covariance matrix</p>

    <ul>
      <li>Mean vector : $\normalsize \mu\,\in\mathbb{R}^{n}$</li>
      <li>Covariance matrix : $\normalsize \Sigma \in \mathbb{R}^{n \times n}$, where $\Sigma \geq 0$ is symmetric and positive definite <br /></li>
    </ul>

    <p>‚ÄÉ‚ÄÉ‚ÄÉ $\normalsize p(x\, |\ \,y)\, \sim \,N(\mu,\,\Sigma)$ <br /></p>

    <p>‚ÄÉ‚ÄÉ‚ÄÉ $\normalsize p(x ; \,\mu, \,\Sigma) = \frac{1}{(2\pi)^{n/2}  |\ \Sigma |\ ^{1/2}}\,exp(-\frac{1}{2}\,(x - \mu)^{T}\,\Sigma^{-1}\,(x-\mu))$ <br /></p>

    <p>‚ÄÉ‚ÄÉ‚ÄÉ $\normalsize E(x)\, =\, \mu$ <br /></p>

    <p>‚ÄÉ‚ÄÉ‚ÄÉ $\normalsize \,Cov(X) = E((X\,-\,E(X))(X\,-\,E(X))^{T})\,=\, E(XX^{T}) - E(X)E(X)^{T}$ <br /></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Density of Multivariate Gaussian Distribution</code> varies by $\Sigma$ and $\mu$ <br /></p>

    <ul>
      <li>Diagnal entries of $\Sigma$ : determines the compression of pdf with respect to the direction parallel to each axis
        <ul>
          <li>$\Sigma = I$ : standard normal distribution</li>
          <li>each represents pdf with $\Sigma$ equals to I , 2I, 0.4I, respectively</li>
        </ul>

        <p><img width="781" alt="Screen Shot 2023-03-29 at 9 10 27 PM" src="https://user-images.githubusercontent.com/92680829/228531295-aaf9f6cd-530f-4700-b19b-724aaecb10c5.png" /> <br /></p>
      </li>
      <li>Off-diagonal entries (symmetric) : determines the compression towards the $45^{\circ}$ line between the axes of each feature
        <ul>
          <li>$\Sigma = \begin{bmatrix} 1\quad 0 \ 0\quad 1 \end{bmatrix}$,  $\Sigma = \begin{bmatrix} 1\quad 0.5 \ 0.5\quad 1 \end{bmatrix}$, $\Sigma = \begin{bmatrix} 1\quad 0.8 \ 0.8\quad 1 \end{bmatrix}$ <br /></li>
        </ul>

        <p><img width="781" src="https://user-images.githubusercontent.com/92680829/228532809-6ac82dc4-2840-4591-a8f0-042206faf67b.png" /></p>
      </li>
      <li>varying $\mu$ moves the distribution along the axis</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="22-the-gaussian-discriminant-analysis-gda-model">2.2. The Gaussian Discriminant Analysis (GDA) Model</h2>

<p><br /></p>

<ul>
  <li>
    <p>classification problem in which input features $x$ are continuous random variables distributed in normal form and $y \in {0, 1}$ follows Bernoulli distribution</p>

    <p><img width="220" alt="Screen Shot 2023-03-29 at 9 30 34 PM" src="https://user-images.githubusercontent.com/92680829/228535989-608a5252-8f7e-42e8-998e-1ac155dc3a96.png" /></p>
  </li>
  <li>
    <p>tries to maximize the log-likelihood, which is the product of $p(x^{i}, y^{i} ; \phi, \mu_{0}, \mu_{1}, \Sigma)$</p>

    <p>‚ÄÉ‚ÄÉ $\normalsize \ell(\phi, \mu_{0}, \mu_{1}, \Sigma) = log\,\prod\, p(x^{i}, y^{i} ; \phi, \mu_{0}, \mu_{1}, \Sigma)$</p>

    <p>‚ÄÉ‚ÄÉ using Bayes Rule, can be expressed as <br /></p>

    <p>‚ÄÉ‚ÄÉ $\normalsize log\,\prod\, p(x^{i}\, |\ \, y^{i} ; \phi, \mu_{0}, \mu_{1}, \Sigma)\,p(y^{i}\,;\,\phi)$</p>
  </li>
  <li>
    <p>Each distribution (class y=0 and y=1),</p>

    <p><img width="600" alt="Screen Shot 2023-03-29 at 9 33 06 PM" src="https://user-images.githubusercontent.com/92680829/228540713-5ed10a55-78f1-4110-bf13-884a8d2fd5f1.png" /></p>
  </li>
  <li>
    <p>the result of MLE : By maximizing the $\ell$ with respect to each paramter, find the best estimates of the parameters,</p>

    <p><img width="420" alt="Screen Shot 2023-03-29 at 9 52 59 PM" src="https://user-images.githubusercontent.com/92680829/228541298-5077b02f-52a2-4d6f-ac69-3501035cefcd.png" /></p>
  </li>
  <li>
    <p>Predcit : Then, we can find the class of each training example that maximizes the log likelihood function</p>

    <p>‚ÄÉ‚ÄÉ $\normalsize y^{i} = argmax{\,p(y^{i}\, |\ \,x^{i})} = argmax(\,\large \frac{p(x^{i}  |\  y^{i})\,p(y^{i})}{p(x^{i})})$</p>

    <p>‚ÄÉ‚ÄÉ $p(x^{i})$ is no more than a common constant for both classes, can ignore the demoninator.</p>

    <p>‚ÄÉ‚ÄÉ Hence, $\normalsize y^{i} = argmax(\,\large p(x^{i}  |\  y^{i})\,p(y^{i}))$</p>
  </li>
  <li>
    <p>Pictorically, what the algorithm is actually doing can be seen in as follows,</p>

    <p><img width="472" alt="Screen Shot 2023-03-29 at 10 07 05 PM" src="https://user-images.githubusercontent.com/92680829/228544989-00d562a2-cb48-466f-af8f-eeea629014aa.png" /> <br /></p>

    <ul>
      <li>In summary, GDA models the distribution of input features $p(x  |\  y=0)$ and $p(x  |\  y=1)$ and calculate the $p(y^{i}  |\  x^{i})$ as a product of $p(x^{i}  |\  y^{i}) p(y^{i})$ using Bayes rule.</li>
      <li>Then find the most likely output, maximizing the probability</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="23-gda-vs-logistic-regression">2.3. GDA vs Logistic Regression</h2>

<p><br /></p>

<ul>
  <li>
    <p>If we view the quantity $p(y=1 \,  |\ \, x \,;\, \phi, \mu_{0}, \mu_{1}, \Sigma)$ as the function of $x$, we can find that it can actually be expressed in the following form,</p>

    <p>‚ÄÉ‚ÄÉ $p(y=1 \,  |\ \, x \,;\, \phi, \mu_{0}, \mu_{1}, \Sigma)\,=\, \large \frac{1}{1\,+\,e^{-\theta^{T}x}}$ ‚ÄÉ , where $\theta$ is an appropriate function of $\phi, \mu_{0}, \mu_{1}, \Sigma$</p>
  </li>
  <li>
    <p>The converse, however, is not true. (logistic regression doesn‚Äôt guarantee normally distributed x). <br /> This means that GDA is stronger modeling assumption than logistic regression. <br /> Hence, as long as the assumption is correct, GDA can make better prediction than logistic regression.</p>
  </li>
  <li>
    <p>In contrast, logistic regression is less sensitive to incorrect modeling assumptions so that it‚Äôs not significantly affected by the actual distrtibution of data (for example, Poisson distribution also makes $p(y |\ x)$ logistic)</p>
  </li>
  <li>
    <p>To summarize, GDA can be more efficient and has better fit to the data when the modeling assumptions are at least approximately correct. <br /> Logistic regression makes wearker assumptions, thus more robust to deviations from the modeling assumptions</p>
  </li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="3-naive-bayes">3. Naive Bayes</h1>
<ul>
  <li>Probabilistic classifiers based on applying Bayes‚Äô theorem with strong Naive Bayes (NB) assumptions between the features</li>
  <li>NB assumption assumes that each input feature is conditionally independent to each other given y (class), which is highly unlikely in reality.</li>
  <li>this algorithm still works okay even with this very ‚Äúnaive‚Äù assumption and provides clear advantage in terms of computational efficiency</li>
  <li>But for the data where input features are strongly correlated, the assumptions significantly limit its accuracy.</li>
</ul>

<p><br /></p>

<h2 id="31-application-of-nb-algorithm-as-a-spam-classifier">3.1 Application of NB Algorithm as a Spam Classifier</h2>

<ul>
  <li>build a spam classifier that automatically classifies the email into spam or non-spam using Naive Bayes algorithm</li>
  <li><code class="language-plaintext highlighter-rouge">Training set</code> :
    <ul>
      <li>given an email with labeled with 1 for spam ($y^{i} = 1$) and 0 for non-spam ($y^{i} = 0$)</li>
      <li>construct a feature vector whose lengith is equal to the number of words in vocab dictionary and each $jth$ feature represents whether $jth$ vocabulary is present in the mail $(x^{i}<em>{j} = 1)$ or not $(x^{i}</em>{j} = 0)$</li>
    </ul>

    <p>‚ÄÉ‚ÄÉ $ith$ email : $x^{i} = \begin{bmatrix} 1 \ 0 \ 0 \ .\.\.\1\0 \end{bmatrix}$</p>
  </li>
  <li>
    <ol>
      <li>model $\normalsize p(x |\ y)$ :
        <ul>
          <li>use NB assumption that features are conditionally independent within a class</li>
        </ul>
      </li>
    </ol>

    <p><img width="650" alt="Screen Shot 2023-04-01 at 1 39 30 PM" src="https://user-images.githubusercontent.com/92680829/229265955-d73e14f6-782a-443e-8883-894ec3de1eab.png" /></p>
  </li>
  <li>
    <ol>
      <li>Log-Likelihood function</li>
    </ol>

    <p>‚ÄÉ‚ÄÉ $\normalsize L(\phi_{y}, \phi_{(j |\ y=0)}, \phi_{(j |\ y=1)}) = \prod_{i=1}^{m} \,p(x^{i}, y^{i})$</p>

    <p>‚ÄÉ‚ÄÉ $p(x^{i}, y^{i}) = \prod_{j=1}^{n}\,p(x^{i}<em>{j} |\ y)\,p(y)$ ‚ÄÉ‚ÄÉ, where each $p(x^{i}</em>{j} |\ y)$ and $p(y)$ follows Bernoulli distribution</p>
  </li>
  <li>
    <ol>
      <li>MLE estimates</li>
    </ol>

    <p><img width="400" alt="Screen Shot 2023-04-01 at 1 48 44 PM" src="https://user-images.githubusercontent.com/92680829/229266215-df32267c-de7c-4b37-b1b5-16b949c56ae6.png" /></p>
  </li>
  <li>
    <ol>
      <li>Prediction
        <ul>
          <li>find $argmax(y)\,\,p(y |\ x)$</li>
        </ul>
      </li>
    </ol>

    <p><img width="700" alt="Screen Shot 2023-04-01 at 1 52 51 PM" src="https://user-images.githubusercontent.com/92680829/229266313-2b9683f4-d91e-4490-a3fe-45c6fe3e98eb.png" /></p>

    <ul>
      <li>repeat for y = 0, and select the class with max probability</li>
    </ul>
  </li>
</ul>

:ET