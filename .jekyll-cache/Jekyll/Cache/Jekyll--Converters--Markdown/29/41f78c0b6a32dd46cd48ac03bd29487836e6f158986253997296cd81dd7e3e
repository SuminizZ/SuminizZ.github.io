I"O<p><br /></p>

<h2 id="how-to-use-svm-with-kernels"><strong>How to Use SVM with Kernels</strong></h2>
<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/92680829/157623910-d62e41b6-0b4c-4c89-92e3-12b405c27ccb.png" width="600" />
<br /></p>

<ul>
  <li>When to use linear kernel (No kernel)
    <ul>
      <li>when feature number (n) is large, while training set size (m) is small</li>
    </ul>
  </li>
  <li>When to use Gaussian kernel (similarity)
    <ul>
      <li>when n is small, while m is large (ideally)</li>
      <li>note <strong>Do perform feature scailing before using Gaussian kernel function</strong></li>
    </ul>

    <p><img src="https://user-images.githubusercontent.com/92680829/157625901-88bc1081-819d-46c1-8b65-4f63fe5e9b48.png" width="500" /></p>
  </li>
</ul>

<p><br /></p>

<h3 id="restrictions-for-using-kernels--mercers-theorem"><strong>Restrictions for using Kernels : Mercer’s Theorem</strong></h3>
<p><br /></p>

<ul>
  <li>Linear and Gaussian are most common, but not all similarity functions you develop are valid kernels</li>
  <li>Must satisfy Merecer’s Theorem</li>
  <li>Other Types of Kernels (not that common..)
    <ul>
      <li>Polynomial Kernel
        <ul>
          <li>e.g ) (xT l)^2 , (xT l)^3 , (xT l+1)^3 , …</li>
          <li>General form is (xT l+Const)^Dim</li>
          <li>If they’re similar then the <strong>inner product tends to be large</strong></li>
          <li>Not used that often, Usually performs worse than the Gaussian kernel
            <ul>
              <li>Used when x and l are both non-negative</li>
            </ul>
          </li>
          <li>Two parameters
            <ul>
              <li>Degree of polynomial (D)</li>
              <li>Number you add to l (Con)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>String kernel
        <ul>
          <li>Used if input is text strings, Use for text classification</li>
        </ul>
      </li>
      <li>Chi-squared kernel</li>
      <li>Histogram intersection kernel</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="multi-class-classification-for-svm"><strong>Multi-class classification for SVM</strong></h3>
<p><br /></p>

<ul>
  <li>Many packages have built in multi-class classification packages</li>
  <li>Otherwise use one-vs all method</li>
  <li>Not a big issue</li>
</ul>

<p><br /></p>

<h3 id="logistic-regression-vs-svm"><strong>Logistic regression vs. SVM</strong></h3>
<p><br /></p>

<ul>
  <li>
    <p>Logistic regression and SVM with a linear kernel are pretty similar</p>
  </li>
  <li>if n (~10,000) is large (compared to m (10 ~1000)) –&gt; use LR or SVM w/o kernel</li>
  <li>if n is small (1~10000) and m is intermediate( 10-50,000) –&gt; use SVM with Gaussian Kernel</li>
  <li>
    <p>if n is small and m is Large(50,000~) –&gt; <strong>Create more features</strong>, then use LR or SVM w/o kernels</p>
  </li>
  <li>It’s not always clear how to chose an algorithm
    <ul>
      <li>Often more important to get enough data</li>
      <li>Designing new features</li>
      <li>Debugging the algorithm</li>
    </ul>
  </li>
</ul>
:ET