I"4<p><br /></p>

<h2 id="motivations-of-data-compression"><strong>Motivations of Data Compression</strong></h2>
<p><br /></p>

<ul>
  <li>Compression
    <ul>
      <li>Speeds up algorithms learning</li>
      <li>Reduces memory and space used by data for them</li>
      <li>Visualize your data</li>
    </ul>
  </li>
  <li>What is <strong>dimensionality reduction</strong>?
    <ul>
      <li>When you’ve collected unnecessarily many features
        <ul>
          <li>How to “simplify” your data set in a useful way</li>
        </ul>
      </li>
      <li>Example
        <ul>
          <li>Redundant data set - different units for same attribute</li>
          <li>Reduce data to 1D (2D-&gt;1D)</li>
        </ul>
      </li>
      <li>
        <p><img src="https://user-images.githubusercontent.com/92680829/157796065-1fbf4fe7-8baf-45d2-8368-98552de76da8.png" width="250" /></p>

        <ul>
          <li>x(i) : 2D –&gt; z(i) : 1D</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="reducing-dimensionality"><strong>Reducing Dimensionality</strong></h3>
<p><br /></p>

<ul>
  <li>if all the data lie on one plane, then we can reduce the 3D data to 2D</li>
  <li>project all data to a surface,</li>
  <li>specify the location along each axis on newly set plane</li>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/157796675-4a6fce2e-8672-4307-bca6-b76046a91653.png" width="500" />
<br /></p>
  </li>
  <li>x(i) : 3D –&gt; z(i) : 2D</li>
  <li>if we reduce the dimension of training examples {x1, x2,… xm} where x(i) is n vector
    <ul>
      <li>we can get lower dimension of {z1, z2…zm} where z(i) is k vector, and k &lt;= n
<br /></li>
    </ul>
  </li>
</ul>

<h3 id="visualization-of-high-dimensional-data"><strong>Visualization of High-Dimensional Data</strong></h3>
<p><br /></p>

<ul>
  <li>Dimensionality reduction can improve how we display information in a tractable manner for human consumption</li>
  <li>Example :
    <ul>
      <li>collect a large dataset about many facts of countries, let’s say we have 50 features</li>
      <li>it’s very hard to visualize 50 Dimensional features</li>
      <li>but if we reduce the Dimension from 50 to 2 features that can summarize those features, now we can easily plot the dataset by new features</li>
    </ul>
  </li>
  <li>Typically you don’t generally ascribe meaning to the new features (so we have to determine what these summary values mean)
    <ul>
      <li>e.g. may find horizontal axis corresponds to overall country size/economic activity</li>
      <li>and y axis may be the per-person well being/economic activity</li>
    </ul>
  </li>
  <li>
    <p>So despite having 50 features, there may be two “dimensions” of information, with features associated with each of those dimensions</p>
  </li>
  <li><strong>It’s up to you to choose what of the features can be grouped to form summary features</strong>, and how best to do that (<strong>feature scaling</strong> is probably important)</li>
</ul>

<hr />

<p><br /></p>

<h2 id="principal-component-analysis-pca"><strong>Principal Component Analysis (PCA)</strong></h2>
<p><br /></p>

<ul>
  <li>try to find a lower dimensional surface onto which data points can be projected</li>
  <li>try to find the surface that can minimize the squared distance between that surface and original data, which is called <strong>“Projection Error”</strong></li>
  <li>
    <p><strong>PCA tries to find the surface which has the minimum projection error</strong></p>
  </li>
  <li>For 2D-1D PCA, we must find a vector u(1)
    <ul>
      <li>Onto which you can project the data so as to minimize the projection error</li>
      <li>
        <p>u(1) can be positive or negative (-u(1)) which makes no difference</p>
      </li>
      <li><img src="https://user-images.githubusercontent.com/92680829/157801086-09ed948e-b78b-4602-9473-e3988072da17.png" width="300" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>In more general cases, we may want to reduce from nD to kD surface
    <ul>
      <li>Find k vectors (u(1), u(2), … u(k)) onto which to project the data to minimize the projection error</li>
      <li>e.g. 3D-&gt;2D
        <ul>
          <li>Find pair of vectors which define a 2D plane (surface) onto which you’re going to project your data</li>
          <li><img src="https://user-images.githubusercontent.com/92680829/157801877-0490fae6-b36e-4ed3-a4cb-1e7056f7685d.png" width="300" />
<br /></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="pca-and-linear-regression"><strong>PCA and Linear Regression</strong></h3>
<p><br /></p>

<ul>
  <li>you can find the cosmetic similarity between two of them, as both seems to calcuate the line(or surface) to minimize the sum of the squared distance between that line and original data</li>
  <li>However, they have clear difference in two way
    <ol>
      <li>How to calculate the error
        <ul>
          <li>linear regression : it calculates the vertical distance (y difference)</li>
          <li>PCA : it calcuates the orthogonal distance (not y)</li>
          <li>this gives very different effect</li>
        </ul>
      </li>
      <li>objective
        <ul>
          <li>lr : this tries to predict “y” value by drawing the line that can best explain the relationship between x features</li>
          <li>PCA : there is no “y”, it just tries to reduce the dimension, nothing more than that</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<hr />

<p><br /></p>

<h2 id="pca-algorithm"><strong>PCA Algorithm</strong></h2>
<p><br /></p>

<ol>
  <li>
    <p><strong>Data Preprocessing</strong>
<br /></p>

    <ul>
      <li>feature scailing</li>
      <li>mean normalization : replace each xji with xji - μj (avg of feature j)</li>
    </ul>
  </li>
  <li>
    <p>Compute the surface that minimizes the projection error
<br /></p>

    <ul>
      <li>Compute the u vectors : The new planes</li>
      <li>Compute the z vectors : z vectors are the new, lower dimensionality feature vectors</li>
    </ul>
  </li>
</ol>

<ul>
  <li>Then, how can we find the u vectors ?</li>
</ul>

<p><br /></p>

<h3 id="principal-component-pc--eigen-decomposition"><strong>Principal Component (PC) : Eigen Decomposition</strong></h3>

<p><br /></p>

<h4 id="1-compute-covariance-matrix-sigma">1. Compute <strong>Covariance Matrix (Sigma)</strong></h4>
<p><br /></p>

<ul>
  <li><strong>[n x n] Square matrix giving the covariance between each pair of features</strong></li>
  <li>shows how much the variance of x(i) and x(j) resembles each other</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/92680829/157807419-b21a3767-aaab-4575-ad01-c1bc44a54b9e.png" width="340" /></p>

<ul>
  <li>Divide Cov(x1,..xm) / m</li>
  <li>
    <p>Note that x(i) is <strong>“Mean Normalized”</strong> (i)th example of [nx1] vector</p>
  </li>
  <li><img src="https://user-images.githubusercontent.com/92680829/157807701-9c052b37-3c56-452e-97a7-4ca8a1afa1aa.png" width="200" /></li>
</ul>

<p><br /></p>

<h4 id="2-compute-eigenvector-from-covariance-matrix-of-all-features-you-can-use-svd-function-too">2. Compute <strong>EigenVector</strong> from Covariance Matrix of all features (you can use <strong>svd function</strong> too)</h4>
<p><br /></p>

<ul>
  <li>CV matrix have <strong>n eigen vectors</strong> in n-d matrix [nxn] : U matrix</li>
  <li>Select the 1~k(th) longest eigen vectors that have maximal variance (maximal eigen value) to find k-d plane : Ureduce
    <ul>
      <li>principal surface [nxk] : plane that consists of the eigenvectors of a maximal variance from covariance matrix [nxn] 
<br /></li>
    </ul>
  </li>
</ul>

<h4 id="3-next-we-need-to-change-xn-d-to-zk-d--reduce-dimensionality">3. Next we need to change x(n-d) to z(k-d) : Reduce Dimensionality</h4>
<p><br /></p>

<ul>
  <li>[Ureduce]T (kxn) x [x matrix] (nx1) = z matrix (kx1)</li>
  <li>now we get the z matrix (k-dimensional) that are new features we got from PCA</li>
</ul>

<hr />

<p><br /></p>

<h2 id="reconstruction-from-compressed-data"><strong>Reconstruction from Compressed Data</strong></h2>
<p><br /></p>

<ul>
  <li>from the compressed representation, you can go back to the approximation of your high-dimensional original representation</li>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/157820465-17d8d503-ac42-4045-a482-9878512b077f.png" width="250" />
<br /></p>
  </li>
  <li>Considering z (vector) [kx1] = (Ureduce)T [kxn] * x [nx1],
    <ul>
      <li>x(approx) [nx1] = Ureduce [nxk] x z [kx1]</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/157821903-c1010a5a-764f-47c2-b9ec-6818336a0472.png" width="300" /></li>
    </ul>
  </li>
</ul>

<hr />

<p><br /></p>

<h2 id="how-to-use-pca-in-practice"><strong>How to Use PCA in Practice</strong></h2>
<p><br /></p>

<h3 id="choosing-the-number-of-principal-components"><strong>Choosing the number of Principal Components</strong></h3>
<p><br /></p>

<ul>
  <li>1) PCA tries to minimize the projection error : sigma([[X(i) - Xapprox(i)]]^2)
    <ul>
      <li>this can actually be the <strong>loss of original variance through the projection process</strong></li>
    </ul>
  </li>
  <li>
    <p>2) PCA tries to maintain the original x variance as much as they can : sigma([[x(i)]]^2)</p>
  </li>
  <li>Thus, make sure that you choose the k to be smallest value that can follow the below term
    <ul>
      <li>value within the range of 0.01~0.05 can be used</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/157823110-39c12741-12e8-4b3b-8449-344697b24d7b.png" width="400" /></li>
    </ul>
  </li>
  <li>It means that you can retain at least 99% of variance after applying PCA</li>
  <li>How to implement it ?
    <ul>
      <li>Plot, or prepare (k, variance) on validation set, and Select the k that gives the minimum acceptable variance, e.g. 90% or 99%.</li>
      <li><a href="https://towardsdatascience.com/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0">[Link] How to Implenment PCA in Scikit-Learn</a></li>
      <li>
        <p>Seeing the plot below, you can see that k value over about 15 can retain great variability</p>
      </li>
      <li><img src="https://user-images.githubusercontent.com/92680829/157825718-dc4e1e5e-3aab-4659-9c49-8baf33d63bf1.png" width="400" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="supervised-learning-speed-up"><strong>Supervised Learning Speed Up</strong></h3>

<ul>
  <li>when you have dataset with too much features (over 10,000)</li>
  <li>learning with all these features will severly slow down the training</li>
  <li>
    <p>in this case, you can reduce the features by applying PCA</p>
  </li>
  <li>Extract only x inputs from complete dataset (x(i), y(i))</li>
  <li>with earned unlabeled x dataset, you can apply PCA and get new unlabeled data with lower dimensionality z !</li>
  <li>Now you can get new training set (z(i), y(i))</li>
  <li>Make sure Mapping of x(i) to z(i) should be only defined by training examples !!</li>
  <li>and then, you can apply this form of mapping to x(test) and x(cv)</li>
</ul>

<p><br /></p>

<h3 id="bad-use-of-pca"><strong>Bad Use of PCA</strong></h3>

<ul>
  <li>to prevent overfitting
    <ul>
      <li>PCA doesn’t really care about the y value</li>
      <li>Sol, it can ignore the useful information that y value could possibly give</li>
      <li>Use regularization instead!</li>
    </ul>
  </li>
  <li>Applying PCA first to desing ML system
    <ul>
      <li>Always use the original raw data first, and then observe the outcome of it</li>
      <li>Only if that doesn’t work, you can try to include PCA step in your ML system design</li>
    </ul>
  </li>
</ul>

<hr />

<p><br /></p>

<h3 id="-eigen-decomposition-"><strong>– Eigen Decomposition –</strong></h3>

<ul>
  <li>Let’s say <strong>square matrix A as a linear transformation</strong>,
    <ul>
      <li>eigen vector :
        <ul>
          <li>the non-zero vector that <strong>gives itself multiplied by a constant value</strong> from linear transformation A</li>
          <li>Matrix-vector multiplication –&gt; Scalar-vector multiplication</li>
          <li>the directions in which the data varies the most.</li>
        </ul>
      </li>
      <li>eigen value : that <strong>constant value multiplied to eigen vector</strong> after linear transformation</li>
      <li>
        <p>how to compute eigenvector and eigenvalue?</p>

        <ul>
          <li>
            <p><img src="https://user-images.githubusercontent.com/92680829/157813105-0c5b0025-7854-4c60-9f37-ca2d14efa26e.png" width="600" /></p>
          </li>
          <li>
            <p>Explain</p>
            <ul>
              <li>Ax = λx</li>
              <li>we can set λ to λI, which makes no difference in overall equation</li>
              <li>(A-λI)x = 0</li>
              <li>as the eigenvector (x) is non-zero vector, (A-λI) vector cannot have inverse vector</li>
              <li>therefore, det(A-λI) = 0</li>
              <li>when you calculate the equation above, you can get the eigenvector x</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

:ET