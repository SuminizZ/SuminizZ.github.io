I"
<h2 id="outlines"><strong>Outlines</strong></h2>
<p><br /></p>

<ul>
  <li><a href="#references"><strong>References</strong></a></li>
</ul>

<p><br /></p>

<h2 id="references"><strong>References</strong></h2>

<p><br /></p>

<ul>
  <li><a href="https://arxiv.org/pdf/1712.09913.pdf" target="_blank">Visualizing the Loss Landscape of Neural Nets, Hao Li1 (2018)</a></li>
  <li><a href="https://arxiv.org/abs/1412.6544" target="_blank">Qualitatively characterizing neural network optimization problems, Ian J. Goodfellow (2015)</a></li>
</ul>

<p><br /></p>

<h2 id="varying-trainability-of-networks-architectures"><strong>Varying Trainability of Networks Architectures</strong></h2>
<p><br /></p>

<ul>
  <li>
    <p>ResNet successfully address the degradation issue of deeper layers where training performance tends to decay with the depth of neural networks by introducing a novel architecture design named ‚Äúskip-connection‚Äù. Authors of the paper explained the reason behind the poorer trainability of deeper networks than its shallower counterpart is that deeper networks have difficulties in approximating identity mappings and to deal with this, they added some shortcut paths that directly connects the input to output of 2 or more layers that only fits the residual part (gap between the input and desired underlying output). Letting the networks to fit complicated functions only for residuals and simply adding the input to that residual mappings improve the training accuracy of the networks even with very deep structure over than 100 layers. This examples tells the trainability of networks is highly dependent of the architecture design choices. However, the fundamental mechanism of they affects the performance of networks has not been clearly explained.</p>
  </li>
  <li>
    <p>This paper provides a variety of visualizations for the loss landscapes of multiple networks architectures (e.g. VGG, ResNet, WideNet), helping intuitive understanding of how the geometry of neural loss function affects the generalization error and trainabiltiy of the networks.</p>
  </li>
</ul>

<p><br /></p>

<p>‚ÄÉ‚ÄÉ‚ÄÉ<img width="600" alt="image" src="https://github.com/SuminizZ/Physics/assets/92680829/141f483e-cb1e-48c0-b7ff-2ccd2c525870" /></p>

<p><br /></p>

<h2 id="basic-visualizations-of-loss-function"><strong>Basic Visualizations of Loss Function</strong></h2>

<p><br /></p>

<h3 id="1-one-dimensional-linear-interpolation">‚ÄÉ<strong>1. One-Dimensional Linear Interpolation</strong></h3>
<p><br /></p>

<ul>
  <li>strategy taken by Goodfellow in 2015 (https://arxiv.org/abs/1412.6544)</li>
  <li>choose two sets of parameters $\large \theta$ and $\large \theta^{\prime}$, and simply evaluate the loss ($\large J(\theta)$) at a series of points along the line $\large e^{\theta(\alpha)} = (1-\alpha)\theta + \alpha\theta^{\prime}$ for varing $\alpha$, which is a scailing parameter.</li>
</ul>

<p>‚ÄÉ‚ÄÉ‚ÄÉ<img width="470" alt="image" src="https://github.com/SuminizZ/Physics/assets/92680829/c60840ec-620a-4746-8410-be2217df0d1b" /></p>

<ul>
  <li>Even though this approach provides relatively simple and general visualization of how sharp or flattened the loss function is, it is very difficult to express non-covexities using 1D plots.</li>
</ul>

<p><br /></p>

<h3 id="2-contour-plots--random-directions">‚ÄÉ<strong>2. Contour Plots &amp; Random Directions</strong></h3>

<ul>
  <li>Plots the loss as a function of $\large \alpha$ and $\large \beta$, which both are scailing factors for two direction vectors $\large \delta$ and $\large \eta$, respectively.</li>
</ul>

:ET