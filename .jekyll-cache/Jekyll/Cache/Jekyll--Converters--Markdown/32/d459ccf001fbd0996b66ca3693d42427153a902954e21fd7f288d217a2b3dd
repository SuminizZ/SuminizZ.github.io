I"å?<p><br /></p>

<h2 id="binary-classification"><strong>Binary Classification</strong></h2>

<ul>
  <li>Example : Cat Classifier
    <ul>
      <li>with a given image, you can convert the image to three 64 x 64 matrices corresponding to Red, Green, Blue pixel intensity values for your image</li>
      <li>Now unroll all these matrices to a single feature vector X, with the size of [64x64x3 x 1] vector</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/159686660-02d7def4-1739-4e30-bbd3-697f96abb98e.png" width="550" /></li>
    </ul>
  </li>
  <li>Notation
    <ul>
      <li>stacking different data(examples) into different column of X and Y</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/159687336-06bad1e9-b864-4a29-9f38-4d24226e6975.png" width="500" /></li>
    </ul>
  </li>
  <li>X.shape = (nx, m)</li>
  <li>nx : length of x(i), the size of all R, G, B matrices unrolled</li>
  <li>Y : [y1, y2, y3‚Ä¶, ym] (1, m)</li>
</ul>

<p><br /></p>

<h2 id="logistic-regression-as-a-neural-network"><strong>Logistic Regression as a Neural Network</strong></h2>

<ul>
  <li>Binary output : Outputs of Y is always either 1 or 0
    <ul>
      <li>you want</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/159688518-5dc9fc6a-ab5e-4b2b-bd08-b99927d1be23.png" width="350" /></li>
    </ul>
  </li>
  <li><strong>In Linear Regression</strong>
    <ul>
      <li>you can get output by using the equation <strong>y = WTx + b</strong></li>
      <li>W : (nx, 1) vector of weights of each feature / b : real number (intersect)</li>
      <li>BUT, with this linear function, you can‚Äôt get what you want, the chance that y of given example equals to 1 (value ranging from 0 ~ 1)</li>
    </ul>
  </li>
  <li>In Logistic Regression
    <ul>
      <li>Instead, you can use sigmoid function with which you can get the output ranging from 0 ~ 1 depending on the x values</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/159690058-5a5af644-c928-499a-a494-911aeb44741b.png" width="250" /></li>
    </ul>
  </li>
  <li>here, z equals to the previous value obtained from linear regression, WTX + b</li>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/159690511-892b5dc6-e079-44df-b7c7-5139d49ed710.png" width="400" /></p>
  </li>
  <li>when x infinitely increases, g(z) converge to 1, whereas x infinitely decreases, g(z) converge to 0.</li>
  <li>all g(z) values are within between 0 ~ 1</li>
  <li>when z equals to 0, you will get 0.5 as g(z)</li>
  <li>Also, you can alternatively define <strong>b</strong> as <strong>x0</strong> and set w0 as 1, so that you can incorporate b into WTX part
    <ul>
      <li>here‚Äôs the outcome</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/159691164-fe46ef04-02a0-4bad-806b-8214f0bc1da5.png" width="150" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="logistic-regression-cost-function"><strong>Logistic Regression Cost Function</strong></h3>
<ul>
  <li>Training set of m training examples, Each example has is n+1 length column vector</li>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/156683168-6dfb6801-f65a-4a2c-815a-4d37f69839a8.png" width="500" /></p>
  </li>
  <li>Given the training set how to we chose/fit Œ∏?
    <ul>
      <li>Cost function of linear regression was like below,</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156683283-033fa772-f636-4bdf-87f3-643d477483a1.png" width="300" /></li>
    </ul>
  </li>
  <li>Instead of writing the squared error term, we can write</li>
  <li><strong>cost(hŒ∏(xi), y) = 1/2(hŒ∏(xi) - yi)2</strong></li>
  <li>Which evaluates the cost for an individual example using the same measure as used in linear regression
    <ul>
      <li>We can redefine J(Œ∏) as</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156683371-e8fb4778-a11f-4199-a99e-3ca4de1588fa.png" width="300" /></li>
    </ul>
  </li>
</ul>

<p>Which, appropriately, is the sum of all the individual costs over the training data (i.e. the same as linear regression)</p>

<ul>
  <li>This is the cost you want the learning algorithm to pay if the outcome is hŒ∏(x) and the actual outcome is y</li>
  <li>Issue : If we use this function for logistic regression, this is a <strong>Non-convex function</strong> for parameter optimization
    <ul>
      <li>non-convex function : wavy - has some ‚Äòvalleys‚Äô (local minima) that aren‚Äôt as deep as the overall deepest ‚Äòvalley‚Äô (global minimum).</li>
      <li>Optimization algorithms can get stuck in the local minimum, and it can be hard to tell when this happens.</li>
    </ul>
  </li>
  <li><strong>A convex logistic regression cost function</strong>
    <ul>
      <li>To get around this we need a different, convex Cost() function which means we can apply gradient descent</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156684011-edda5943-64ce-43b9-924b-7a7fd1ce0ddc.png" width="400" /></li>
    </ul>
  </li>
  <li>This is our logistic regression cost function
    <ul>
      <li>This is the penalty the algorithm pays</li>
      <li>Plot the function</li>
    </ul>

    <ol>
      <li>Plot y = 1
        <ul>
          <li>So hŒ∏(x) evaluates as -log(hŒ∏(x))</li>
          <li><img src="https://user-images.githubusercontent.com/92680829/156685913-f0e750ef-56db-4deb-9a0d-f2cbcab3e3f5.png" width="220" /></li>
        </ul>
      </li>
      <li>plot y=0
        <ul>
          <li>So hŒ∏(x) evaluates as -log(1-hŒ∏(x))</li>
          <li><img src="https://user-images.githubusercontent.com/92680829/156686219-e35d4c6c-2001-480a-9acd-cd927f906fb3.png" width="220" /></li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<p><br /></p>

<h3 id="combined-cost-function-of-rl"><strong>Combined Cost Function of RL</strong></h3>
<ul>
  <li>Instead of separating cost function into two parts differing by the value of y (0 or 1),</li>
  <li>
    <p>we can compress it into one cost function, which makes it more convenient to write out the cost.</p>

    <ul>
      <li><strong>cost(hŒ∏, (x),y) = -ylog( hŒ∏(x) ) - (1-y)log( 1- hŒ∏(x) )</strong></li>
      <li>y can only be either 0 or 1</li>
      <li>when y = 0, only -log( 1- hŒ∏(x) ) part remains, which is exactly the same as the original one</li>
      <li>when y =1, only -log( hŒ∏(x) ) part remains</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156687790-4532412e-706c-435c-b5aa-7d4a5f9145c3.png" width="600" /></li>
    </ul>
  </li>
  <li>now! you can finally get convex cost function that has global optima</li>
</ul>

<p><br /></p>

<h3 id="optimizing-cost-function-w-gradient-descent"><strong>Optimizing Cost Function w/ Gradient Descent</strong></h3>
<ul>
  <li>Interestingly, derivative of J(Œ∏) of logistic regression is exactly identical with that of linear regression (proof of this statement will be covered later)</li>
  <li>Firstly, you would set all the features(w1~wm) as 0, including w0 (intersect, b)</li>
  <li>and then, Repeat
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156696635-ab555f91-5544-40e9-9855-fe92787b3901.png" width="350" /></li>
    </ul>
  </li>
  <li>Representation of the process of finding global optima
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/160386110-1938217c-1d25-4455-a643-f49c84816f51.png" width="500" /></li>
    </ul>
  </li>
  <li>BUT! this optimizing algorithm has serious weakness, which is explicit double for-loop</li>
  <li>first for-loop is for iterations of algorithm untill you reach to global optima</li>
  <li>secondly, you need to have a for loop over all the features</li>
  <li>this explicit for-loop can severly slower the training rate with the large dataset</li>
  <li>So, instaead of this, you need to learn <strong>‚ÄúVectorization‚Äù</strong> with which you can get rid of these explicit for-loop</li>
</ul>

<p><br /></p>

<h4 id="-proof--getting-derivative-of-lr-cost-function-"><strong>‚Äì Proof : Getting Derivative of LR Cost Function ‚Äì</strong></h4>

<ul>
  <li>Remember hŒ∏(x) is
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/160383571-e315c407-ec0f-4e69-95cb-b30d669d4435.png" width="200" /></li>
    </ul>
  </li>
  <li>Step1 : take partial derivative of h(Œ∏) = 1/(1 + e-z)
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156696402-799da3b1-8d66-4ab4-b7e6-8e92c27d46f3.png" width="400" /></li>
    </ul>
  </li>
  <li>Step2 : take partial derivative to J(Œ∏)
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156696529-a943aceb-f987-4324-9a57-b2ad41e3a35f.png" width="400" /></li>
      <li><img src="https://user-images.githubusercontent.com/92680829/160385000-1694ccca-9f27-413d-a8fc-bd15794a7237.png" width="400" /></li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156696592-9857ffd5-6637-46ed-abef-2f47d21b64c0.png" width="500" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="computation-graph"><strong>Computation Graph</strong></h3>

<ul>
  <li>Previously, I figured out the partial derivative of J (dJ/dŒ∏), by using <strong>Chain Rule</strong>
    <ul>
      <li>Chain Rule : backward propagation of taking derivative partially with respect to from final output variable (here, v) to starting variable (here, a)</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/160389908-ce557dd3-7a18-40ed-a52d-6816c0addfc2.png" width="500" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="vectorization-with-python"><strong>Vectorization with Python</strong></h2>
<ul>
  <li>vectoriztion can save you a great amount of time by removing explicit for loop from your algorithm!
    <ul>
      <li>
        <p><img src="https://user-images.githubusercontent.com/92680829/160834341-2ff411f3-9cc5-4b3a-98c3-c2d8a2849fa2.png" width="500" /></p>
      </li>
      <li>
        <p>let‚Äôs see if it‚Äôs true with python code</p>
      </li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>

<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">vec</span><span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>    <span class="c1"># calculate inner product of a, b vector (1D)
</span><span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Vectorized Version : {0}ms"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span><span class="o">-</span><span class="n">tic</span><span class="p">)))</span>

<span class="n">tick</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">skr</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000</span><span class="p">):</span>
    <span class="n">skr</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">tock</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    
<span class="k">print</span><span class="p">(</span><span class="n">skr</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Scalar Version : {0}ms"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">tock</span><span class="o">-</span><span class="n">tick</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>249812.28927442286
Vectorized Version : 2.006053924560547ms
249812.28927442944
Scalar Version : 1888.9873027801514ms
</code></pre></div></div>

<ul>
  <li>the results of both algorithm are same</li>
  <li>
    <p>BUT, it takes about 1000 times longer time to calculate the inner product of 1d vector a &amp; b</p>
  </li>
  <li>There are some numpy functions that allow you to apply exponential or log operation on every element of a matrix/vector</li>
  <li>np.log(V), np.exp(V)</li>
</ul>

<p><br /></p>

<h3 id="logistic-regression-with-vectorization"><strong>Logistic Regression with Vectorization</strong></h3>
<ul>
  <li>logistic regression with For-Loops
    <ul>
      <li>suppose we have ‚Äòn‚Äô features</li>
      <li>there are ‚Äòm‚Äô samples</li>
      <li>without vectorization, you have to use 2 for-loops, one for i (from 1 to n) and another for j (from 1 to m)
  <img src="https://user-images.githubusercontent.com/92680829/161545636-50b6f7b2-a508-4faf-a177-ab4a0a3732c5.png" width="460" /></li>
    </ul>
  </li>
  <li><strong>Vectorizing Logistic Regression</strong>
    <ul>
      <li>with vectorized LR, all you need to calculate the Gradient Descent of Cost function for each iteration is just two liens of code</li>
      <li>db = 1/m(np.sum(dZ)</li>
      <li>dw = 1/m(XdZT)</li>
      <li>
        <p><img src="https://user-images.githubusercontent.com/92680829/161550157-e994fe73-0a1f-49af-a24d-f0adc4f94752.png" width="270" /></p>
      </li>
      <li>you don‚Äôt need <strong>‚ÄúANY‚Äù</strong> foor-loops</li>
      <li>but even with vectorized LR, you still need to use for-loop for iterations to gd minimizing the cost 
  <img src="https://user-images.githubusercontent.com/92680829/161549261-0fba8b93-f524-4b90-8355-756ed790e51c.png" width="220" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="broadcasting-in-python"><strong>Broadcasting in Python</strong></h3>
<ul>
  <li>It refers to how <strong>numpy</strong> treats arrays with different Dimension during arithmetic operations(+, -, *, /) which lead to certain constraints</li>
  <li>the smaller array is broadcasted across the larger array so that they have compatible shapes</li>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/161551568-dba2c92a-53bb-4ca6-9bf1-d04dc6cc50bd.png" width="410" /></p>
  </li>
  <li>Even though broadcasting of python-numpy provides lots of benefits such as convenience and flexibility, but it can also cause a few bugs when mistreated</li>
  <li>For effective usage of only the strengths of broadcasting of python-numpy, except the weaknesses</li>
  <li>recommend not to use ‚ÄúRank 1 Array‚Äù like np.random.randn(5), which has pretty non-intuitive shapes</li>
  <li>Instead, you can use vector like np.random.randn(5,1)</li>
</ul>
:ET