I"<p><br /></p>

<h2 id="debugging-a-learning-algorithm"><strong>Debugging a learning algorithm</strong></h2>
<p><br /></p>

<ul>
  <li>There are many things you can do;
    <ol>
      <li>Get more training data â€“&gt; fix high variance (compensate overfitting)
        <ul>
          <li>you should always do some preliminary testing to make sure more data will actually make a difference (discussed later)</li>
        </ul>
      </li>
      <li>Try a smaller set a features â€“&gt; fix high variance
 You can do this by hand, or use some dimensionality reduction technique (e.g. PCA)</li>
      <li>Try getting additional features â€“&gt; fix high bias</li>
      <li>Adding polynomial features â€“&gt; fix high bias
        <ul>
          <li>Can be risky if you accidentally over fit your data by creating new features which are inherently specific/relevant to your training data</li>
        </ul>
      </li>
      <li>Try decreasing or increasing Î» â€“&gt; fix either high bias or variance
        <ul>
          <li>apporpriate regularization</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>There are some simple techniques which can let you rule out half the things on the list, which can help you save a lot of time!</li>
</ul>

<p><br /></p>

<h2 id="machine-learning-diagnostic"><strong>Machine Learning Diagnostic</strong></h2>
<p><br /></p>

<ul>
  <li>a test to gain insight what is/isnâ€™t working with your learning algorithm, and get guidance how best to improve its performance.</li>
</ul>

<hr />
<p><br /></p>

<h2 id="how-to-evaluate-hypothesis-function"><strong>How to Evaluate Hypothesis Function</strong></h2>
<p><br /></p>

<ul>
  <li>Evaluate the Generalization Power of your Hypothesis
    <ul>
      <li>model with low training error and high test error can be a sign of overfitting</li>
      <li>training error is not a good estimate for actual generalization error of your model</li>
      <li>However, picking the model with smallest test error will end up in model that is overfitted to only test set, not the new examples that model has never seen before</li>
      <li>Try, split the  dataset into three parts, <strong>1. training set (60), 2. cross-validation set (cv) (20), 3. test set (20)</strong></li>
      <li>We can now calculate three <strong>separate error values for the three different sets using the following method</strong>:
        <ol>
          <li>Optimize the parameters in Î˜ using the training set for each polynomial degree.</li>
          <li>select the model with the least error using the cross validation set.</li>
          <li>Estimate the generalization error using the test set with 
  This way, the degree of the polynomial d has not been trained using the test set.</li>
        </ol>
      </li>
      <li>I think I can deal with it using <strong>GridSearch</strong></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="diagnosing-bias-underfitting-vs-variance-overfitting"><strong>Diagnosing Bias (underfitting) vs Variance (overfitting)</strong></h2>
<p><br /></p>

<ul>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/157142532-07133edf-8ca0-4b39-9b6b-6efff5dec38c.png" width="450" /></p>
  </li>
  <li>
    <p>Draw Error Plot using Training set and CV set</p>
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/157142795-d1b88587-a280-4997-99e5-7aa2a2117a1d.png" width="500" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>if d is too small â€“&gt; this probably corresponds to a high bias problem</li>
  <li>if d is too large â€“&gt; this probably corresponds to a high variance problem</li>
  <li><strong>For the high bias case (under-fitting)</strong>
    <ul>
      <li>we find both cross validation and training error are high</li>
    </ul>
  </li>
  <li><strong>For high variance (over-fitting)</strong>
    <ul>
      <li>we find the cross validation error is high but training error is low</li>
      <li>training set fits well, But generalizes poorly</li>
    </ul>
  </li>
  <li>the point near where the Jcv value reach the minimum can be optimal â€˜dâ€™</li>
</ul>

<p><br /></p>

<h2 id="regularization-and-biasvariance"><strong>Regularization and Bias/Variance</strong></h2>
<p><br /></p>

<ul>
  <li>How we can automatically choose good regularization parameter Î»
    <ul>
      <li>too large Î» â€“&gt; too simple model (high bias)</li>
      <li>too small Î» â€“&gt; too fitted model (high variance)</li>
      <li>Plot Jcv (w/o Î») and J train,  against each Î» value</li>
    </ul>

    <p><img src="https://user-images.githubusercontent.com/92680829/157145429-e5cd04b7-cf93-4e9d-89a8-135c5b769671.png" width="600" /></p>
  </li>
</ul>

<p><br /></p>

<hr />

<h2 id="learning-curves"><strong>Learning Curves</strong></h2>
<p><br /></p>

<ul>
  <li>The more training data you have,
    <ul>
      <li>the harder you could fit your hypothesis to all training data â€“&gt; J train will increase</li>
      <li>the better you can generalize your hypothesis to more new examples â€“&gt; J cv will decrase</li>
    </ul>
  </li>
  <li>
    <p>So basically, leaning curves against training set size looks like below,</p>

    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/157146471-1474ffea-f65e-4435-ae2b-a89e35dae3f2.png" wdith="350" /></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="1-in-case-of-high-bias">1. In case of <strong>High Bias</strong></h3>
<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/92680829/157146854-76428e78-74fa-4a3d-8ae8-53b36af8ee9b.png" width="500" /></p>

<p><br /></p>

<ul>
  <li>With the strong bias, as the hypothesis is too simple, so large training set doesnâ€™t really help to improve hypothesis</li>
  <li>Eventually, J train and J cv both will converge to the similarly high error</li>
</ul>

<p><br /></p>

<h3 id="2-in-case-of-high-variance-small-Î»">2. In case of <strong>High Variance</strong> (small Î»)</h3>
<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/92680829/157147772-677071d8-c7f8-435a-af28-ad57d8249ff5.png" width="500" />
 <br /></p>

<ul>
  <li>When the training size is relatively small, the error gap between J train and J cv is large due to the high variance</li>
  <li>However, even with the high variance, as training size gets larger, hypothesis can cover more and more new examples   so it will have better generalization power â€“&gt; J cv will keep decrasing</li>
  <li>But still, large training size will make it more harder for the hypothesis to fit all the data â€“&gt; J train will be saturated.</li>
  <li>Gap is getting smaller and smaller as training size gets bigger</li>
</ul>

<hr />
<p><br /></p>

<h2 id="neural-networks-and-overfitting"><strong>Neural Networks and Overfitting</strong></h2>
<p><br /></p>

<ul>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/157149592-7ab87882-b045-4599-acbf-ffd110dc506d.png" width="500" /></p>
  </li>
  <li>when the model is suffering from high variance ( J cvÂ Â» J train)</li>
  <li>â€“&gt; it wonâ€™t gonna help to add more hidden units or layers as it will end up increasing variance even more</li>
</ul>

:ET