I" )<h2 id="logistic-regression"><strong>Logistic Regression</strong></h2>
<ul>
  <li>Classification Algorithmn</li>
  <li>Where y is a discrete value : Develop the logistic regression algorithm to determine what class a new input should fall into</li>
  <li>How do we develop a classification algorithm?
  Tumour size vs malignancy (0 or 1)
    <ul>
      <li>We could use linear regression</li>
      <li>Then threshold the classifier output (i.e. anything over some value is yes, else no)</li>
      <li>In our example below linear regression with thresholding seems to work</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156676623-16341baa-323a-4736-9d40-921fa77560c7.png" width="500" /></li>
    </ul>
  </li>
</ul>

<h3 id="function-that-determines-discrete-classification"><strong>Function that determines discrete classification</strong></h3>
<ul>
  <li>We want our classifier to output values between 0 and 1</li>
  <li>When using linear regression we did hŒ∏(x) = (Œ∏T x) = z</li>
  <li>For classification hypothesis representation we do hŒ∏(x) = g((Œ∏T x)), where g(z) with z as a real number and g(z) is the final classified outcome, which is either 1 or 0.</li>
  <li>g(z) = 1/(1 + e-z)</li>
  <li>g(z) is the sigmoid function, or the logistic function</li>
  <li>when x infinitely increases, g(z) converge to 1, whereas x infinitely decreases, g(z) converge to 0.</li>
  <li>all g(z) values are within between 0 ~ 1</li>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/156677845-3711a946-4037-4051-a5d3-0fd47545acd7.png" width="400" /></p>
  </li>
  <li>Since this is a binary classification task we know y = 0 or 1</li>
  <li>P(y=1|x ; Œ∏) ‚Ä¶ probability of y equal to 1, given x parameterized by Œ∏</li>
  <li>So the following must be true
    <ul>
      <li>P(y=1|x ; Œ∏) + P(y=0|x ; Œ∏) = 1</li>
      <li>P(y=0|x ; Œ∏) = 1 - P(y=1| ; Œ∏)</li>
    </ul>
  </li>
</ul>

<h3 id="decision-boundary"><strong>Decision Boundary</strong></h3>
<ul>
  <li>x criteria that determines whether y is either 0 or 1</li>
  <li>suppose that y =  1 when  hŒ∏(x) &gt;= 0.5, which means that z, Œ∏T x &gt;= 0 and y = 0 when hŒ∏(x) &lt; 0.5, Œ∏T x &lt; 0</li>
  <li>for example, hŒ∏(x) = g(Œ∏0 + Œ∏1x1 + Œ∏2x2), where Œ∏0 = -3, Œ∏1 = 1, Œ∏2 = 1</li>
  <li>So, Œ∏T is a row vector = [-3,1,1]</li>
  <li>The z here becomes Œ∏T x</li>
  <li>We predict ‚Äúy = 1‚Äù if -3x0 + 1x1 + 1x2 &gt;= 0, so as x0 equals to 1, -3 + x1 + x2 &gt;= 0</li>
  <li>If (x1 + x2 &gt;= 3) then we predict y = 1</li>
  <li>Let‚Äôs plot how z differs by each combination of x1, x2</li>
  <li>the line x1 + x2 = 3, that separates g(z), y is the decision boundary</li>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/156680691-a5498da1-0e86-4281-9135-5735a5f25b10.png" width="300" /></p>
  </li>
  <li>Non-Linear Decision Boundary
    <ul>
      <li>hŒ∏(x) = g(Œ∏0 + Œ∏1x1+ Œ∏3x12 + Œ∏4x22)</li>
      <li>Say Œ∏T was [-1,0,0,1,1] then we say;</li>
      <li>Predict that ‚Äúy = 1‚Äù if -1 + x12 + x22 &gt;= 0 or x1^2 + x2^2 &gt;= 1</li>
      <li>If we plot this, This gives us a circle with a radius of 1 around 0</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156681451-336e69b6-1fff-423f-8ad8-5bb580e91bb3.png" width="300" /></li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="cost-function-of-logistic-regression"><strong>Cost Function of Logistic Regression</strong></h2>
<p>Training set of m training examples, Each example has is n+1 length column vector
x0 = 1
y ‚àà {0,1}
Hypothesis is based on parameters (Œ∏)</p>

<p><img src="https://user-images.githubusercontent.com/92680829/156683168-6dfb6801-f65a-4a2c-815a-4d37f69839a8.png" width="500" /></p>

<ul>
  <li>Given the training set how to we chose/fit Œ∏?
    <ul>
      <li>Cost function of linear regression was like below,</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156683283-033fa772-f636-4bdf-87f3-643d477483a1.png" width="300" /></li>
    </ul>
  </li>
  <li>Instead of writing the squared error term, we can write</li>
  <li><strong>cost(hŒ∏(xi), y) = 1/2(hŒ∏(xi) - yi)2</strong></li>
  <li>Which evaluates to the cost for an individual example using the same measure as used in linear regression
    <ul>
      <li>We can redefine J(Œ∏) as</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156683371-e8fb4778-a11f-4199-a99e-3ca4de1588fa.png" width="300" /></li>
    </ul>
  </li>
</ul>

<p>Which, appropriately, is the sum of all the individual costs over the training data (i.e. the same as linear regression)</p>

<ul>
  <li>This is the cost you want the learning algorithm to pay if the outcome is hŒ∏(x) and the actual outcome is y</li>
  <li>If we use this function for logistic regression, this is a non-convex function for parameter optimization
    <ul>
      <li>non-convex function : wavy - has some ‚Äòvalleys‚Äô (local minima) that aren‚Äôt as deep as the overall deepest ‚Äòvalley‚Äô (global minimum).</li>
      <li>Optimization algorithms can get stuck in the local minimum, and it can be hard to tell when this happens.</li>
    </ul>
  </li>
  <li><strong>A convex logistic regression cost function</strong>
    <ul>
      <li>To get around this we need a different, convex Cost() function which means we can apply gradient descent</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156684011-edda5943-64ce-43b9-924b-7a7fd1ce0ddc.png" width="500" /></li>
    </ul>
  </li>
  <li>This is our logistic regression cost function
    <ul>
      <li>This is the penalty the algorithm pays</li>
      <li>Plot the function</li>
    </ul>
  </li>
  <li>
    <ol>
      <li>Plot y = 1
        <ul>
          <li>So hŒ∏(x) evaluates as -log(hŒ∏(x))</li>
          <li><img src="https://user-images.githubusercontent.com/92680829/156685913-f0e750ef-56db-4deb-9a0d-f2cbcab3e3f5.png" width="270" /></li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<ol>
  <li>plot y=0
    <ul>
      <li>So hŒ∏(x) evaluates as -log(1-hŒ∏(x))
 -</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156686219-e35d4c6c-2001-480a-9acd-cd927f906fb3.png" width="250" /></li>
    </ul>
  </li>
</ol>

<hr />

<h3 id="simplified-cost-function-and-gradient-descent"><strong>Simplified Cost Function and Gradient Descent</strong></h3>

<ul>
  <li>Instead of separating cost function into two parts differing by the value of y (0 or 1),</li>
  <li>
    <p>we can compress it into one cost function, which makes it more convenient to write out the cost.</p>

    <ul>
      <li><strong>cost(hŒ∏, (x),y) = -ylog( hŒ∏(x) ) - (1-y)log( 1- hŒ∏(x) )</strong></li>
      <li>y can only be either 0 or 1</li>
      <li>when y = 0, only -log( 1- hŒ∏(x) ) part remains, which is exactly the same as the original one</li>
      <li>when y =1, only -log( hŒ∏(x) ) part remains
  -</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156687790-4532412e-706c-435c-b5aa-7d4a5f9145c3.png" width="600" /></li>
    </ul>
  </li>
  <li>Repeat Below to improve cost
    <ul>
      <li>Interestingly, derivative of J(Œ∏) of logistic regression is exactly identical with that of linear regression</li>
      <li></li>
      <li><img src="https://user-images.githubusercontent.com/92680829/156696635-ab555f91-5544-40e9-9855-fe92787b3901.png" width="400" /></li>
    </ul>
  </li>
</ul>

<h3 id="derivative-of-cost-function-of-logistic-regression"><strong>Derivative of Cost Function of Logistic Regression</strong></h3>
<ul>
  <li>Step1 : get derivative of h(Œ∏) = 1/(1 + e-z)
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156696402-799da3b1-8d66-4ab4-b7e6-8e92c27d46f3.png" width="400" /></li>
    </ul>
  </li>
  <li>
    <p>Step2 : apply derivative to J(Œ∏)</p>

    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156696529-a943aceb-f987-4324-9a57-b2ad41e3a35f.png" width="400" /></li>
    </ul>
  </li>
  <li>
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156696553-9fbfbcfe-76a5-4e34-9fac-27300e9c4994.png" width="400" /></li>
    </ul>
  </li>
  <li>
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/156696592-9857ffd5-6637-46ed-abef-2f47d21b64c0.png" width="500" /></li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="advanced-optimization"><strong>Advanced Optimization</strong></h2>
<p>Alternatively, instead of <strong>gradient descent</strong> to minimize the cost function we could use</p>
<ul>
  <li>Conjugate gradient</li>
  <li>BFGS (Broyden-Fletcher-Goldfarb-Shanno)</li>
  <li>L-BFGS (Limited memory - BFGS)</li>
  <li>Advantages
    <ul>
      <li>No need to manually pick alpha (learning rate)</li>
      <li>Have a clever inner loop (line search algorithm) which tries a bunch of alpha values and picks a good one</li>
      <li>Often faster than gradient descent</li>
      <li>Do more than just pick a good learning rate</li>
      <li>Can be used successfully without understanding their complexity</li>
    </ul>
  </li>
  <li>Disadvantages
    <ul>
      <li>Could make debugging more difficult</li>
      <li>Should not be implemented themselves</li>
      <li>Different libraries may use different implementations - may hit performance</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="multiclass-classification"><strong>Multiclass Classification</strong></h2>

<ul>
  <li><strong>One vs. all classification</strong>
    <ul>
      <li>Split the training set into three separate binary classification problems</li>
      <li>i.e. create a new fake training set
        <ul>
          <li>Triangle (1) vs crosses and squares (0) hŒ∏1(x)
            <ul>
              <li>P(y=1 | x1; Œ∏)</li>
            </ul>
          </li>
          <li>Crosses (1) vs triangle and square (0) hŒ∏2(x)
            <ul>
              <li>P(y=1 | x2; Œ∏)</li>
            </ul>
          </li>
          <li>Square (1) vs crosses and square (0) hŒ∏3(x)
            <ul>
              <li>P(y=1 | x3; Œ∏)</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Overall
    <ul>
      <li>Train a logistic regression classifier hŒ∏(i)(x) for each class i to predict the probability that y = i</li>
      <li>On a new input, to make a prediction,
        <ul>
          <li><strong>run all three classifiers on the input, and then pick the class i that maximizes the probability that hŒ∏(i)(x) = 1</strong></li>
        </ul>
      </li>
    </ul>

    <p><img src="https://user-images.githubusercontent.com/92680829/156708103-9d51b0cf-c811-4060-83b4-f3e383e04b96.png" width="250" /></p>
  </li>
</ul>
:ET