I"ó4<p><br /></p>

<h2 id="learning-with-large-datasets"><strong>Learning with large datasets</strong></h2>
<ul>
  <li>One of best ways to get high performance is take a low bias algorithm and train it on a lot of data</li>
  <li>We saw that so long as you feed an algorithm lots of data, they all perform pretty similarly</li>
  <li>However, simply increasing the size of data cannot always gaurantee good performance of model
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158741191-89aa8267-e785-44a5-96cc-0cbcfe3c36f6.png" width="300" />
<br /></li>
    </ul>
  </li>
  <li>Suppose you have a huge dataset with m greater than 1 bilion,</li>
  <li>then if you are using linear regression, it means that you have to calculate (update) more than 1 bilion times per each learning step</li>
  <li>this will cause sever compuational cost</li>
  <li>then, how can you deal with this situation
    <ul>
      <li>
        <ol>
          <li>you have to check if we can train on 1000 examples instead of 100 000 000
            <ul>
              <li>Randomly pick a small selection</li>
            </ul>
          </li>
        </ol>
      </li>
      <li>Then, plot the <strong>Learniing Curves of Jcv and J train</strong> according to the training size and figure out the issue that you‚Äôre dealing with
        <ul>
          <li><strong>High Variance</strong> : seems like add more training examples will help
            <ul>
              <li><img src="https://user-images.githubusercontent.com/92680829/158741783-290abb11-5614-46eb-8fcb-8c3965a6e227.png" width="200" /></li>
            </ul>
          </li>
          <li><strong>High Bias</strong> : increasing the size of training examples will not gonna help to improve your model
            <ul>
              <li><img src="https://user-images.githubusercontent.com/92680829/158741846-ef5cfc23-89ee-4e8d-8613-b406890580c1.png" width="200" /></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><br /></p>

<ul>
  <li>Therefore, the most reliable ways to get a high performance machine learning system is to take a <strong>low bias algorithm and train on a massive data set</strong></li>
  <li>How to deal with large scale dataset
    <ul>
      <li>
        <ol>
          <li>Stochastic Gradient Descent : efficiency</li>
        </ol>
      </li>
      <li>
        <ol>
          <li>Map Reduce</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<hr />
<p><br /></p>

<h2 id="stochastic-gradient-descent"><strong>Stochastic Gradient Descent</strong></h2>
<p><br /></p>

<ul>
  <li>When you have very large dataset, gradient descent becomes computationally very expensive</li>
  <li>to solve this issue, modifications to gradient descent is applied, which is called ‚ÄúStochastic Gradient Descent‚Äù</li>
  <li>Previous version of Training Linear Regression with <strong>Gradient Descent</strong>
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158743275-2088e588-77f4-469d-948c-c5eec4480188.png" width="500" />
<br /></li>
    </ul>
  </li>
  <li>we have to repeat this updating process for <strong>entire m</strong> per one iteration</li>
  <li>this kind of gradient descent is called <strong>Batch gradient descent</strong>
    <ul>
      <li>not a effective choice for large dataset to train : LONG time to converge</li>
    </ul>
  </li>
  <li>
    <p>Instead of this, we will use different algorithm that doesn‚Äôt require to see all of the training examples for every update</p>

    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158744213-5e4c6eca-a3aa-427c-9cf3-18ff1e451b22.png" width="320" />
<br /></li>
    </ul>
  </li>
  <li>So the function represents the SGD calculates <strong>cost of Œ∏j with respect to a specific single example (xi, yi)</strong></li>
  <li>measure how well is my hypothesis doing on a single example</li>
</ul>

<p><br /></p>

<h3 id="algorithm"><strong>Algorithm</strong></h3>

<ol>
  <li>Randomly shuffle dataset
    <ul>
      <li>means we ensure the data is in a random order so we don‚Äôt bias the movement
        <ul>
          <li>speed up convergence a little bit</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Loop (about 1~10 times)
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158745937-b9bdcfcc-6b2e-4816-b75d-a1b90d6aa988.png" width="250" /></li>
    </ul>
  </li>
</ol>

<ul>
  <li>Can you see the difference here?</li>
  <li>gradient descent requires to compute the derivatives of all training example to update Œ∏ once!
    <ul>
      <li>On the other hand, SGD only takes a single training example x(i) for updating Œ∏j onece</li>
      <li>and repeat updating for 1 ~ m examples</li>
      <li>Means we update the parameters on EVERY step through single data, instead of updating at the end of each loop through all the data</li>
    </ul>
  </li>
  <li>Pattern of Convergence is different
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158748465-5e302586-7b60-4960-b7a4-bd43a121bbce.png" width="330" /></li>
    </ul>
  </li>
  <li>red line shows the updating track of parameters from batch gradient descent (<strong>1 Update per Batch</strong>)</li>
  <li>pink line is from SGD (<strong>1 Update per 1 data</strong>)
    <ul>
      <li>seems every update is slightly titled to every single data point, but converge to the optima at a much faster rate
        <ul>
          <li>Not necessarily decrease Jtrain for every update (even with well-tuned learning rate)</li>
        </ul>
      </li>
      <li>variant but faster</li>
      <li>May need to loop over the entire dataset 1-10 times
        <ul>
          <li>If you have a truly massive dataset it‚Äôs possible that by the time you‚Äôve taken a single pass through the dataset you may already have a perfectly good hypothesis</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Due to its variance, SGD never actually converges like batch gradient descent does, but ends up wandering around some region close to the global minimum
    <ul>
      <li>In practice, this isn‚Äôt a problem - as long as you‚Äôre close enough to the global minimum
<br /></li>
    </ul>
  </li>
</ul>

<h3 id="stochastic-gradient-descent-convergence"><strong>Stochastic gradient descent convergence</strong></h3>
<ul>
  <li>how can you be certain that your sgd has convergd to global minimum (at least close)</li>
  <li>
    <p>how do you tune your learning rate Œ±?</p>
  </li>
  <li><strong>Checking Convergence : Plot cost(Œ∏, (xi, yi)), averaged over N examples</strong>
    <ul>
      <li>
        <ol>
          <li>decreasing learning rate (upper left)
            <ul>
              <li>slower the convergence</li>
              <li>but obtain slightly better cost (negligible sometimes)</li>
            </ul>
          </li>
        </ol>
      </li>
      <li>
        <ol>
          <li>increasing N (&gt;= 5000) (upper right)
            <ul>
              <li>also takes more time to plot (longer time to get single plotting point)</li>
              <li>can smoothen the cost line</li>
            </ul>
          </li>
        </ol>
      </li>
      <li>
        <ol>
          <li>small N (lower right)
            <ul>
              <li>line will fluctuate too much, preventing you from seeing actual trend</li>
              <li>if you elevate N, then you can see what‚Äôs actually going on</li>
            </ul>
          </li>
        </ol>
      </li>
      <li>
        <ol>
          <li>increasing cost : diverging (lower right)
            <ul>
              <li>it shows that your algorithm fails to converge to minimum, (fails to find optimal parameters)</li>
              <li>you should adjust your learning rate smaller, so that it can converge</li>
            </ul>
          </li>
        </ol>
      </li>
      <li><img src="https://user-images.githubusercontent.com/92680829/158756682-e98ca189-c71e-48a8-929a-9718bbb3967b.png" width="550" /></li>
    </ul>
  </li>
  <li><strong>Learning rate (Œ±)</strong>
    <ul>
      <li>! typically, Œ± helds constant through entire learning process</li>
      <li>but, you can also slowly decrease Œ± over time (if you want the model to converge better)
        <ul>
          <li><strong>Œ± = const1/(iterationNumber + const2)</strong></li>
          <li>but you have take additional time to decide what const1 and const2 are</li>
          <li>this means you‚Äôre guaranteed to converge somewhere rathter than oscillating around it</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>SGD can be a good algorithm for online learning where there‚Äôs a great influx of data per second (massive training examples)
    <ul>
      <li>SGD will boost up your journey to find best parameters that will help your business decision</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="mini-batch-gradient-descent"><strong>Mini-Batch Gradient Descent</strong></h2>
<ul>
  <li>Compromise between Batch gradient descent &amp; Stochastic gradient descent
    <ul>
      <li>Batch gradient descent: Use all m examples in each iteration (update)</li>
      <li>Stochastic gradient descent: Use 1 example in each iteration</li>
      <li><strong>Mini-batch gradient descent</strong>: Use b examples in each iteration (b = mini-batch size , b &lt;= m)</li>
    </ul>
  </li>
  <li>
    <p>Can work better than SGD in some cases</p>
  </li>
  <li><strong>Algorithm</strong>
    <ul>
      <li>needs to update 100 times with mini-batch size 10 and total data size 1000</li>
      <li><img src="https://user-images.githubusercontent.com/92680829/158750901-d4aad284-9fee-43c6-aefd-5881c43ce8da.png" width="400" /></li>
    </ul>
  </li>
</ul>

<h3 id="mini-batch-vs-stochastic"><strong>Mini-batch vs. stochastic</strong></h3>
<ul>
  <li>Advantage
    <ul>
      <li>Allows <strong>Vectorized implementation</strong>
        <ul>
          <li>each sum of b examples can be performed in a vectorized way</li>
          <li>can temporarily parallelize your computation (i.e. do 10 at once)</li>
          <li>much more efficient to compute rather than computing all examples just as a single number</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Disadvantage
    <ul>
      <li>Optimization process to decide parameter b
        <ul>
          <li>But this is often worth it!</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Actually, Stochastic gradient descent (b=1) and Batch gradient descent (b=m) are just specific forms of batch-gradient descent</li>
</ul>

<hr />

<h2 id="map-reduce-and-data-parallelism"><strong>Map Reduce and Data Parallelism</strong></h2>
<ul>
  <li>Sometimes you have so massive data that you can‚Äôt even handle all of them in one computer, no matter what algorithms you choose to use (even SGD)</li>
  <li>Some says that Map Reduce is equally or even more important than SGD !!</li>
</ul>

<h3 id="example"><strong>Example</strong></h3>
<ul>
  <li>Assume we are training Mini-Batch Algorithms with massive data (commonly greater than 4 bilions)
    <ul>
      <li><img src="https://user-images.githubusercontent.com/92680829/158761990-651aeeda-8892-46a6-a68a-a1711a73b69a.png" width="600" /></li>
    </ul>
  </li>
  <li>training all these examples at one computer will take too much time and cost</li>
  <li>So, we split these examples into (say) 4 Computers : Parallelising over different computers
    <ul>
      <li>Machine 1 : temp1 : Œ£ use (x1, y1), ‚Ä¶, (x100, y100)</li>
      <li>Machine 2 : temp2 : Œ£ use (x101, y101), ‚Ä¶, (x200, y200)</li>
      <li>Machine 3 : temp3 : Œ£use (x201, y201), ‚Ä¶, (x300, y300)</li>
      <li>Machine 4 : temp4 : Œ£ use (x301, y301), ‚Ä¶, (x400, y400)</li>
    </ul>
  </li>
  <li>And then, we will gonna send all four result (temp 1~4) from each computer to one centralized computer where the actual update occurs
    <ul>
      <li>Put them together, and Update Œ∏ using
        <ul>
          <li><img src="https://user-images.githubusercontent.com/92680829/158762720-5dde4100-af27-4b68-92c8-9183925a4330.png" width="500" /></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Same optimization algorithm can be applied to Logistic Regression (if remember, partial derivative term of cost function of logistic and linear regression is same : sum over training set)</li>
</ul>

<h3 id="scheme-of-how-map-reduce-happens"><strong>Scheme of how map reduce happens</strong></h3>
<ul>
  <li>
    <p><img src="https://user-images.githubusercontent.com/92680829/158763099-38384558-f303-4ee1-be86-9d645425583a.png" width="500" /></p>
  </li>
  <li>The bulk of the work in gradient descent is actually the summation of functions over training set</li>
  <li>the reason why batch gd takes more time to sgd is because it calculates the sum of larger data</li>
  <li>With Map Reduce Approach, each of the computers does a quarter of the work at the same time, so you get a 4x speedup
    <ul>
      <li>Of course, in practice, because of network latency, combining results, it‚Äôs slightly less than 4x, but still good!</li>
    </ul>
  </li>
  <li>Parallelization can come from
    <ul>
      <li>Multiple machines, CPUs, cores in each CPU</li>
    </ul>
  </li>
  <li>So even on a single compute can you implement parallelization, which is called ‚ÄúLocal Parallelization‚Äù</li>
  <li>with Hadoop : Open source implementation of Map reduce</li>
</ul>
:ET